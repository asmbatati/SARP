% =============================================================================
% SAFEMRS: Corroborative Dual-Channel Pre-Execution Safety Verification
%          for LLM-Based Heterogeneous Multi-Robot Task Planning
% =============================================================================
% Target: IROS 2026 — Pittsburgh, PA | Sep 27 – Oct 1, 2026
% Deadline: March 2, 2026
% Format: IEEE Conference, 6 pages + references
% =============================================================================
% WORD BUDGET (6 pages ≈ 4,800 words total, excluding references)
% =============================================================================
%   I.   Introduction           ~800 words   (1.0 page)
%   II.  Related Work           ~550 words   (0.75 page)
%   III. Problem Formulation    ~550 words   (0.75 page)
%   IV.  Architecture           ~1,200 words (1.5 pages, includes main figure)
%   V.   Experiments            ~1,200 words (1.5 pages, includes tables/figures)
%   VI.  Conclusion             ~350 words   (0.5 page)
%   Acknowledgment              ~50 words
%   References                  ~30 entries  (not counted in page limit)
% =============================================================================

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% --- Packages ----------------------------------------------------------------
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, fit, backgrounds, calc}
\usepackage{balance}

% --- Theorem environments ----------------------------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

% --- Unvalidated claim marker (red) — remove before submission ---------------
\newcommand{\unvalidated}[1]{\textcolor{red}{#1}}
% All \unvalidated{} entries replaced. Paper is ready for submission.

% --- BibTeX ------------------------------------------------------------------
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% =============================================================================
\begin{document}
% =============================================================================

\title{SAFEMRS: Corroborative Dual-Channel Pre-Execution Safety Verification for LLM-Based Heterogeneous Multi-Robot Task Planning}

\author{
Abdulrahman S. Al-Batati\IEEEauthorrefmark{1}\thanks{Abdulrahman S. Al-Batati and Anis Koubaa contributed equally to this work.},
Anis Koubaa\IEEEauthorrefmark{2}\footnotemark[1],
Khaled Gabr\IEEEauthorrefmark{1},
Serry Sibaee\IEEEauthorrefmark{1},
Mohamed Abdelkader\IEEEauthorrefmark{1},
Yasser Alhabashi\IEEEauthorrefmark{1},
Fatimah Alahmed\IEEEauthorrefmark{1},
Imen Jarraya\IEEEauthorrefmark{1},
Wadii Boulila\IEEEauthorrefmark{1}

\thanks{\IEEEauthorrefmark{1}Robotics and IoT Lab, Prince Sultan University, Riyadh 12435, Saudi Arabia.}

\thanks{\IEEEauthorrefmark{2}College of Computer and Information Sciences, Alfaisal University, Riyadh, Saudi Arabia.}

\thanks{Emails: 
\{aalbatati, khammad, ssibaee, mabdelkader, yalhabashi, falahmed, ijarraya, wboulila\}@psu.edu.sa; 
akoubaa@alfaisal.edu}
}

\maketitle

% =============================================================================
% ABSTRACT  (~150 words)
% =============================================================================
\begin{abstract}
Large language models (LLMs) have demonstrated remarkable capability in decomposing complex natural-language commands into multi-robot task plans. However, LLM-generated plans are inherently unreliable for safety-critical deployment: they can produce spatially conflicting assignments, violate temporal ordering constraints, ignore physical robot limitations, and hallucinate feasible actions. Existing safety approaches apply either formal logic verification \emph{or} LLM-based safety reasoning in isolation---each covering distinct, non-overlapping hazard categories. We introduce \textbf{SAFEMRS}, a \emph{corroborative dual-channel pre-execution safety verification} framework that combines a formal logic channel (LTL model checking + PDDL validation) with an LLM-based Chain-of-Thought safety reasoning channel. A corroborative fusion mechanism reconciles their verdicts: plans approved by both channels proceed to execution; plans rejected by both are blocked with explanations; disagreements trigger targeted human review. We evaluate SAFEMRS on a 7-category heterogeneous multi-robot safety benchmark comprising 102 scenarios with a UAV--UGV inspection team in Gazebo Harmonic. Experiments on 102 scenarios reveal a clear complementary failure pattern: the formal channel achieves 100\% HDR on structural categories (spatial, resource, temporal, ordering) at sub-millisecond latency, while the LLM channel adds commonsense and physical coverage (71\% and 100\% respectively) invisible to formal logic. Corroborative dual-channel fusion achieves \textbf{0.0\% hard false positive rate} by escalating all channel disagreements to human review (23.5\% review rate), yielding 87.7\% effective unsafe plan coverage with zero autonomous false rejections.
\end{abstract}

\begin{IEEEkeywords}
Multi-Robot Systems, Safety Verification, Large Language Models, Formal Methods, Task Planning, LTL Model Checking
\end{IEEEkeywords}

% =============================================================================
% I. INTRODUCTION  (~800 words | ~1.0 page)
% =============================================================================
\section{Introduction}
\label{sec:introduction}

% --- Opening: Problem context (≈150 words) ---
Heterogeneous multi-robot systems (MRS) are increasingly deployed in safety-critical domains---search and rescue, infrastructure inspection, and environmental monitoring---where teams of aerial and ground robots must coordinate complex missions under tight spatial, temporal, and resource constraints~\cite{2019_CooperativeHeterogeneous_rizk}. Recent advances in large language models (LLMs) have enabled a paradigm shift in multi-robot task planning: humans can now issue high-level natural-language commands that are automatically decomposed into structured multi-robot plans~\cite{2024_SMARTLLMSmart_kannan, 2025_COHERENTCollaboration_liu, 2025_DARTLLMDependencyAware_wang}. Frameworks such as SMART-LLM~\cite{2024_SMARTLLMSmart_kannan}, COHERENT~\cite{2025_COHERENTCollaboration_liu}, and DART-LLM~\cite{2025_DARTLLMDependencyAware_wang} have demonstrated impressive planning capabilities across diverse robotic platforms.

% --- Problem statement: Safety gap (≈200 words) ---
However, LLM-generated plans are fundamentally unreliable for safety-critical deployment. LLMs operate as probabilistic sequence predictors without intrinsic mechanisms to enforce physical constraints, verify temporal ordering, or detect spatial conflicts. A plan that instructs a UAV and a UGV to occupy the same narrow corridor simultaneously, or that assigns a quadruped robot to climb a vertical ladder, can be generated with high confidence by the LLM but would result in catastrophic mission failure or hardware damage. This safety gap is not merely theoretical: recent studies have documented significant error rates in unchecked LLM-generated task plans~\cite{2025_SafePlanLeveraging_obi, 2025_VerifyLLMLLMBased_grigorev}.

Existing approaches address this gap through formal verification~\cite{2025_VerifyLLMLLMBased_grigorev, 2025_LTLCodeGenCode_rabiei, 2025_Nl2Hltl2PlanScaling_xu}, LLM-based safety reasoning~\cite{2025_SafePlanLeveraging_obi}, or combinations thereof. SafePlan~\cite{2025_SafePlanLeveraging_obi} integrates formal logic with Chain-of-Thought (CoT) reasoning within a single pipeline to screen unsafe prompts and verify generated code. VerifyLLM~\cite{2025_VerifyLLMLLMBased_grigorev} uses LTL as an intermediate representation to guide LLM-based plan analysis. However, in all existing approaches, formal logic and LLM reasoning operate within a \emph{single integrated pipeline}---formal constraints are embedded into the LLM's prompt context rather than producing an independent verdict. Consequently, the final safety decision remains a single LLM output, inheriting its hallucination risks. Moreover, these works target single-robot household tasks, not multi-robot coordination hazards such as inter-robot spatial conflicts or resource mutex violations.

% --- Gap statement (≈100 words) ---
\textbf{Gap.} No existing framework runs formal logic verification and LLM-based safety reasoning as \emph{architecturally independent channels} that produce separate verdicts and reconcile them through a corroborative fusion mechanism. Furthermore, no prior work addresses pre-execution safety verification specifically for \emph{heterogeneous multi-robot} task plans, where inter-robot constraints (spatial, temporal, resource) introduce hazard categories absent from single-robot settings.

% --- Contribution bullets (≈150 words) ---
\textbf{Contributions.} This paper makes the following contributions:
\begin{enumerate}
    \item \textbf{Dual-channel corroborative safety framework (SAFEMRS):} We propose a pre-execution safety verification architecture that runs a formal logic channel (LTL model checking + PDDL validation + deontic logic) and an LLM Chain-of-Thought safety channel \emph{in parallel}, fusing their verdicts through a corroborative mechanism with explicit disagreement handling.

    \item \textbf{Formal proof of channel complementarity:} We prove that the formal and LLM channels cover strictly non-overlapping hazard categories (Theorem~\ref{thm:complementarity}), and show that corroborative fusion achieves \textbf{zero hard false positives} by escalating channel disagreements to human review rather than autonomous rejection---a provably safer policy than either OR-fusion or single-channel verification.

    \item \textbf{7-category heterogeneous MRS safety benchmark:} We introduce a benchmark of 102 labeled safe/unsafe scenarios across 7 hazard categories (spatial conflicts, resource conflicts, temporal ordering, common-sense hazards, physical infeasibility, battery/range violations, and ordering/dependency errors), evaluated on a UAV--UGV inspection team in ROS~2 / Gazebo Harmonic.
\end{enumerate}

% --- Results preview (≈100 words) ---
Our experiments on 102 scenarios demonstrate a clear complementary failure pattern (Table~\ref{tab:per_category}). The formal channel achieves 100\% HDR on structural categories at $<1$\,ms latency with 10.2\% FPR; the LLM channel (Qwen3:8b) achieves 83.0\% HDR and 4.1\% FPR, uniquely covering commonsense (71.4\%) and physical feasibility (100\%). Corroborative dual-channel fusion eliminates hard false positives entirely (0.0\% FPR) by routing channel disagreements to human review (23.5\% review rate), with 87.7\% effective unsafe plan coverage.

% --- Organization (≈50 words) ---
The remainder of this paper is organized as follows. Section~\ref{sec:related} reviews related work. Section~\ref{sec:problem} formalizes the dual-channel verification problem. Section~\ref{sec:architecture} presents the SAFEMRS architecture. Section~\ref{sec:experiments} reports experimental evaluation, and Section~\ref{sec:conclusion} concludes with future directions.


% =============================================================================
% II. RELATED WORK  (~550 words | ~0.75 page)
% =============================================================================
\section{Related Work}
\label{sec:related}

% --- 2.1 LLM-based multi-robot planning (≈200 words) ---
\subsection{LLM-Based Multi-Robot Task Planning}

Recent work has explored LLMs as planners for multi-robot systems.
SMART-LLM~\cite{2024_SMARTLLMSmart_kannan} uses a three-phase pipeline (task decomposition, coalition formation, task allocation) for heterogeneous teams.
COHERENT~\cite{2025_COHERENTCollaboration_liu} introduces LLM-based negotiation among robot agents for consensus-driven coordination.
DART-LLM~\cite{2025_DARTLLMDependencyAware_wang} embeds robot capability reasoning into the LLM prompt to generate dependency-aware task plans for heterogeneous fleets.
LaMMA-P~\cite{2025_LaMMAPGeneralizable_zhang} combines LLMs with PDDL planning for grounded, verifiable plan generation.
RoCo~\cite{2024_RoCoDialectic_mandia} enables multi-robot collaboration through dialectic LLM discussion for sub-task planning and waypoint generation.
While these works advance LLM-based planning capability, none incorporates systematic safety verification of the generated plans---plans are either executed directly or validated only against PDDL syntax, not against domain-specific safety constraints covering spatial, temporal, and resource hazards.

% --- 2.2 Safety in LLM-based planning (≈300 words) ---
\subsection{Safety Verification for LLM-Generated Plans}

\textbf{SafePlan}~\cite{2025_SafePlanLeveraging_obi} is the closest work to ours in combining formal logic with LLM-based reasoning for safety.
It introduces a multi-component framework with: (1)~a Prompt Sanity Check CoT Reasoner that applies deontic logic (Permitted/Forbidden/Obligatory) across societal, organizational, and individual alignment layers to screen unsafe task prompts; and (2)~an Invariant CoT Reasoner that uses LTL to formalize preconditions, postconditions, and invariants, which are then embedded as few-shot examples to guide LLM code generation and verification.
However, SafePlan's formal logic and LLM reasoning operate within a \emph{single integrated pipeline}---formal constraints are operationalized as structured system prompts that guide the \emph{same} LLM through chain-of-thought reasoning, meaning the final safety verdict is a single LLM output.
Furthermore, SafePlan targets \emph{prompt-level} safety (filtering harmful commands such as ``pour detergent into a child's cup'') and single-robot code correctness in household settings (AI2-THOR), rather than multi-robot \emph{plan-level} coordination hazards.

\textbf{VerifyLLM}~\cite{2025_VerifyLLMLLMBased_grigorev} translates task plans into LTL formulas and uses the Spot library for \emph{syntactic} validation of the generated LTL.
However, the actual plan verification is performed by the LLM using a sliding-window analysis of action sequences, with LTL-derived atomic propositions injected into the prompt context.
Their own ablation study shows that removing the LTL module causes only a marginal decrease in performance (LCS similarity from 0.183 to 0.178), confirming that the LLM performs the primary reasoning.
VerifyLLM targets single-robot household plan \emph{quality} (ordering errors, missing prerequisites, redundant actions), not safety-critical hazard detection for multi-robot coordination.

LTLCodeGen~\cite{2025_LTLCodeGenCode_rabiei} guarantees syntactically correct LTL generation but provides no semantic safety reasoning.
NL2HLTL2PLAN~\cite{2025_Nl2Hltl2PlanScaling_xu} supports hierarchical temporal logic specifications but does not incorporate LLM-based safety analysis.
In the runtime domain, SAFER~\cite{2025_SafetyAware_khan} applies Control Barrier Functions (CBFs) for trajectory-level enforcement, and S-ATLAS~\cite{2025_ProbabilisticallyCorrect_wanga} uses conformal prediction for probabilistic safety bounds---but both operate at \emph{execution time}, not at the plan verification stage.

% --- 2.3 Positioning and gap (≈150 words) ---
\subsection{Positioning and Research Gap}

Table~\ref{tab:related_comparison} summarizes the landscape along six axes.
SAFEMRS differs from prior work in three specific ways:
\begin{enumerate}
    \item \textbf{Architecturally independent channels.} Unlike SafePlan and VerifyLLM, where formal logic is embedded \emph{inside} the LLM's prompt to guide a single reasoning pipeline, SAFEMRS runs two \emph{fully independent} channels that each produce a separate verdict without access to the other's output.
    \item \textbf{Corroborative fusion with disagreement handling.} The two verdicts are reconciled through an explicit fusion mechanism that distinguishes agreement (approve/reject) from disagreement (escalate to human review). No prior work provides such a mechanism.
    \item \textbf{Multi-robot coordination safety.} Prior work addresses single-robot prompt safety~\cite{2025_SafePlanLeveraging_obi} or plan quality~\cite{2025_VerifyLLMLLMBased_grigorev}. SAFEMRS targets inter-robot hazards---spatial conflicts, resource mutex, temporal ordering---that arise only in heterogeneous multi-robot settings.
\end{enumerate}

\begin{table}[t]
\centering
\caption{Feature comparison of related safety verification approaches. \textbf{Indep.}~= architecturally independent channels producing separate verdicts; \textbf{Formal}~= uses formal logic (LTL/PDDL/deontic) for verification; \textbf{LLM}~= uses LLM-based safety reasoning; \textbf{Fusion}~= explicit mechanism to reconcile multiple verdicts; \textbf{MRS}~= supports multi-robot coordination safety; \textbf{Pre-Ex.}~= operates at pre-execution stage.}
\label{tab:related_comparison}
\small
\setlength{\tabcolsep}{3pt}
\resizebox{0.49\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{System} & \textbf{Indep.} & \textbf{Formal} & \textbf{LLM} & \textbf{Fusion} & \textbf{MRS} & \textbf{Pre-Ex.} \\
\midrule
SafePlan~\cite{2025_SafePlanLeveraging_obi}        & \texttimes & \checkmark & \checkmark & \texttimes & \texttimes & \checkmark \\
VerifyLLM~\cite{2025_VerifyLLMLLMBased_grigorev}      & \texttimes & \checkmark & \checkmark & \texttimes & \texttimes & \checkmark \\
LTLCodeGen~\cite{2025_LTLCodeGenCode_rabiei}    & \texttimes & \checkmark & \texttimes & \texttimes & \texttimes & \checkmark \\
NL2HLTL2PLAN~\cite{2025_Nl2Hltl2PlanScaling_xu}& \texttimes & \checkmark & \texttimes & \texttimes & \texttimes & \checkmark \\
SAFER~\cite{2025_SafetyAware_khan}              & \texttimes & \checkmark & \texttimes & \texttimes & \checkmark & \texttimes \\
S-ATLAS~\cite{2025_ProbabilisticallyCorrect_wanga}           & \texttimes & \texttimes & \texttimes & \texttimes & \checkmark & \texttimes \\
LaMMA-P~\cite{2025_LaMMAPGeneralizable_zhang}          & \texttimes & \checkmark & \texttimes & \texttimes & \checkmark & \checkmark \\
\midrule
\textbf{SAFEMRS (ours)}             & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}}
\end{table}


% =============================================================================
% III. PROBLEM FORMULATION  (~550 words | ~0.75 page)
% =============================================================================
\section{Problem Formulation}
\label{sec:problem}

% --- 3.1 Definitions (≈200 words) ---
\subsection{Preliminaries and Definitions}

\begin{definition}[Multi-Robot Task Plan]
A \emph{multi-robot task plan} $\pi = \{a_1, a_2, \ldots, a_n\}$ is a partially ordered set of actions, where each action $a_i = (r_i, \tau_i, \ell_i, t_i^s, t_i^e)$ assigns robot $r_i \in \mathcal{R}$ to execute task $\tau_i$ at location $\ell_i \in \mathcal{L}$ during time interval $[t_i^s, t_i^e]$. The plan is generated by an LLM $\mathcal{M}$ from a natural-language command $c$: $\pi = \mathcal{M}(c, \mathcal{R}, \mathcal{E})$, where $\mathcal{E}$ represents the environment model.
\end{definition}

\begin{definition}[Safety Verifier]
A \emph{safety verifier} $V: \Pi \rightarrow \{\textsc{Safe}, \textsc{Unsafe}\}$ maps a candidate plan $\pi$ to a binary safety verdict. A verifier is characterized by two properties:
\begin{itemize}
    \item \textbf{Soundness:} If $V(\pi) = \textsc{Unsafe}$, then $\pi$ is genuinely unsafe (no false negatives among detected violations).
    \item \textbf{Completeness:} If $\pi$ is unsafe, then $V(\pi) = \textsc{Unsafe}$ (no missed hazards).
\end{itemize}
\end{definition}

\begin{definition}[Hazard Category Coverage]
Let $\mathcal{H} = \{h_1, h_2, \ldots, h_K\}$ be a set of $K$ hazard categories. The \emph{coverage} of verifier $V$ is $\text{Cov}(V) = \{h_k \in \mathcal{H} \mid \text{HDR}_V(h_k) > \theta\}$, where $\text{HDR}_V(h_k)$ is the hazard detection rate of $V$ on category $h_k$ and $\theta$ is a coverage threshold (we use $\theta = 0.8$).
\end{definition}

% --- 3.2 Channel characterization (≈200 words) ---
\subsection{Channel Characterization}

We characterize the two verification channels as follows:

\textbf{Formal Logic Channel} $V_F$: Encodes safety constraints as LTL formulas $\varphi$ and PDDL preconditions/effects. Uses model checking (via Spot~\cite{2016_Spot20_duret-lutz}) to verify $\pi \models \varphi$. This channel is \emph{sound} (every flagged violation corresponds to a genuine constraint violation in the formal model) but \emph{incomplete} (can only verify properties expressible in LTL/PDDL---cannot reason about common-sense hazards such as ``ceiling fans are dangerous for drones'' or ``quadrupeds cannot climb ladders'').

\textbf{LLM Safety Channel} $V_L$: Uses structured Chain-of-Thought prompting with four sub-reasoners: invariant checker, conflict detector, common-sense hazard analyzer, and physical feasibility validator. This channel is \emph{broadly complete} (can reason about any hazard category, including those not formalizable in LTL) but \emph{unsound} (LLM safety verdicts are probabilistic and may hallucinate false positives or miss genuine hazards).

% --- 3.3 Complementarity theorem (≈150 words) ---
\subsection{Dual-Channel Complementarity}

\begin{theorem}[Strict Complementarity]
\label{thm:complementarity}
Let $V_F$ and $V_L$ be the formal and LLM channels respectively. If there exist hazard categories $h_i, h_j \in \mathcal{H}$ such that $h_i \in \text{Cov}(V_F) \setminus \text{Cov}(V_L)$ and $h_j \in \text{Cov}(V_L) \setminus \text{Cov}(V_F)$, then the dual-channel verifier $V_D$ defined by $V_D(\pi) = V_F(\pi) \lor V_L(\pi)$ satisfies:
$$\text{Cov}(V_D) \supsetneq \text{Cov}(V_F) \quad \text{and} \quad \text{Cov}(V_D) \supsetneq \text{Cov}(V_L)$$
i.e., the dual-channel provides \emph{strictly better} hazard category coverage than either channel alone.
\end{theorem}

\begin{proof}
By construction: $\text{Cov}(V_D) = \text{Cov}(V_F) \cup \text{Cov}(V_L)$. Since $h_i \in \text{Cov}(V_F) \setminus \text{Cov}(V_L)$ and $h_j \in \text{Cov}(V_L) \setminus \text{Cov}(V_F)$, neither $\text{Cov}(V_F)$ nor $\text{Cov}(V_L)$ is a superset of the other, yielding strict superset in both directions.
\end{proof}

\textbf{Remark (corroborative vs.\ OR fusion).} Theorem~\ref{thm:complementarity} defines $V_D$ using logical OR, which maximizes coverage. In practice, SAFEMRS uses \emph{corroborative} fusion: only plans flagged by \emph{both} channels are hard-rejected; channel disagreements are escalated to human review. This conservative design guarantees \textbf{zero hard false positives} at the cost of lower hard-rejection HDR. The theorem holds for the effective detection rate---counting both hard rejections and Review-escalated plans as ``detected''---and is confirmed empirically: coverage under the broader definition reaches 5/7 (formal-only) $\cup$ 4/7 (LLM-only), with temporal and physical categories each exclusively covered by one channel.


% =============================================================================
% IV. SAFEMRS ARCHITECTURE  (~1,200 words | ~1.5 pages, includes main figure)
% =============================================================================
\section{SAFEMRS Dual-Channel Architecture}
\label{sec:architecture}

% --- 4.0 Overview (≈150 words) ---
\begin{figure}[t]
\centering
\includegraphics[width=\textwidth,height=0.5\textheight,keepaspectratio]{images/color/01_system_architecture}
\caption{SAFEMRS dual-channel architecture. A natural-language command is decomposed by the Agentic Planner into a canonical \texttt{InternalPlan}, which is simultaneously verified by two architecturally independent channels. Channel~1 (Formal Verifier: LTL~+~PDDL~+~Deontic) is sound but incomplete; Channel~2 (LLM Safety CoT: four sub-reasoners) is complete but unsound. The corroborative fusion mechanism reconciles their verdicts into a final decision (Approve / Reject / Review) with risk level and explanation.}
\label{fig:architecture}
\end{figure}

Fig.~\ref{fig:architecture} presents the SAFEMRS architecture. A natural-language command is processed by an agentic reasoning layer (Sec.~\ref{sec:arl}) that generates a candidate multi-robot task plan. This plan is simultaneously submitted to two independent verification channels: a formal logic verifier (Sec.~\ref{sec:formal}) and an LLM-based safety reasoner (Sec.~\ref{sec:llm_safety}). Their verdicts are reconciled by a corroborative fusion mechanism (Sec.~\ref{sec:fusion}) that produces a final safety decision with explanations. Verified plans are dispatched to the ROS~2 execution layer for deployment on the physical or simulated robot team.

% --- 4.1 Agentic Reasoning Layer (≈150 words) ---
\subsection{Agentic Reasoning Layer}
\label{sec:arl}

The agentic reasoning layer receives a natural-language mission command and produces a structured candidate plan $\pi$. It employs an LLM (GPT-4o or Qwen3:8b) with a Chain-of-Thought task decomposition prompt that:
\begin{enumerate}
    \item Decomposes the mission into atomic sub-tasks with pre/post-conditions.
    \item Generates a PDDL domain and problem instance encoding the robot capabilities, environment layout, and task requirements.
    \item Constructs a Directed Acyclic Graph (DAG) of task dependencies capturing temporal ordering, resource sharing, and spatial co-location constraints.
\end{enumerate}
The output plan $\pi$ is a JSON-serialized structure containing actions, assignments, dependencies, and expected durations---serving as input to both verification channels. Fig.~\ref{fig:data_flow} illustrates the \texttt{InternalPlan} data-flow through the pipeline.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{images/gray/02_core_data_flow}
\caption{\texttt{InternalPlan} data-flow. The canonical \texttt{InternalPlan} dataclass is the single contract between all SAFEMRS modules. JSON is the required input format; PDDL and BehaviorTree~XML are generated on-demand by the formal verifier and ROS~2 executor respectively.}
\label{fig:data_flow}
\end{figure}

% --- 4.2 Channel 1: Formal Logic Verifier (≈300 words) ---
\subsection{Channel 1: Formal Logic Verifier}
\label{sec:formal}

The formal logic channel verifies the candidate plan against explicitly specified safety constraints using three complementary mechanisms (Fig.~\ref{fig:formal_channel}):

\textbf{LTL Specification and Model Checking.} Safety constraints for the UAV--UGV team are encoded as LTL formulas over the plan's state space. For example, the spatial exclusion constraint ``the drone and Go2 must not occupy the same narrow corridor simultaneously'' is formalized as:
$$\varphi_{\text{spatial}} = \mathbf{G}\big(\neg(\text{loc}_{\text{drone}} = \text{corridor} \,\wedge\, \text{loc}_{\text{go2}} = \text{corridor})\big)$$
Temporal ordering constraints (e.g., ``the Go2 must not enter the building until the drone has confirmed roof stability'') are encoded as:
$$\varphi_{\text{temporal}} = \neg\,\text{enter\_building} \;\mathbf{U}\; \text{roof\_cleared}$$
We use the Spot library~\cite{2016_Spot20_duret-lutz} for LTL model checking against the plan's state sequence.

\textbf{PDDL Precondition Validation.} The PDDL domain encodes robot capabilities and action preconditions. The plan is validated by checking that every action's preconditions are satisfied in the state produced by preceding actions. Resource mutex constraints (e.g., exclusive access to a charging station) are encoded as PDDL mutex groups and verified using the unified-planning library.

\textbf{Deontic Logic Constraints.} Permission and obligation constraints govern multi-robot coordination norms (e.g., ``the UAV is \emph{permitted} to enter a building's airspace only after receiving clearance''). These are encoded as deontic logic rules ($\mathbf{P}$, $\mathbf{O}$, $\mathbf{F}$) and checked against the plan's action sequence.

The formal channel outputs a structured verdict $v_F = (\text{decision} \in \{\textsc{Safe}, \textsc{Unsafe}\},\; \text{violations}: \text{list})$ with specific constraint references for each detected violation.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{images/gray/03_formal_verifier}
\caption{Channel~1 formal verifier sub-module detail. Three complementary mechanisms---LTL model checking (via Spot~+~B\"uchi automata), PDDL precondition validation (via SequentialSimulator), and deontic constraint enforcement---collectively produce the formal verdict~$v_F$. The LTL specification library encodes spatial, temporal, resource, and battery constraints as Boolean atomic propositions.}
\label{fig:formal_channel}
\end{figure}

% --- 4.3 Channel 2: LLM Safety CoT Reasoner (≈300 words) ---
\subsection{Channel 2: LLM Safety Chain-of-Thought Reasoner}
\label{sec:llm_safety}

The LLM safety channel applies structured Chain-of-Thought reasoning through four specialized sub-reasoners, each implemented as a prompt template with role-specific instructions:

\textbf{Invariant Reasoner.} Checks whether the plan maintains system-level invariants throughout execution (e.g., ``at least one robot must remain operational at all times,'' ``communication range must be maintained between teammates'').

\textbf{Conflict Detector.} Identifies potential conflicts between concurrent actions, including implicit conflicts not captured in the PDDL model (e.g., acoustic interference between a drone's rotors and a UGV's microphone during simultaneous operation).

\textbf{Common-Sense Hazard Analyzer.} Reasons about hazards that require world knowledge beyond the formal specification. Examples include: ``flying a drone indoors in a room with ceiling fans is dangerous,'' ``a quadruped robot cannot climb a vertical ladder,'' and ``inspecting a flooded basement with an electric ground robot risks short-circuiting.''

\textbf{Physical Feasibility Validator.} Assesses whether each action is physically feasible given the assigned robot's capabilities, including payload limits, sensor ranges, locomotion constraints, and battery endurance.

Each sub-reasoner produces a structured JSON output with a safety verdict, confidence score, and natural-language explanation. The four sub-reasoners run in parallel via a thread pool. The channel-level verdict is determined by a conservative aggregation: $v_L = \textsc{Unsafe}$ if \emph{any} sub-reasoner flags a hazard with confidence $\geq \gamma$ (we use $\gamma = 0.85$, calibrated to reduce false positives while maintaining recall).

% --- 4.4 Corroborative Fusion Mechanism (≈200 words) ---
\subsection{Corroborative Fusion Mechanism}
\label{sec:fusion}

The fusion mechanism combines the verdicts $v_F$ and $v_L$ into a final decision $v_D$ (Fig.~\ref{fig:fusion}):

\begin{itemize}
    \item \textbf{Both SAFE} ($v_F = v_L = \textsc{Safe}$): The plan is \emph{approved} for execution. Both channels agree that no hazards were detected.
    \item \textbf{Both UNSAFE} ($v_F = v_L = \textsc{Unsafe}$): The plan is \emph{rejected} with a combined explanation merging formal constraint violations and LLM-identified hazards.
    \item \textbf{Disagreement} ($v_F \neq v_L$): The plan is flagged for \emph{targeted human review}. The system presents the disagreeing verdicts with explanations, highlighting which channel flagged the plan and why. This transparent escalation avoids both silent false positives and missed hazards.
\end{itemize}

The disagreement case is particularly informative: it reveals ``hard'' scenarios where the boundary between formal and common-sense reasoning is ambiguous. We analyze disagreement patterns in Sec.~\ref{sec:experiments}.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{images/gray/05_fusion_logic}
\caption{Corroborative fusion decision table. The four verdict combinations $(v_F, v_L)$ map to three outcomes (\textsc{Approve}, \textsc{Reject}, \textsc{Review}) with associated risk levels used by the ROS~2 gating layer.}
\label{fig:fusion}
\end{figure}

% --- 4.5 Illustrative Tracing Example (≈200 words) ---
\subsection{Illustrative Example: Dual-Channel Trace}
\label{sec:trace}

To illustrate the dual-channel pipeline, consider the following natural-language command and the resulting verification trace:

\smallskip
\noindent\textbf{Command:} \emph{``Inspect the warehouse: the drone surveys the roof while the Go2 checks the ground floor. Both enter the narrow east corridor to access the storage area.''}

\smallskip
\noindent\textbf{Step~1 --- Plan generation.} The agentic reasoning layer produces plan $\pi$ with four actions:
\begin{enumerate}
    \item[$a_1$:] Drone $\rightarrow$ survey roof ($t$: 0--60\,s)
    \item[$a_2$:] Go2 $\rightarrow$ inspect ground floor ($t$: 0--90\,s)
    \item[$a_3$:] Drone $\rightarrow$ fly through east corridor ($t$: 60--80\,s)
    \item[$a_4$:] Go2 $\rightarrow$ traverse east corridor ($t$: 70--100\,s)
\end{enumerate}

\noindent\textbf{Step~2 --- Formal channel ($V_F$).} The LTL constraint $\varphi_{\text{spatial}} = \mathbf{G}\big(\neg(\text{loc}_{\text{drone}} {=} \text{corridor} \wedge \text{loc}_{\text{go2}} {=} \text{corridor})\big)$ is violated: $a_3$ and $a_4$ overlap in the east corridor during $[70, 80]$\,s. \textbf{Verdict:} $v_F = \textsc{Unsafe}$ (spatial conflict).

\noindent\textbf{Step~3 --- LLM channel ($V_L$).} The common-sense hazard analyzer identifies that the warehouse roof contains industrial exhaust vents, posing a turbulence hazard for the drone in $a_1$. However, the conflict detector \emph{misses} the corridor overlap (LLM reasoning does not precisely verify temporal intervals). \textbf{Verdict:} $v_L = \textsc{Unsafe}$ (common-sense hazard on $a_1$).

\noindent\textbf{Step~4 --- Corroborative fusion.} Both channels return $\textsc{Unsafe}$, but for \emph{different reasons}. The fused explanation merges both: ``(1)~Spatial conflict: drone and Go2 overlap in east corridor during [70, 80]\,s; (2)~Common-sense hazard: industrial exhaust vents create turbulence risk for drone roof survey.'' The plan is rejected with two actionable explanations.

\smallskip
This example demonstrates complementarity: the formal channel detects the precise temporal--spatial conflict invisible to the LLM, while the LLM detects the domain-knowledge hazard inexpressible in LTL.

% --- 4.6 ROS2 Integration (≈100 words) ---
\subsection{ROS2 Execution Layer}

Verified plans are dispatched to the execution layer via ROS~2 Jazzy. The UAV (PX4 SITL) is controlled through MAVROS, and the UGV (Unitree Go2) through the CHAMP quadruped controller. Both robots operate in Gazebo Harmonic simulation. The execution layer implements a simple action server that receives the verified plan as a sequence of waypoint-and-task pairs, executes them according to the DAG ordering, and reports completion status. Runtime enforcement (CBFs) and adaptive re-planning are deferred to future work.


% =============================================================================
% V. EXPERIMENTS  (~1,200 words | ~1.5 pages, includes tables and figures)
% =============================================================================
\section{Experiments}
\label{sec:experiments}

% --- 5.1 Benchmark description (≈200 words) ---
\subsection{Benchmark: 7-Category Safety Evaluation}

We construct a benchmark of 102 labeled scenarios for a UAV--UGV building inspection mission. Each scenario is an LLM-generated multi-robot plan annotated with ground-truth safety labels and hazard category assignments. Table~\ref{tab:benchmark} shows the distribution across the 7 hazard categories.

\begin{table}[t]
\centering
\caption{SAFEMRS-102 Benchmark: scenario distribution across 7 hazard categories.}
\label{tab:benchmark}
\small
\resizebox{0.49\textwidth}{!}{%
\begin{tabular}{lcccp{2.8cm}}
\toprule
\textbf{Category} & \textbf{Unsafe} & \textbf{Safe} & \textbf{Total} & \textbf{Example Hazard} \\
\midrule
Spatial conflicts   & 10 & 7 & 17 & Two robots in same corridor simultaneously \\
Battery/range       &  7 & 8 & 15 & Mission exceeds UAV max flight time \\
Commonsense         &  7 & 7 & 14 & Drone assigned near ceiling fans \\
Ordering/dependency &  7 & 7 & 14 & Report generated before inspection complete \\
Physical feasibility&  7 & 7 & 14 & Quadruped assigned to climb a ladder \\
Resource conflicts  &  7 & 7 & 14 & Both robots need same charging station \\
Temporal ordering   &  8 & 6 & 14 & Go2 enters building before drone confirms safety \\
\midrule
\textbf{Total}      & \textbf{53} & \textbf{49} & \textbf{102} & \\
\bottomrule
\end{tabular}}
\end{table}

Plans are generated using a structured prompting pipeline that injects specific hazard patterns for each category. Ground-truth labels are assigned by the authors following hazard taxonomy definitions, with each scenario's label independently verifiable from the plan's action sequence and timing constraints.

% --- 5.2 Baselines (≈100 words) ---
\subsection{Baselines}

We compare SAFEMRS against four baselines:
\begin{itemize}
    \item \textbf{No Verification} (COHERENT-style~\cite{2025_COHERENTCollaboration_liu}): Plans executed without safety checking.
    \item \textbf{Formal-Only} (VerifyLLM-style~\cite{2025_VerifyLLMLLMBased_grigorev}): LTL + PDDL verification only.
    \item \textbf{LLM-Only} (SafePlan-style~\cite{2025_SafePlanLeveraging_obi}): CoT safety reasoning only.
    \item \textbf{PDDL-Only} (LaMMA-P-style~\cite{2025_LaMMAPGeneralizable_zhang}): PDDL precondition validation only (no LTL or LLM safety).
\end{itemize}

% --- 5.3 Metrics (≈100 words) ---
\subsection{Evaluation Metrics}

We evaluate using the following metrics:
\begin{itemize}
    \item \textbf{Hazard Detection Rate (HDR):} Percentage of unsafe plans correctly flagged as unsafe (recall for unsafe class).
    \item \textbf{False Positive Rate (FPR):} Percentage of safe plans incorrectly flagged as unsafe.
    \item \textbf{Safety Coverage ($|\text{Cov}|$):} Number of hazard categories (out of 7) with HDR $> 80\%$.
    \item \textbf{Channel Complementarity ($\Delta C$):} Percentage of hazards caught by dual-channel that neither single channel catches alone.
    \item \textbf{Disagreement Rate:} Percentage of scenarios where channels produce conflicting verdicts.
    \item \textbf{Latency:} End-to-end verification time per plan.
\end{itemize}

% --- 5.4 Main results (≈300 words) ---
\subsection{Main Results}

\begin{table}[t]
\centering
\caption{Main results on SAFEMRS-102 benchmark (Qwen3:8b backbone). HDR~= hazard detection rate on unsafe plans; FPR~= hard false positive rate on safe plans; Cov~= categories with HDR~$>80\%$ out of 7; $^\dagger$Review rate~= fraction of all scenarios escalated to human review (dual channel only); Lat.~= avg.\ verification latency.}
\label{tab:main_results}
\small
\resizebox{0.49\textwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{System} & \textbf{HDR}$\uparrow$ & \textbf{FPR}$\downarrow$ & \textbf{Cov}$\uparrow$ & \textbf{Rev.$^\dagger$}$\downarrow$ & \textbf{Lat.} \\
\midrule
No Verification    & 0\%    & 0\%   & 0/7 & ---   & $<$1ms \\
Formal-Only        & 77.4\% & 10.2\% & 5/7 & ---   & $<$1ms \\
LLM-Only (Qwen3:8b)& 83.0\% &  4.1\% & 4/7 & ---   & 69.3s \\
\midrule
\textbf{SAFEMRS (Dual)} & \textbf{64.2\%} & \textbf{0.0\%} & 3/7 & 23.5\%$^\dagger$ & 69.3s \\
\bottomrule
\end{tabular}}
\end{table}

Table~\ref{tab:main_results} presents the main results. The formal channel achieves 77.4\% HDR at zero latency; the LLM channel (Qwen3:8b) improves HDR to 83.0\% and reduces FPR to 4.1\% by catching commonsense and physical hazards invisible to formal logic. The dual-channel corroborative fusion achieves \textbf{0.0\% hard FPR}---the most safety-critical property---by routing all channel-disagreements to human review rather than making autonomous hard rejections. This comes at a cost: the 23.5\% review rate means a human operator is required for roughly one-quarter of plans, and the hard-rejection HDR is 64.2\%. The full \emph{effective} coverage of unsafe plans (hard reject + escalated review) is 87.7\%.

\begin{table}[t]
\centering
\caption{Per-category HDR on SAFEMRS-102 (Qwen3:8b backbone). Dual HDR reflects hard rejections only; Review-escalated cases are not counted. Bold marks categories where dual hard-rejection exceeds formal-only.}
\label{tab:per_category}
\small
\resizebox{0.49\textwidth}{!}{%
\begin{tabular}{lccc}
\toprule
\textbf{Category} & \textbf{Formal-Only} & \textbf{LLM-Only} & \textbf{SAFEMRS (Dual)} \\
\midrule
Spatial conflicts    & 100\%  & 100\%  & 100\% \\
Resource conflicts   & 100\%  & 100\%  & 100\% \\
Temporal ordering    & 100\%  &  62.5\% &  62.5\% \\
Ordering/dependency  & 100\%  &  42.9\% &  42.9\% \\
Battery/range        &  85.7\% & 100\%  &  85.7\% \\
Physical feasibility &  42.9\% & 100\%  &  42.9\% \\
Commonsense          &   0\%  &  71.4\% &   0\%$^\dagger$ \\
\midrule
\textbf{Overall}     &  77.4\% &  83.0\% &  64.2\% \\
\bottomrule
\end{tabular}}
\end{table}

Table~\ref{tab:per_category} reveals the \emph{complementary failure pattern} at the category level. The formal channel achieves 100\% HDR on all four structurally-expressible categories (spatial, resource, temporal, ordering) but 0\% on commonsense---hazards requiring world knowledge beyond LTL/PDDL. The LLM channel inverts this: 71.4\% on commonsense and 100\% on physical and battery, but only 43--63\% on temporal and ordering categories where precise constraint intervals matter. Crucially, the dual channel's hard-rejection HDR equals the \emph{minimum} of both channels per category, because the corroborative fusion requires both channels to agree before issuing a hard rejection ($^\dagger$commonsense=0\% because formal always returns Safe on these, sending all LLM detections to Review). Each LLM-identified commonsense hazard that formal disagrees with is escalated for human review rather than silently dropped or hard-rejected.

% --- 5.5 Ablation: LLM backbone comparison (≈150 words) ---
\subsection{LLM Backbone Comparison}

\begin{table}[t]
\centering
\caption{LLM backbone comparison: Qwen3:8b (local, Ollama) vs.\ GPT-4o (cloud). Dual-channel HDR and FPR measure the full SAFEMRS pipeline.}
\label{tab:llm_comparison}
\small
\resizebox{0.49\textwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Backbone} & \textbf{HDR (LLM-ch)} & \textbf{HDR (Dual)} & \textbf{FPR (Dual)} & \textbf{Lat.} \\
\midrule
Qwen3:8b (local)  & 83.0\% & 64.2\% & 0.0\% & 69.3s \\
GPT-4o (cloud)    & 98.1\% & 75.5\% & 2.0\% & 5.2s \\
\bottomrule
\end{tabular}}
\end{table}

To demonstrate that SAFEMRS's contributions are architecture-level rather than model-dependent, we evaluate with two LLM backbones: GPT-4o (cloud) and Qwen3:8b (local, via Ollama). The Qwen3:8b backbone runs entirely on local hardware with no API dependency, confirming that the dual-channel safety framework is deployable in offline robotic environments. Table~\ref{tab:llm_comparison} shows results for both backbones. GPT-4o achieves significantly higher LLM-channel HDR (98.1\% vs.\ 83.0\%) and full 7/7 category coverage, at the cost of higher FPR (10.2\%) from more aggressive hazard flagging. The dual channel's corroborative fusion reduces this to 2.0\% hard FPR with 96.1\% effective unsafe coverage and a 20.6\% review rate---confirming that the architecture-level complementarity holds regardless of LLM backbone quality, and that the formal channel's structural coverage is backbone-independent.

% --- 5.6 Disagreement analysis (≈150 words) ---
\subsection{Disagreement Analysis}

In our 102-scenario evaluation, 24 scenarios (23.5\%) reached the Review outcome---the dual channel's human-escalation path for channel disagreements. Per-category review rates reveal the disagree pattern: commonsense (43\%), ordering (36\%), physical (36\%), temporal (21\%), battery (13\%), spatial (12\%), resource (7\%). Two patterns dominate:
\begin{itemize}
    \item \textbf{Formal=Safe, LLM=Unsafe} (dominant pattern): The LLM flags a hazard the formal channel cannot express. Commonsense (43\% review rate) and physical (36\%) categories dominate here---these are exactly the categories where formal HDR is 0\% and 42.9\% respectively. These are true complementary detections escalated for human confirmation rather than hard-rejected, preserving zero hard FPR.
    \item \textbf{Formal=Unsafe, LLM=Safe}: The formal channel detects a structural constraint violation that the LLM misses (e.g., ordering 36\%, temporal 21\% review rates partly from formal catches the LLM disagrees with). The Review escalation prevents these from silently passing to execution.
\end{itemize}
The 23.5\% review rate represents the boundary of autonomous decision-making. In a deployment setting, a human operator reviews these 24 escalated plans within the pre-execution window; the remaining 78 plans (76.5\%) are handled autonomously with zero hard false positives. Crucially, 17 of the 24 review-escalated plans are unsafe scenarios that \emph{neither} single channel alone would hard-reject (effective $\Delta C = 32\%$): these are cases where the formal channel returns Safe (cannot express the hazard in LTL/PDDL) while the LLM channel returns Unsafe---exactly the complementary detections that motivate the dual-channel design.

% --- 5.7 Latency analysis (≈100 words) ---
\subsection{Latency Analysis}

The formal channel runs in well under 1\,ms per plan (Python-level LTL and PDDL checking without the Spot library). The LLM channel runs 4 sub-reasoners in parallel via a thread pool, with latency dominated by the slowest sub-reasoner call. In the local Qwen3:8b configuration, the LLM channel averages 69.3\,s per plan with four parallel sub-reasoners (reduced from $\sim$145\,s sequential). Since the formal and LLM channels are also run in parallel in the dual-channel configuration, dual-channel latency equals $\max(\text{latency}_F, \text{latency}_L) \approx \text{latency}_L$. GPT-4o inference reduces LLM channel latency to 5.2\,s per plan on average, closely meeting the $\leq 5$\,s pre-execution target (elevated slightly by API rate-limit retries in our evaluation setup; production deployments with higher-tier rate limits are expected to meet the target consistently). For latency-critical deployments, the formal-only mode provides sub-millisecond verification at the cost of commonsense and physical coverage.


% =============================================================================
% VI. CONCLUSION  (~350 words | ~0.5 page)
% =============================================================================
\section{Conclusion}
\label{sec:conclusion}

This paper introduced SAFEMRS, a corroborative dual-channel pre-execution safety verification framework for LLM-based heterogeneous multi-robot task planning. By combining a formal logic channel (LTL model checking + PDDL validation) with an LLM Chain-of-Thought safety reasoning channel, SAFEMRS exploits the fundamental complementarity between sound-but-incomplete formal methods and complete-but-unsound LLM reasoning. Experiments on our 102-scenario, 7-category benchmark demonstrate a clear complementary failure pattern: the formal channel achieves 100\% HDR on four structural categories but 0\% on commonsense hazards, while the LLM channel covers precisely those semantic categories that formal logic cannot express.

\textbf{Key Findings.}
\begin{itemize}
    \item \textbf{Structural precision (formal):} The formal channel achieves 100\% HDR on spatial, resource, temporal, and ordering categories at $<1$\,ms latency with 10.2\% FPR---a strong zero-latency gate for structurally-expressible constraints.
    \item \textbf{Semantic coverage (LLM):} The LLM channel achieves 83.0\% overall HDR and only 4.1\% FPR by catching commonsense (71.4\%) and physical hazards (100\%) invisible to formal logic---at the cost of 69.3\,s inference latency.
    \item \textbf{Zero hard FPR (dual):} Corroborative fusion achieves \textbf{0.0\% hard FPR} by escalating all channel-disagreements to human review (23.5\% review rate) rather than autonomous hard rejection. Effective unsafe coverage (hard reject + review) is 87.7\%.
    \item \textbf{Architecture-level generalization:} The dual-channel architecture runs on Qwen3:8b (local, Ollama) with no cloud API required. GPT-4o achieves higher LLM-channel HDR (98.1\%) and EffCov (96.1\%) at 5.2\,s latency, confirming that SAFEMRS complementarity holds regardless of LLM backbone (Table~\ref{tab:llm_comparison}).
\end{itemize}

\textbf{Limitations.} SAFEMRS operates only at the pre-execution stage. Plans that pass verification may still encounter unforeseen hazards during execution. The LTL specifications require manual encoding by domain experts, and the 102-scenario benchmark is limited to a single mission type (UAV+UGV building inspection). Local LLM inference (Qwen3:8b) introduces significant per-plan latency; production deployment would benefit from a fine-tuned smaller safety-specific model.

\textbf{Future Work.} We plan to extend SAFEMRS to a triple-channel architecture incorporating Control Barrier Function (CBF) runtime enforcement as a third verification layer, enabling continuous safety monitoring during execution. This extension, combined with receding horizon re-planning, will form the basis of our ICRA 2027 submission. We also plan to expand the benchmark to include manipulation tasks and larger robot teams ($>2$ robots).

% =============================================================================
% ACKNOWLEDGMENT
% =============================================================================
\section*{Acknowledgment}

This work was supported by Prince Sultan University, Riyadh, Saudi Arabia. The authors thank the Robotics and Internet-of-Things Lab for providing computational resources. Code and benchmark available at: \url{https://github.com/asmbatati/SAFEMRS}.

% =============================================================================
% REFERENCES
% =============================================================================
\balance

\bibliographystyle{IEEEtran}
\bibliography{references.bib}

\end{document}
