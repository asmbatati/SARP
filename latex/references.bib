@incollection{2014_ReducingBarrier_coleman,
  title = {Reducing the {{Barrier}} to {{Entry}} of {{Complex Robotic Software}}: A {{MoveIt}}! {{Case Study}}},
  shorttitle = {Reducing the {{Barrier}} to {{Entry}} of {{Complex Robotic Software}}},
  author = {Coleman, David T. and Sucan, Ioan A. and Chitta, Sachin and Correll, Nikolaus},
  year = 2014,
  publisher = {Italy},
  doi = {10.6092/JOSER_2014_05_01_p3},
  urldate = {2026-01-20},
  abstract = {Developing robot agnostic software frameworks involves synthesizing the disparate fields of robotic theory and software engineering while simultaneously accounting for a large variability in hardware designs and control paradigms. As the capabilities of robotic software frameworks increase, the setup difficulty and learning curve for new users also increase. If the entry barriers for configuring and using the software on robots is too high, even the most powerful of frameworks are useless. A growing need exists in robotic software engineering to aid users in getting started with, and customizing, the software framework as necessary for particular robotic applications. In this paper a case study is presented for the best practices found for lowering the barrier of entry in the MoveIt! framework, an open-source tool for mobile manipulation in ROS, that allows users to 1) quickly get basic motion planning functionality with minimal initial setup, 2) automate its configuration and optimization, and 3) easily customize its components. A graphical interface that assists the user in configuring MoveIt! is the cornerstone of our approach, coupled with the use of an existing standardized robot model for input, automatically generated robot-specific configuration files, and a plugin-based architecture for extensibility. These best practices are summarized into a set of barrier to entry design principles applicable to other robotic software. The approaches for lowering the entry barrier are evaluated by usage statistics, a user survey, and compared against our design objectives for their effectiveness to users.},
  langid = {english},
  annotation = {Accepted: 2017-06-16T16:10:40Z},
  file = {G:\My Drive\03 Resources\Literature\2014 - Reducing the Barrier to Entry of Complex Robotic Software a MoveIt! Case Study - Coleman et al..pdf}
}

@article{2020_CorroborativeApproach_webster,
  title = {A Corroborative Approach to Verification and Validation of Human--Robot Teams},
  author = {Webster, Matt and Western, David and {Araiza-Illan}, Dejanira and Dixon, Clare and Eder, Kerstin and Fisher, Michael and Pipe, Anthony G},
  year = 2020,
  month = jan,
  journal = {The International Journal of Robotics Research},
  volume = {39},
  number = {1},
  pages = {73--99},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/0278364919883338},
  urldate = {2026-01-26},
  abstract = {We present an approach for the verification and validation (V\&V) of robot assistants in the context of human--robot interactions, to demonstrate their trustworthiness through corroborative evidence of their safety and functional correctness. Key challenges include the complex and unpredictable nature of the real world in which assistant and service robots operate, the limitations on available V\&V techniques when used individually, and the consequent lack of confidence in the V\&V results. Our approach, called corroborative V\&V, addresses these challenges by combining several different V\&V techniques; in this paper we use formal verification (model checking), simulation-based testing, and user validation in experiments with a real robot. This combination of approaches allows V\&V of the human--robot interaction task at different levels of modeling detail and thoroughness of exploration, thus overcoming the individual limitations of each technique. We demonstrate our approach through a handover task, the most critical part of a complex cooperative manufacturing scenario, for which we propose safety and liveness requirements to verify and validate. Should the resulting V\&V evidence present discrepancies, an iterative process between the different V\&V techniques takes place until corroboration between the V\&V techniques is gained from refining and improving the assets (i.e., system and requirement models) to represent the human--robot interaction task in a more truthful manner. Therefore, corroborative V\&V affords a systematic approach to ``meta-V\&V,'' in which different V\&V techniques can be used to corroborate and check one another, increasing the level of certainty in the results of V\&V.},
  langid = {english},
  file = {G:\My Drive\03 Resources\Literature\2020 - A corroborative approach to verification and validation of human–robot teams - Webster et al..pdf}
}

@inproceedings{2020_MonteCarloPlanning_jang,
  title = {Monte-{{Carlo Planning}} and {{Learning}} with {{Language Action Value Estimates}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Jang, Youngsoo and Seo, Seokin and Lee, Jongmin and Kim, Kee-Eung},
  year = 2020,
  month = oct,
  urldate = {2026-01-26},
  abstract = {Interactive Fiction (IF) games provide a useful testbed for language-based reinforcement learning agents, posing significant challenges of natural language understanding, commonsense reasoning, and non-myopic planning in the combinatorial search space. Agents based on standard planning algorithms struggle to play IF games due to the massive search space of language actions. Thus, language-grounded planning is a key ability of such agents, since inferring the consequence of language action based on semantic understanding can drastically improve search. In this paper, we introduce Monte-Carlo planning with Language Action Value Estimates (MC-LAVE) that combines a Monte-Carlo tree search with language-driven exploration. MC-LAVE invests more search effort into semantically promising language actions using locally optimistic language value estimates, yielding a significant reduction in the effective search space of language actions. We then present a reinforcement learning approach via MC-LAVE, which alternates between MC-LAVE planning and supervised learning of the self-generated language actions. In the experiments, we demonstrate that our method achieves new high scores in various IF games.},
  langid = {english},
  file = {G:\My Drive\03 Resources\Literature\2020 - Monte-Carlo Planning and Learning with Language Action Value Estimates - Jang et al..pdf}
}

@inproceedings{2022_LanguageModels_huang,
  title = {Language {{Models}} as {{Zero-Shot Planners}}: {{Extracting Actionable Knowledge}} for {{Embodied Agents}}},
  shorttitle = {Language {{Models}} as {{Zero-Shot Planners}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  year = 2022,
  month = jun,
  pages = {9118--9147},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2026-01-26},
  abstract = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. ``make breakfast''), to a chosen set of actionable steps (e.g. ``open fridge''). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models.},
  langid = {english},
  file = {G:\My Drive\03 Resources\Literature\2022 - Language Models as Zero-Shot Planners Extracting Actionable Knowledge for Embodied Agents - Huang et al..pdf}
}

@inproceedings{2022_PreTrainedLanguage_li,
  title = {Pre-{{Trained Language Models}} for {{Interactive Decision-Making}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Shuang and Puig, Xavier and Paxton, Chris and Du, Yilun and Wang, Clinton and Fan, Linxi and Chen, Tao and Huang, De-An and Aky{\"u}rek, Ekin and Anandkumar, Anima and Andreas, Jacob and Mordatch, Igor and Torralba, Antonio and Zhu, Yuke},
  year = 2022,
  month = oct,
  urldate = {2026-01-26},
  abstract = {Language model (LM) pre-training is useful in many language processing tasks. But can pre-trained LMs be further leveraged for more general machine learning problems? We propose an approach for using LMs to scaffold learning and generalization in general sequential decision-making problems. In this approach, goals and observations are represented as a sequence of embeddings, and a policy network initialized with a pre-trained LM predicts the next action. We demonstrate that this framework enables effective combinatorial generalization across different environments and supervisory modalities. We begin by assuming access to a set of expert demonstrations, and show that initializing policies with LMs and fine-tuning them via behavior cloning improves task completion rates by 43.6\% in the VirtualHome environment. Next, we integrate an active data gathering procedure in which agents iteratively interact with the environment, relabel past "failed" experiences with new goals, and update their policies in a self-supervised loop. Active data gathering further improves combinatorial generalization, outperforming the best baseline by 25.1\%. Finally, we explain these results by investigating three possible factors underlying the effectiveness of the LM-based policy. We find that sequential input representations (vs. fixed-dimensional feature vectors) and LM-based weight initialization are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g. as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans; these representations can aid learning and generalization even outside of language processing.},
  langid = {english},
  file = {G:\My Drive\03 Resources\Literature\2022 - Pre-Trained Language Models for Interactive Decision-Making - Li et al..pdf}
}

@article{2022_RobotOperating_macenski,
  title = {Robot {{Operating System}} 2: {{Design}}, Architecture, and Uses in the Wild},
  shorttitle = {Robot {{Operating System}} 2},
  author = {Macenski, Steven and Foote, Tully and Gerkey, Brian and Lalancette, Chris and Woodall, William},
  year = 2022,
  month = may,
  journal = {Science Robotics},
  volume = {7},
  number = {66},
  pages = {eabm6074},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/scirobotics.abm6074},
  urldate = {2026-01-20},
  abstract = {The next chapter of the robotics revolution is well underway with the deployment of robots for a broad range of commercial use cases. Even in a myriad of applications and environments, there exists a common vocabulary of components that robots share---the need for a modular, scalable, and reliable architecture; sensing; planning; mobility; and autonomy. The Robot Operating System (ROS) was an integral part of the last chapter, demonstrably expediting robotics research with freely available components and a modular framework. However, ROS 1 was not designed with many necessary production-grade features and algorithms. ROS 2 and its related projects have been redesigned from the ground up to meet the challenges set forth by modern robotic systems in new and exploratory domains at all scales. In this Review, we highlight the philosophical and architectural changes of ROS 2 powering this new chapter in the robotics revolution. We also show through case studies the influence ROS 2 and its adoption has had on accelerating real robot systems to reliable deployment in an assortment of challenging environments.},
  file = {G:\My Drive\03 Resources\Literature\2022 - Robot Operating System 2 Design, architecture, and uses in the wild - Macenski et al. 1.pdf}
}

@article{2022_Ros2_tracingMultipurpose_bedard,
  title = {Ros2\_tracing: {{Multipurpose Low-Overhead Framework}} for {{Real-Time Tracing}} of {{ROS}} 2},
  shorttitle = {Ros2\_tracing},
  author = {B{\'e}dard, Christophe and L{\"u}tkebohle, Ingo and Dagenais, Michel},
  year = 2022,
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {3},
  pages = {6511--6518},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3174346},
  urldate = {2026-01-25},
  abstract = {Testing and debugging have become major obstacles for robot software development, because of high system complexity and dynamic environments. Standard, middleware-based data recording does not provide sufficient information on internal computation and performance bottlenecks. Other existing methods also target very specific problems and thus cannot be used for multipurpose analysis. Moreover, they are not suitable for real-time applications. In this letter, we present ros2\_tracing, a collection of flexible tracing tools and multipurpose instrumentation for ROS 2. It allows collecting runtime execution information on real-time distributed systems, using the low-overhead LTTng tracer. Tools also integrate tracing into the invaluable ROS 2 orchestration system and other usability tools. A message latency experiment shows that the end-to-end message latency overhead, when enabling all ROS 2 instrumentation, is on average 0.0033 ms, which we believe is suitable for production real-time systems. ROS 2 execution information obtained using ros2\_tracing can be combined with trace data from the operating system, enabling a wider range of precise analyses, that help understand an application execution, to find the cause of performance bottlenecks and other issues.},
  keywords = {Codes,distributed robot systems,Instruments,Libraries,Middleware,performance analysis,Real-time systems,Robot Operating System (ROS),Robots,Runtime,Software tools for robot programming,tracing},
  annotation = {50 citations (Semantic Scholar/DOI) [2026-01-25]},
  file = {G:\My Drive\03 Resources\Literature\2022 - ros2_tracing Multipurpose Low-Overhead Framework for Real-Time Tracing of ROS 2 - Bédard et al..pdf}
}

@article{2023_AutoRTEmbodied_ahn,
  title = {{{AutoRT}}: {{Embodied Foundation Models}} for {{Large Scale Orchestration}} of {{Robotic Agents}}},
  shorttitle = {{{AutoRT}}},
  author = {Ahn, Michael and Dwibedi, Debidatta and Finn, Chelsea and Arenas, Montserrat Gonzalez and Gopalakrishnan, Keerthana and Hausman, Karol and Ichter, Brian and Irpan, Alex and Joshi, Nikhil J. and Julian, Ryan and Kirmani, Sean and Leal, Isabel and Lee, Tsang-Wei Edward and Levine, Sergey and Lu, Yao and Maddineni, Sharath and Rao, Kanishka and Sadigh, Dorsa and Sanketi, Pannag R. and Sermanet, Pierre and Vuong, Quan and Welker, Stefan and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Xu, Zhuo},
  year = 2023,
  month = oct,
  urldate = {2026-01-30},
  abstract = {Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such ``in-the-wild'' data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that are aligned with human preferences.},
  langid = {english},
  file = {G:\My Drive\03 Resources\Literature\2023 - AutoRT Embodied Foundation Models for Large Scale Orchestration of Robotic Agents - Ahn et al..pdf}
}

@inproceedings{2023_CodePolicies_liang,
  title = {Code as {{Policies}}: {{Language Model Programs}} for {{Embodied Control}}},
  shorttitle = {Code as {{Policies}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  year = 2023,
  month = may,
  pages = {9493--9500},
  doi = {10.1109/ICRA48891.2023.10160591},
  urldate = {2026-02-16},
  abstract = {Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (`faster') depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
  keywords = {Codes,Detectors,Feedback loop,Impedance,Libraries,Natural languages,Process control},
  annotation = {1337 citations (Semantic Scholar/DOI) [2026-02-18]},
  file = {G:\My Drive\03 Resources\Literature\2023 - Code as Policies Language Model Programs for Embodied Control - Liang et al..pdf}
}

@inproceedings{2023_CuRoboParallelized_sundaralingam,
  title = {{{CuRobo}}: {{Parallelized Collision-Free Robot Motion Generation}}},
  shorttitle = {{{CuRobo}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Sundaralingam, Balakumar and Hari, Siva Kumar Sastry and Fishman, Adam and Garrett, Caelan and Van Wyk, Karl and Blukis, Valts and Millane, Alexander and Oleynikova, Helen and Handa, Ankur and Ramos, Fabio and Ratliff, Nathan and Fox, Dieter},
  year = 2023,
  month = may,
  pages = {8112--8119},
  doi = {10.1109/ICRA48891.2023.10160765},
  urldate = {2026-01-25},
  abstract = {This paper explores the problem of collision-free motion generation for manipulators by formulating it as a global motion optimization problem. We develop a parallel optimization technique to solve this problem and demonstrate its effectiveness on massively parallel GPUs. We show that combining simple optimization techniques with many parallel seeds leads to solving difficult motion generation problems within 53ms on average, 62x faster than SOTA trajectory optimization methods. We achieve SOTA performance by combining L-BFGS step direction estimation with a novel parallel noisy line search scheme and a particle-based optimization solver. To further aid trajectory optimization, we develop a parallel geometric planner that is atleast 28x faster than SOTA RRTConnect implementations. We also introduce a collision-free IK solver that can solve over 9000 queries/s. We are releasing our GPU accelerated library CuRobo that contains core components for robot motion generation. Additional details are available at sites.google.com/nvidia.com/curobo.},
  keywords = {Automation,Estimation,Graphics processing units,Libraries,Manipulators,Planning,Robot motion},
  annotation = {128 citations (Semantic Scholar/DOI) [2026-01-25]},
  file = {G:\My Drive\03 Resources\Literature\2023 - CuRobo Parallelized Collision-Free Robot Motion Generation - Sundaralingam et al..pdf}
}

@inproceedings{2023_EmbodiedGPTVisionlanguage_mu,
  title = {{{EmbodiedGPT}}: Vision-Language Pre-Training via Embodied Chain of Thought},
  shorttitle = {{{EmbodiedGPT}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
  year = 2023,
  month = dec,
  series = {{{NIPS}} '23},
  pages = {25081--25094},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2026-01-26},
  abstract = {Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments. In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the "Chain of Thoughts" mode for effective embodied planning. (ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated planning queries to form a closed loop between high-level planning and low-level control. Extensive experiments show the effectiveness of EmbodiedGPT on embodied tasks, including embodied planning, embodied control, visual captioning, and visual question answering. Notably, EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset. More demos, code, and dataset information can be found at our https://embodiedgpt.github.io/.},
  file = {G:\My Drive\03 Resources\Literature\2023 - EmbodiedGPT vision-language pre-training via embodied chain of thought - Mu et al..pdf}
}

@misc{2023_HDDL21_pellier,
  title = {{{HDDL}} 2.1: {{Towards Defining}} a {{Formalism}} and a {{Semantics}} for {{Temporal HTN Planning}}},
  shorttitle = {{{HDDL}} 2.1},
  author = {Pellier, Damien and Albore, Alexandre and Fiorino, Humbert and {Bailon-Ruiz}, Rafael},
  year = 2023,
  month = jun,
  number = {arXiv:2306.07353},
  eprint = {2306.07353},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.07353},
  urldate = {2026-01-26},
  abstract = {Real world applications as in industry and robotics need modelling rich and diverse automated planning problems. Their resolution usually requires coordinated and concurrent action execution. In several cases, these problems are naturally decomposed in a hierarchical way and expressed by a Hierarchical Task Network (HTN) formalism. HDDL, a hierarchical extension of the Planning Domain Definition Language (PDDL), unlike PDDL 2.1 does not allow to represent planning problems with numerical and temporal constraints, which are essential for real world applications. We propose to fill the gap between HDDL and these operational needs and to extend HDDL by taking inspiration from PDDL 2.1 in order to express numerical and temporal expressions. This paper opens discussions on the semantics and the syntax needed for a future HDDL 2.1 extension.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {4 citations (Semantic Scholar/arXiv) [2026-01-26]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2023 - HDDL 2.1 Towards Defining a Formalism and a Semantics for Temporal HTN Planning - Pellier et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\4XAT77FW\\2306.html}
}

@inproceedings{2023_HowFast_betz,
  title = {How {{Fast}} Is {{My Software}}? {{Latency Evaluation}} for a {{ROS}} 2 {{Autonomous Driving Software}}},
  shorttitle = {How {{Fast}} Is {{My Software}}?},
  booktitle = {2023 {{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  author = {Betz, Tobias and Schmeller, Maximilian and Teper, Harun and Betz, Johannes},
  year = 2023,
  month = jun,
  pages = {1--6},
  issn = {2642-7214},
  doi = {10.1109/IV55152.2023.10186585},
  urldate = {2026-01-25},
  abstract = {Violations of real-time properties and high latencies have emerged as crucial issues in autonomous vehicles since they can lead to unwanted vehicle behavior and critical maneuvers. Our study aims to provide a comprehensive understanding of latencies in a software stack for autonomous vehicles. In this paper, we present an evaluation workflow to inspect software and the occurring latencies for ROS 2 applications. This workflow was used to analyze the open-source autonomous driving stack Autoware. Universe by showing the influence of different soft- and hardware configurations. Our focus is on the evaluation of end-to-end, communication, computation, and idle latencies. Based on the results, we show the bottlenecks and motivate future directions to optimize ROS 2 autonomous driving software.},
  keywords = {Autonomous vehicles,Behavioral sciences,Full stack,Hardware,Real-time systems,Software},
  annotation = {16 citations (Semantic Scholar/DOI) [2026-01-25]},
  file = {G:\My Drive\03 Resources\Literature\2023 - How Fast is My Software Latency Evaluation for a ROS 2 Autonomous Driving Software - Betz et al..pdf}
}

@inproceedings{2023_LargeLanguage_zhao,
  title = {Large Language Models as Commonsense Knowledge for Large-Scale Task Planning},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Zhao, Zirui and Lee, Wee Sun and Hsu, David},
  year = 2023,
  month = dec,
  series = {{{NIPS}} '23},
  pages = {31967--31987},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2026-01-26},
  abstract = {Large-scale task planning is a major challenge. Recent work exploits large language models (LLMs) directly as a policy and shows surprisingly interesting results. This paper shows that LLMs provide a commonsense model of the world in addition to a policy that acts on it. The world model and the policy can be combined in a search algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task planning. In our new LLM-MCTS algorithm, the LLM-induced world model provides a commonsense prior belief for MCTS to achieve effective reasoning; the LLM-induced policy acts as a heuristic to guide the search, vastly improving search efficiency. Experiments show that LLM-MCTS outperforms both MCTS alone and policies induced by LLMs (GPT2 and GPT3.5) by a wide margin for complex, novel tasks. Further experiments and analyses on multiple tasks---multiplication, travel planning, object rearrangement---suggest minimum description length (MDL) as a general guiding principle: if the description length of the world model is substantially smaller than that of the policy, using LLM as a world model for model-based planning is likely better than using LLM solely as a policy. The code and supplementary materials are available at https://llm-mcts.github.io.},
  file = {G:\My Drive\03 Resources\Literature\2023 - Large Language Models as Commonsense Knowledge for Large-Scale Task Planning - Zhao et al..pdf}
}

@article{2023_LEMMALearning_gong,
  title = {{{LEMMA}}: {{Learning Language-Conditioned Multi-Robot Manipulation}}},
  shorttitle = {{{LEMMA}}},
  author = {Gong, Ran and Gao, Xiaofeng and Gao, Qiaozi and Shakiah, Suhaila and Thattai, Govind and Sukhatme, Gaurav S.},
  year = 2023,
  month = oct,
  journal = {IEEE Robotics and Automation Letters},
  volume = {8},
  number = {10},
  pages = {6835--6842},
  issn = {2377-3766},
  doi = {10.1109/LRA.2023.3313058},
  urldate = {2026-02-16},
  abstract = {Complex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for LanguagE-Conditioned Multi-robot MAnipulation (LEMMA) focused on task allocation and long-horizon object manipulation based on human language instructions in a tabletop setting. LEMMA features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. LEMMA poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of LEMMA for developing future language-conditioned multi-robot systems.},
  keywords = {Benchmark testing,Collaboration,Data Sets for Robot Learning,Multi-robot systems,Multi-Robot Systems,Multitasking,Natural Dialog for HRI,Planning,Robot kinematics,Robots,Task analysis},
  annotation = {14 citations (Semantic Scholar/DOI) [2026-02-18]\\
14 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2023 - LEMMA Learning Language-Conditioned Multi-Robot Manipulation - Gong et al..pdf}
}

@misc{2023_LLMMARSLarge_lykov,
  title = {{{LLM-MARS}}: {{Large Language Model}} for {{Behavior Tree Generation}} and {{NLP-enhanced Dialogue}} in {{Multi-Agent Robot Systems}}},
  shorttitle = {{{LLM-MARS}}},
  author = {Lykov, Artem and Dronova, Maria and Naglov, Nikolay and Litvinov, Mikhail and Satsevich, Sergei and Bazhenov, Artem and Berman, Vladimir and Shcherbak, Aleksei and Tsetserukou, Dzmitry},
  year = 2023,
  month = dec,
  number = {arXiv:2312.09348},
  eprint = {2312.09348},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.09348},
  urldate = {2026-01-30},
  abstract = {This paper introduces LLM-MARS, first technology that utilizes a Large Language Model based Artificial Intelligence for Multi-Agent Robot Systems. LLM-MARS enables dynamic dialogues between humans and robots, allowing the latter to generate behavior based on operator commands and provide informative answers to questions about their actions. LLM-MARS is built on a transformer-based Large Language Model, fine-tuned from the Falcon 7B model. We employ a multimodal approach using LoRa adapters for different tasks. The first LoRa adapter was developed by fine-tuning the base model on examples of Behavior Trees and their corresponding commands. The second LoRa adapter was developed by fine-tuning on question-answering examples. Practical trials on a multi-agent system of two robots within the Eurobot 2023 game rules demonstrate promising results. The robots achieve an average task execution accuracy of 79.28\% in compound commands. With commands containing up to two tasks accuracy exceeded 90\%. Evaluation confirms the system's answers on operators questions exhibit high accuracy, relevance, and informativeness. LLM-MARS and similar multi-agent robotic systems hold significant potential to revolutionize logistics, enabling autonomous exploration missions and advancing Industry 5.0.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  annotation = {24 citations (Semantic Scholar/DOI) [2026-01-30]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2023 - LLM-MARS Large Language Model for Behavior Tree Generation and NLP-enhanced Dialogue in Multi-Agent Robot Systems - Lykov et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\FMEFM5EW\\2312.html}
}

@inproceedings{2023_NovelROS2_dey,
  title = {A {{Novel ROS2 QoS Policy-Enabled Synchronizing Middleware}} for {{Co-Simulation}} of {{Heterogeneous Multi-Robot Systems}}},
  booktitle = {2023 32nd {{International Conference}} on {{Computer Communications}} and {{Networks}} ({{ICCCN}})},
  author = {Dey, Emon and Walczak, Mikolaj and Anwar, Mohammad Saeid and Roy, Nirmalya and Freeman, Jade and Gregory, Timothy and Suri, Niranjan and Busart, Carl},
  year = 2023,
  month = jul,
  pages = {1--10},
  issn = {2637-9430},
  doi = {10.1109/ICCCN58024.2023.10230109},
  urldate = {2026-01-25},
  abstract = {Recent Internet-of-Things (IoT) networks span across a multitude of stationary and robotic devices, namely unmanned ground vehicles, surface vessels, and aerial drones, to carry out mission-critical services such as search and rescue operations, wildfire monitoring, and flood/hurricane impact assessment. Achieving communication synchrony, reliability, and minimal communication jitter among these devices is a key challenge both at the simulation and system levels of implementation due to the underpinning differences between a physics-based robot operating system (ROS) simulator that is time-based and a network-based wireless simulator that is event-based, in addition to the complex dynamics of mobile and heterogeneous IoT devices deployed in a real environment. Nevertheless, synchronization between physics (robotics) and network simulators is one of the most difficult issues to address in simulating a heterogeneous multi-robot system before transitioning it into practice. The existing TCP/IP communication protocol-based synchronizing middleware mostly relied on Robot Operating System 1 (ROS1), which expends a significant portion of communication bandwidth and time due to its master-based architecture. To address these issues, we design a novel synchronizing middleware between robotics and traditional wireless network simulators, relying on the newly released real-time ROS2 architecture with a masterless packet discovery mechanism. Additionally, we propose a ground and aerial agents' velocity-aware customized QoS policy for Data Distribution Service (DDS) to minimize the packet loss and transmission latency between a diverse set of robotic agents, and we offer the theoretical guarantee of our proposed QoS policy. We performed extensive network performance evaluations both at the simulation and system levels in terms of packet loss probability and average latency with line-of-sight (LOS) and non-line-of-sight (NLOS) and TCP/UDP communication protocols over our proposed ROS2-based synchronization middleware. Moreover, for a comparative study, we presented a detailed ablation study replacing NS-3 with a real-time wireless network simulator, EMANE, and masterless ROS2 with master-based ROS1. Our proposed middleware attests to the promise of building a large-scale IoT infrastructure with a diverse set of stationary and robotic devices that achieve low-latency communications (12\% and 11\% reduction in simulation and reality, respectively) while satisfying the reliability (10\% and 15\% packet loss reduction in simulation and reality, respectively) and high-fidelity requirements of mission-critical applications.},
  keywords = {EMANE,Gazebo,Heterogeneous multi-robot systems,IoT,NS-3,Packet loss,Quality of service,Real-time systems,Reliability,Synchronization,TCP,TCPIP,UDP,Windows,Wireless networks},
  annotation = {7 citations (Semantic Scholar/DOI) [2026-01-25]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2023 - A Novel ROS2 QoS Policy-Enabled Synchronizing Middleware for Co-Simulation of Heterogeneous Multi-Robot Systems - Dey et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\L4L9ITUQ\\10230109.html}
}

@inproceedings{2023_ProgPromptGenerating_singh,
  title = {{{ProgPrompt}}: {{Generating Situated Robot Task Plans}} Using {{Large Language Models}}},
  shorttitle = {{{ProgPrompt}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  year = 2023,
  month = may,
  pages = {11523--11530},
  doi = {10.1109/ICRA48891.2023.10161317},
  urldate = {2026-02-16},
  abstract = {Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io},
  keywords = {Automation,Manipulators,Natural languages,Planning,Task analysis},
  annotation = {886 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2023 - ProgPrompt Generating Situated Robot Task Plans using Large Language Models - Singh et al..pdf}
}

@article{2023_RobotOperating_bonci,
  title = {Robot {{Operating System}} 2 ({{ROS2}})-{{Based Frameworks}} for {{Increasing Robot Autonomy}}: {{A Survey}}},
  shorttitle = {Robot {{Operating System}} 2 ({{ROS2}})-{{Based Frameworks}} for {{Increasing Robot Autonomy}}},
  author = {Bonci, Andrea and Gaudeni, Francesco and Giannini, Maria Cristina and Longhi, Sauro},
  year = 2023,
  month = jan,
  journal = {Applied Sciences},
  volume = {13},
  number = {23},
  pages = {12796},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app132312796},
  urldate = {2026-01-26},
  abstract = {Future challenges in manufacturing will require automation systems with robots that are increasingly autonomous, flexible, and hopefully equipped with learning capabilities. The flexibility of production processes can be increased by using a combination of a flexible human worker and intelligent automation systems. The adoption of middleware software such as ROS2, the second generation of the Robot Operating System, can enable robots, automation systems, and humans to work together on tasks that require greater autonomy and flexibility. This paper has a twofold objective. Firstly, it provides an extensive review of existing literature on the features and tools currently provided by ROS2 and its main fields of application, in order to highlight the enabling aspects for the implementation of modular architectures to increase autonomy in industrial operations. Secondly, it shows how this is currently potentially feasible in ROS2 by proposing a possible high-level and modular architecture to increase autonomy in industrial operations. A proof of concept is also provided, where the ROS2-based framework is used to enable a cobot equipped with an external depth camera to perform a flexible pick-and-place task.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomy,collaborative robotics,flexibility,industrial robotics,Robot Operating System (ROS),Robot Operating System 2 (ROS2)},
  annotation = {40 citations (Semantic Scholar/DOI) [2026-01-26]},
  file = {G:\My Drive\03 Resources\Literature\2023 - Robot Operating System 2 (ROS2)-Based Frameworks for Increasing Robot Autonomy A Survey - Bonci et al..pdf}
}

@inproceedings{2023_TaskMotion_ding,
  title = {Task and {{Motion Planning}} with {{Large Language Models}} for {{Object Rearrangement}}},
  booktitle = {2023 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Ding, Yan and Zhang, Xiaohan and Paxton, Chris and Zhang, Shiqi},
  year = 2023,
  month = oct,
  pages = {2086--2092},
  issn = {2153-0866},
  doi = {10.1109/IROS55552.2023.10342169},
  urldate = {2026-02-16},
  abstract = {Multi-object rearrangement is a crucial skill for service robots, and commonsense reasoning is frequently needed in this process. However, achieving commonsense arrangements requires knowledge about objects, which is hard to transfer to robots. Large language models (LLMs) are one potential source of this knowledge, but they do not naively capture information about plausible physical arrangements of the world. We propose LLM-GROP, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry. LLM-GROP allows us to go from natural-language commands to human-aligned object rearrangement in varied environments. Based on human evaluations, our approach achieves the highest rating while outperforming competitive baselines in terms of success rate while maintaining comparable cumulative action costs. Finally, we demonstrate a practical implementation of LLM-GROP on a mobile manipulator in real-world scenarios. Supplementary materials are available at: https://sites.google.com/view/llm-grop},
  keywords = {Commonsense reasoning,Costs,Geometry,Manipulators,Planning,Service robots,Task analysis},
  annotation = {231 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2023 - Task and Motion Planning with Large Language Models for Object Rearrangement - Ding et al..pdf}
}

@article{2024_ApplyingLarge_zhao,
  title = {Applying {{Large Language Model}} to a {{Control System}} for {{Multi-Robot Task Assignment}}},
  author = {Zhao, Wen and Li, Liqiao and Zhan, Hanwen and Wang, Yingqi and Fu, Yiqi},
  year = 2024,
  month = dec,
  journal = {Drones},
  volume = {8},
  number = {12},
  pages = {728},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2504-446X},
  doi = {10.3390/drones8120728},
  urldate = {2026-01-26},
  abstract = {The emergence of large language models (LLMs), such as GPT (Generative Pre-trained Transformer), has had a profound impact and brought about significant changes across various sectors of human society. Integrating GPT-3.5 into a multi-robot control system, termed MultiBotGPT (Multi-Robot Control System with GPT), represents a notable application. This system utilizes layered architecture and modular design to translate natural language commands into executable tasks for UAVs (Unmanned Aerial Vehicles) and UGVs (Unmanned Ground Vehicles), enhancing capabilities in tasks such as target search and navigation. Comparative experiments with BERT (Bidirectional Encoder Representations from Transformers) in the natural language-processing component show that MultiBotGPT with GPT-3.5 achieves superior task success rates (94.4\% and 55.0\%) across 50 experiments, outperforming BERT significantly. In order to test the auxiliary role of the MultiBotGPT-controlled robot on a human operator, we invited 30 volunteers to participate in our comparative experiments. Three separate experiments were performed, Participant Control (Manual Control only), Mix Control (Mix Manual Contr and MultiBotGPT Control), and MultiBotGPT Control (MultiBotGPT Control only). The performance of MultiBotGPT is recognized by the human operators and it can reduce the mental and physical consumption of the human operators through the scoring of the participants' questionnaires.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {control system,human-robot collaboration,large language model,multi-robot},
  file = {G:\My Drive\03 Resources\Literature\2024 - Applying Large Language Model to a Control System for Multi-Robot Task Assignment - Zhao et al..pdf}
}

@article{2024_ArtificialIntelligence_kreuzer,
  title = {Artificial Intelligence in Digital Twins---{{A}} Systematic Literature Review},
  author = {Kreuzer, Tim and Papapetrou, Panagiotis and Zdravkovic, Jelena},
  year = 2024,
  month = may,
  journal = {Data \& Knowledge Engineering},
  volume = {151},
  pages = {102304},
  issn = {0169-023X},
  doi = {10.1016/j.datak.2024.102304},
  urldate = {2026-01-25},
  abstract = {Artificial intelligence and digital twins have become more popular in recent years and have seen usage across different application domains for various scenarios. This study reviews the literature at the intersection of the two fields, where digital twins integrate an artificial intelligence component. We follow a systematic literature review approach, analyzing a total of 149 related studies. In the assessed literature, a variety of problems are approached with an artificial intelligence-integrated digital twin, demonstrating its applicability across different fields. Our findings indicate that there is a lack of in-depth modeling approaches regarding the digital twin, while many articles focus on the implementation and testing of the artificial intelligence component. The majority of publications do not demonstrate a virtual-to-physical connection between the digital twin and the real-world system. Further, only a small portion of studies base their digital twin on real-time data from a physical system, implementing a physical-to-virtual connection.},
  keywords = {Artificial intelligence,Business intelligence,Data mining,Digital twin,Literature review,Machine learning},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2024 - Artificial intelligence in digital twins—A systematic literature review - Kreuzer et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\V7QWJUA7\\S0169023X24000284.html}
}

@inproceedings{2024_AutoTAMPAutoregressive_chen,
  title = {{{AutoTAMP}}: {{Autoregressive Task}} and {{Motion Planning}} with {{LLMs}} as {{Translators}} and {{Checkers}}},
  shorttitle = {{{AutoTAMP}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Chen, Yongchao and Arkin, Jacob and Dawson, Charles and Zhang, Yang and Roy, Nicholas and Fan, Chuchu},
  year = 2024,
  month = may,
  pages = {6695--6702},
  doi = {10.1109/ICRA57147.2024.10611163},
  urldate = {2026-02-16},
  abstract = {For effective human-robot interaction, robots need to understand, plan, and execute complex, long-horizon tasks described by natural language. Recent advances in large language models (LLMs) have shown promise for translating natural language into robot action sequences for complex tasks. However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal. When complex environmental and temporal constraints are involved, inference over planning tasks must be performed jointly with motion plans using traditional task-and-motion planning (TAMP) algorithms, making factorization into subgoals untenable. Rather than using LLMs to directly plan task sub-goals, we instead perform few-shot translation from natural language task descriptions to an intermediate task representation that can then be consumed by a TAMP algorithm to jointly solve the task and motion plan. To improve translation, we automatically detect and correct both syntactic and semantic errors via autoregressive re-prompting, resulting in significant improvements in task completion. We show that our approach outperforms several methods using LLMs as planners in complex task domains. See our project website\S{} for prompts, videos, and code.},
  keywords = {Market research,Optimization methods,Planning,Runtime,Semantics,Syntactics,Trajectory},
  annotation = {159 citations (Semantic Scholar/DOI) [2026-02-18]},
  file = {G:\My Drive\03 Resources\Literature\2024 - AutoTAMP Autoregressive Task and Motion Planning with LLMs as Translators and Checkers - Chen et al..pdf}
}

@misc{2024_BEHAVIOR1KHumanCentered_li,
  title = {{{BEHAVIOR-1K}}: {{A Human-Centered}}, {{Embodied AI Benchmark}} with 1,000 {{Everyday Activities}} and {{Realistic Simulation}}},
  shorttitle = {{{BEHAVIOR-1K}}},
  author = {Li, Chengshu and Zhang, Ruohan and Wong, Josiah and Gokmen, Cem and Srivastava, Sanjana and {Mart{\'i}n-Mart{\'i}n}, Roberto and Wang, Chen and Levine, Gabrael and Ai, Wensi and Martinez, Benjamin and Yin, Hang and Lingelbach, Michael and Hwang, Minjune and Hiranaka, Ayano and Garlanka, Sujay and Aydin, Arman and Lee, Sharon and Sun, Jiankai and Anvari, Mona and Sharma, Manasi and Bansal, Dhruva and Hunter, Samuel and Kim, Kyu-Young and Lou, Alan and Matthews, Caleb R. and {Villa-Renteria}, Ivan and Tang, Jerry Huayang and Tang, Claire and Xia, Fei and Li, Yunzhu and Savarese, Silvio and Gweon, Hyowon and Liu, C. Karen and Wu, Jiajun and {Fei-Fei}, Li},
  year = 2024,
  month = mar,
  number = {arXiv:2403.09227},
  eprint = {2403.09227},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.09227},
  urldate = {2026-01-26},
  abstract = {We present BEHAVIOR-1K, a comprehensive simulation benchmark for human-centered robotics. BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on "what do you want robots to do for you?". The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 9,000 objects annotated with rich physical and semantic properties. The second is OMNIGIBSON, a novel simulation environment that supports these activities via realistic physics simulation and rendering of rigid bodies, deformable bodies, and liquids. Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions. To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an initial study on transferring solutions learned with a mobile manipulator in a simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's human-grounded nature, diversity, and realism make it valuable for embodied AI and robot learning research. Project website: https://behavior.stanford.edu.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2024 - BEHAVIOR-1K A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation - Li et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\2IXZMJ82\\2403.html}
}

@inproceedings{2024_BenchmarkingMultiRobot_heuer,
  title = {Benchmarking {{Multi-Robot Coordination}} in {{Realistic}}, {{Unstructured Human-Shared Environments}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Heuer, Lukas and Palmieri, Luigi and Mannucci, Anna and Koenig, Sven and Magnusson, Martin},
  year = 2024,
  month = may,
  pages = {14541--14547},
  doi = {10.1109/ICRA57147.2024.10611005},
  urldate = {2026-01-26},
  abstract = {Coordinating a fleet of robots in unstructured, human-shared environments is challenging. Human behavior is hard to predict, and its uncertainty impacts the performance of the robotic fleet. Various multi-robot planning and coordination algorithms have been proposed, including Multi-Agent Path Finding (MAPF) methods to precedence-based algorithms. However, it is still unclear how human presence impacts different coordination strategies in both simulated environments and the real world. With the goal of studying and further improving multi-robot planning capabilities in those settings, we propose a method to develop and benchmark different multi-robot coordination algorithms in realistic, unstructured and human-shared environments. To this end, we introduce a multi-robot benchmark framework that is based on state-of-the-art open-source navigation and simulation frameworks and can use different types of robots, environments and human motion models. We show a possible application of the benchmark framework with two different environments and three centralized coordination methods (two MAPF algorithms and a loosely-coupled coordination method based on precedence constraints). We evaluate each environment for different human densities to investigate its impact on each coordination method. We also present preliminary results that show how informing each coordination method about human presence can help the coordination method to find faster paths for the robots.},
  keywords = {Benchmark testing,Navigation,Prediction algorithms,Robot kinematics,Software,Software algorithms,Uncertainty},
  file = {G:\My Drive\03 Resources\Literature\2024 - Benchmarking Multi-Robot Coordination in Realistic, Unstructured Human-Shared Environments - Heuer et al..pdf}
}

@article{2024_ExploringGPTbased_lakhnati,
  title = {Exploring a {{GPT-based}} Large Language Model for Variable Autonomy in a {{VR-based}} Human-Robot Teaming Simulation},
  author = {Lakhnati, Younes and Pascher, Max and Gerken, Jens},
  year = 2024,
  month = apr,
  journal = {Frontiers in Robotics and AI},
  volume = {11},
  publisher = {Frontiers},
  issn = {2296-9144},
  doi = {10.3389/frobt.2024.1347538},
  urldate = {2026-01-26},
  abstract = {In a rapidly evolving digital landscape autonomous tools and robots are becoming commonplace. Recognizing the significance of this development, this paper explores the integration of Large Language Models (LLMs) like Generative pre-trained transformer (GPT) into human-robot teaming environments to facilitate variable autonomy through the means of verbal human-robot communication. In this paper, we introduce a novel simulation framework for such a GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality (VR) setting. This system allows users to interact with simulated robot agents through natural language, each powered by individual GPT cores. By means of OpenAI's function calling, we bridge the gap between unstructured natural language input and structured robot actions. A user study with 12 participants explores the effectiveness of GPT-4 and, more importantly, user strategies when being given the opportunity to converse in natural language within a simulated multi-robot environment. Our findings suggest that users may have preconceived expectations on how to converse with robots and seldom try to explore the actual language and cognitive capabilities of their simulated robot collaborators. Still, those users who did explore where able to benefit from a much more natural flow of communication and human-like back-and-forth. We provide a set of lessons learned for future research and technical implementations of similar systems.},
  langid = {english},
  keywords = {Assistive Robots,Evaluation,shared control,Variable autonomy,virtual reality},
  file = {G:\My Drive\03 Resources\Literature\2024 - Exploring a GPT-based large language model for variable autonomy in a VR-based human-robot teaming simulation - Lakhnati et al..pdf}
}

@misc{2024_HPRMHighPerformance_kwok,
  title = {{{HPRM}}: {{High-Performance Robotic Middleware}} for {{Intelligent Autonomous Systems}}},
  shorttitle = {{{HPRM}}},
  author = {Kwok, Jacky and Li, Shulu and Lohstroh, Marten and Lee, Edward A.},
  year = 2024,
  month = dec,
  journal = {arXiv.org},
  urldate = {2026-01-25},
  abstract = {The rise of intelligent autonomous systems, especially in robotics and autonomous agents, has created a critical need for robust communication middleware that can ensure real-time processing of extensive sensor data. Current robotics middleware like Robot Operating System (ROS) 2 faces challenges with nondeterminism and high communication latency when dealing with large data across multiple subscribers on a multi-core compute platform. To address these issues, we present High-Performance Robotic Middleware (HPRM), built on top of the deterministic coordination language Lingua Franca (LF). HPRM employs optimizations including an in-memory object store for efficient zero-copy transfer of large payloads, adaptive serialization to minimize serialization overhead, and an eager protocol with real-time sockets to reduce handshake latency. Benchmarks show HPRM achieves up to 173x lower latency than ROS2 when broadcasting large messages to multiple nodes. We then demonstrate the benefits of HPRM by integrating it with the CARLA simulator and running reinforcement learning agents along with object detection workloads. In the CARLA autonomous driving application, HPRM attains 91.1\% lower latency than ROS2. The deterministic coordination semantics of HPRM, combined with its optimized IPC mechanisms, enable efficient and predictable real-time communication for intelligent autonomous systems.},
  howpublished = {https://arxiv.org/abs/2412.01799v1},
  langid = {english},
  annotation = {0 citations (Semantic Scholar/arXiv) [2026-01-25]},
  file = {G:\My Drive\03 Resources\Literature\2024 - HPRM High-Performance Robotic Middleware for Intelligent Autonomous Systems - Kwok et al..pdf}
}

@inproceedings{2024_LargeLanguage_chu,
  title = {Large {{Language Models}} for {{Orchestrating Bimanual Robots}}},
  booktitle = {2024 {{IEEE-RAS}} 23rd {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  author = {Chu, Kun and Zhao, Xufeng and Weber, Cornelius and Li, Mengdi and Lu, Wenhao and Wermter, Stefan},
  year = 2024,
  month = nov,
  pages = {328--334},
  issn = {2164-0580},
  doi = {10.1109/Humanoids58906.2024.10769891},
  urldate = {2026-02-16},
  abstract = {Although there has been rapid progress in endowing robots with the ability to solve complex manipulation tasks, generating control policies for bimanual robots to solve tasks involving two hands is still challenging because of the difficulties in effective temporal and spatial coordination. With emergent abilities in terms of step-by-step reasoning and in-context learning, Large Language Models (LLMs) have demonstrated promising potential in a variety of robotic tasks. However, the nature of language communication via a single sequence of discrete symbols makes LLM-based coordination in continuous space a particular challenge for bimanual tasks. To tackle this challenge, we present LAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM to analyze task configurations and devise coordination control policies for addressing long-horizon bimanual tasks. We evaluate our method through simulated experiments involving two classes of long-horizon tasks using the NICOL humanoid robot. Our results demonstrate that our method outperforms the baseline in terms of success rate. Additionally, we thoroughly analyze failure cases, offering insights into LLM-based approaches in bimanual robotic control and revealing future research trends. The project website can be found at http://labor-agent. github.io.},
  keywords = {Aerospace electronics,Cognition,Correlation,Humanoid robots,Large language models,Market research,Robot kinematics,Shape,Spatiotemporal phenomena,Symbols},
  annotation = {13 citations (Semantic Scholar/DOI) [2026-02-18]\\
13 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2024 - Large Language Models for Orchestrating Bimanual Robots - Chu et al..pdf}
}

@misc{2024_MHRCClosedloop_yu,
  title = {{{MHRC}}: {{Closed-loop Decentralized Multi-Heterogeneous Robot Collaboration}} with {{Large Language Models}}},
  shorttitle = {{{MHRC}}},
  author = {Yu, Wenhao and Peng, Jie and Ying, Yueliang and Li, Sai and Ji, Jianmin and Zhang, Yanyong},
  year = 2024,
  month = sep,
  number = {arXiv:2409.16030},
  eprint = {2409.16030},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.16030},
  urldate = {2026-01-30},
  abstract = {The integration of large language models (LLMs) with robotics has significantly advanced robots' abilities in perception, cognition, and task planning. The use of natural language interfaces offers a unified approach for expressing the capability differences of heterogeneous robots, facilitating communication between them, and enabling seamless task allocation and collaboration. Currently, the utilization of LLMs to achieve decentralized multi-heterogeneous robot collaborative tasks remains an under-explored area of research. In this paper, we introduce a novel framework that utilizes LLMs to achieve decentralized collaboration among multiple heterogeneous robots. Our framework supports three robot categories, mobile robots, manipulation robots, and mobile manipulation robots, working together to complete tasks such as exploration, transportation, and organization. We developed a rich set of textual feedback mechanisms and chain-of-thought (CoT) prompts to enhance task planning efficiency and overall system performance. The mobile manipulation robot can adjust its base position flexibly, ensuring optimal conditions for grasping tasks. The manipulation robot can comprehend task requirements, seek assistance when necessary, and handle objects appropriately. Meanwhile, the mobile robot can explore the environment extensively, map object locations, and communicate this information to the mobile manipulation robot, thus improving task execution efficiency. We evaluated the framework using PyBullet, creating scenarios with three different room layouts and three distinct operational tasks. We tested various LLM models and conducted ablation studies to assess the contributions of different modules. The experimental results confirm the effectiveness and necessity of our proposed framework.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  annotation = {7 citations (Semantic Scholar/DOI) [2026-01-30]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2024 - MHRC Closed-loop Decentralized Multi-Heterogeneous Robot Collaboration with Large Language Models - Yu et al. 1.pdf;C\:\\Users\\asmal\\Zotero\\storage\\AIHDZH74\\2409.html}
}

@inproceedings{2024_OctoOpenSource_ghosh,
  title = {Octo: {{An Open-Source Generalist Robot Policy}}},
  shorttitle = {Octo},
  booktitle = {Robotics: {{Science}} and {{Systems XX}}},
  author = {Ghosh, Dibya and Walke, Homer Rich and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and Luo, Jianlan and Tan, You Liang and Chen, Lawrence Yunliang and Vuong, Quan and Xiao, Ted and Sanketi, Pannag R. and Sadigh, Dorsa and Finn, Chelsea and Levine, Sergey},
  year = 2024,
  month = jul,
  volume = {20},
  urldate = {2026-01-30},
  isbn = {979-8-9902848-0-7},
  file = {G:\My Drive\03 Resources\Literature\2024 - Octo An Open-Source Generalist Robot Policy - Ghosh et al..pdf}
}

@inproceedings{2024_RobotPerfOpenSource_mayoral-vilches,
  title = {{{RobotPerf}}: {{An Open-Source}}, {{Vendor-Agnostic}}, {{Benchmarking Suite}} for {{Evaluating Robotics Computing System Performance}}},
  shorttitle = {{{RobotPerf}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {{Mayoral-Vilches}, V{\'i}ctor and Jabbour, Jason and Hsiao, Yu-Shun and Wan, Zishen and {Crespo-{\'A}lvarez}, Marti{\~n}o and Stewart, Matthew and {Reina-Mu{\~n}oz}, Juan Manuel and Nagras, Prateek and Vikhe, Gaurav and Bakhshalipour, Mohammad and Pinzger, Martin and Rass, Stefan and Panigrahi, Smruti and Corradi, Giulio and Roy, Niladri and Gibbons, Phillip B. and Neuman, Sabrina M. and Plancher, Brian and Reddi, Vijay Janapa},
  year = 2024,
  month = may,
  pages = {8288--8297},
  doi = {10.1109/ICRA57147.2024.10610841},
  urldate = {2026-01-25},
  abstract = {We introduce RobotPerf, a vendor-agnostic bench-marking suite designed to evaluate robotics computing performance across a diverse range of hardware platforms using ROS 2 as its common baseline. The suite encompasses ROS 2 packages covering the full robotics pipeline and integrates two distinct benchmarking approaches: black-box testing, which measures performance by eliminating upper layers and replacing them with a test application, and grey-box testing, an application-specific measure that observes internal system states with minimal interference. Our benchmarking framework provides ready-to-use tools and is easily adaptable for the assessment of custom ROS 2 computational graphs. Drawing from the knowledge of leading robot architects and system architecture experts, RobotPerf establishes a standardized approach to robotics benchmarking. As an open-source initiative, RobotPerf remains committed to evolving with community input to advance the future of hardware-accelerated robotics.},
  keywords = {Benchmark testing,Closed box,Hardware,Interference,Pipelines,System performance,Systems architecture},
  annotation = {20 citations (Semantic Scholar/DOI) [2026-01-25]},
  file = {G:\My Drive\03 Resources\Literature\2024 - RobotPerf An Open-Source, Vendor-Agnostic, Benchmarking Suite for Evaluating Robotics Computing System Performance - Mayoral-Vilches et al..pdf}
}

@inproceedings{2024_RoCoDialectic_mandia,
  title = {{{RoCo}}: {{Dialectic Multi-Robot Collaboration}} with {{Large Language Models}}},
  shorttitle = {{{RoCo}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Mandi, Zhao and Jain, Shreeya and Song, Shuran},
  year = 2024,
  month = may,
  pages = {286--299},
  doi = {10.1109/ICRA57147.2024.10610855},
  urldate = {2026-02-16},
  abstract = {We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset that evaluates LLMs' agent representation and reasoning capability. We experimentally demonstrate the effectiveness of our approach --- it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility --- in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together.},
  keywords = {Benchmark testing,Collaboration,Human in the loop,Large language models,Robot kinematics,Semantics,Trajectory planning},
  annotation = {218 citations (Semantic Scholar/DOI) [2026-02-18]\\
218 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2024 - RoCo Dialectic Multi-Robot Collaboration with Large Language Models - Mandi et al. 1.pdf}
}

@inproceedings{2024_ScalableMultiRobot_chen,
  title = {Scalable {{Multi-Robot Collaboration}} with {{Large Language Models}}: {{Centralized}} or {{Decentralized Systems}}?},
  shorttitle = {Scalable {{Multi-Robot Collaboration}} with {{Large Language Models}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Chen, Yongchao and Arkin, Jacob and Zhang, Yang and Roy, Nicholas and Fan, Chuchu},
  year = 2024,
  month = may,
  pages = {4311--4317},
  doi = {10.1109/ICRA57147.2024.10610676},
  urldate = {2026-01-30},
  abstract = {A flurry of recent work has demonstrated that pre-trained large language models (LLMs) can be effective task planners for a variety of single-robot tasks. The planning performance of LLMs is significantly improved via prompting techniques, such as in-context learning or re-prompting with state feedback, placing new importance on the token budget for the context window. An under-explored but natural next direction is to investigate LLMs as multi-robot task planners. However, long-horizon, heterogeneous multi-robot planning introduces new challenges of coordination while also pushing up against the limits of context window length. It is therefore critical to find token-efficient LLM planning frameworks that are also able to reason about the complexities of multi-robot coordination. In this work, we compare the task success rate and token efficiency of four multi-agent communication frameworks (centralized, decentralized, and two hybrid) as applied to four coordination-dependent multi-agent 2D task scenarios for increasing numbers of agents. We find that a hybrid framework achieves better task success rates across all four tasks and scales better to more agents. We further demonstrate the hybrid frameworks in 3D simulations where the vision-to-text problem and dynamical errors are considered. See our project website 4 for prompts, videos, and code.},
  keywords = {Collaboration,Large language models,Robot kinematics,Solid modeling,State feedback,Three-dimensional displays,Visualization},
  annotation = {135 citations (Semantic Scholar/DOI) [2026-02-18]\\
134 citations (Semantic Scholar/DOI) [2026-02-16]\\
53 citations (Crossref/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2024 - Scalable Multi-Robot Collaboration with Large Language Models Centralized or Decentralized Systems - Chen et al..pdf}
}

@inproceedings{2024_SMARTLLMSmart_kannan,
  title = {{{SMART-LLM}}: {{Smart Multi-Agent Robot Task Planning}} Using {{Large Language Models}}},
  shorttitle = {{{SMART-LLM}}},
  booktitle = {2024 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Kannan, Shyam Sundar and Venkatesh, Vishnunandan L. N. and Min, Byung-Cheol},
  year = 2024,
  month = oct,
  pages = {12140--12147},
  issn = {2153-0866},
  doi = {10.1109/IROS58592.2024.10802322},
  urldate = {2026-01-30},
  abstract = {In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/.},
  keywords = {Benchmark testing,Codes,Complexity theory,Intelligent robots,Large language models,Planning,Resource management,Videos},
  annotation = {225 citations (Semantic Scholar/DOI) [2026-02-18]\\
225 citations (Semantic Scholar/DOI) [2026-02-16]\\
101 citations (Crossref/DOI) [2026-02-16]\\
215 citations (Semantic Scholar/DOI) [2026-01-30]},
  file = {G:\My Drive\03 Resources\Literature\2024 - SMART-LLM Smart Multi-Agent Robot Task Planning using Large Language Models - Kannan et al..pdf}
}

@misc{2024_SurveyRobotics_xu,
  title = {A {{Survey}} on {{Robotics}} with {{Foundation Models}}: Toward {{Embodied AI}}},
  shorttitle = {A {{Survey}} on {{Robotics}} with {{Foundation Models}}},
  author = {Xu, Zhiyuan and Wu, Kun and Wen, Junjie and Li, Jinming and Liu, Ning and Che, Zhengping and Tang, Jian},
  year = 2024,
  month = feb,
  number = {arXiv:2402.02385},
  eprint = {2402.02385},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.02385},
  urldate = {2026-01-26},
  abstract = {While the exploration for embodied AI has spanned multiple decades, it remains a persistent challenge to endow agents with human-level intelligence, including perception, learning, reasoning, decision-making, control, and generalization capabilities, so that they can perform general-purpose tasks in open, unstructured, and dynamic environments. Recent advances in computer vision, natural language processing, and multi-modality learning have shown that the foundation models have superhuman capabilities for specific tasks. They not only provide a solid cornerstone for integrating basic modules into embodied AI systems but also shed light on how to scale up robot learning from a methodological perspective. This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control. Moreover, we showcase their commonly used datasets, simulators, and benchmarks. Importantly, we emphasize the critical challenges intrinsic to this field and delineate potential avenues for future research, contributing to advancing the frontier of academic and industrial discourse.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  annotation = {64 citations (Semantic Scholar/DOI) [2026-01-26]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2024 - A Survey on Robotics with Foundation Models toward Embodied AI - Xu et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\3W3GLLXM\\2402.html}
}

@article{2025_AchievingAdaptive_zhou,
  title = {Achieving Adaptive Tasks from Human Instructions for Robots Using Large Language Models and Behavior Trees},
  author = {Zhou, Haotian and Lin, Yunhan and Yan, Longwu and Min, Huasong},
  year = 2025,
  month = may,
  journal = {Robot. Auton. Syst.},
  volume = {187},
  number = {C},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2025.104937},
  urldate = {2026-01-30},
  annotation = {3 citations (Semantic Scholar/DOI) [2026-01-30]}
}

@inproceedings{2025_AttentionBasedHigherOrder_reasoner,
  title = {Attention-{{Based Higher-Order Reasoning}} for {{Implicit Coordination}} of {{Multi-Robot Systems}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Reasoner, Jonathan and Bramblett, Lauren and Bezzo, Nicola},
  year = 2025,
  month = oct,
  pages = {12428--12434},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11246158},
  urldate = {2026-02-16},
  abstract = {This paper presents a novel theory of mind (ToM)-based approach for implicit coordination of multi robot systems (MRS) in environments where direct communication is unavailable. The proposed approach integrates higher-order reasoning, epistemic theory, and active inference to coordinate the actions of each robot to clarify their own intentions and make them understandable to other robots. Further, to reduce the computational overhead of higher-order reasoning, we implement a large language model (LLM)-based attention selection mechanism that focuses on a subset of robots. Simulations and physical experiments demonstrate the applicability of the proposed approach with high success rates while significantly reducing computation complexity.},
  keywords = {Cognition,Computational modeling,Intelligent robots,Large language models,Logic,Multi-robot systems,Noise measurement,Robot kinematics,Robot sensing systems,Robots},
  annotation = {0 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Attention-Based Higher-Order Reasoning for Implicit Coordination of Multi-Robot Systems - Reasoner et al..pdf}
}

@article{2025_AutoHMALLMEfficient_yang,
  title = {{{AutoHMA-LLM}}: {{Efficient Task Coordination}} and {{Execution}} in {{Heterogeneous Multi-Agent Systems Using Hybrid Large Language Models}}},
  shorttitle = {{{AutoHMA-LLM}}},
  author = {Yang, Tingting and Feng, Ping and Guo, Qixin and Zhang, Jindi and Zhang, Xiufeng and Ning, Jiahong and Wang, Xinghan and Mao, Zhongyang},
  year = 2025,
  month = apr,
  journal = {IEEE Transactions on Cognitive Communications and Networking},
  volume = {11},
  number = {2},
  pages = {987--998},
  issn = {2332-7731},
  doi = {10.1109/TCCN.2025.3528892},
  urldate = {2026-01-30},
  abstract = {Heterogeneous multi-agent systems (HMAS) comprise various intelligent agents with specialized functions, such as drones, ground robots, and automated devices, working in coordinated settings. This paper presents AutoHMA-LLM, a novel framework that combines Large Language Models (LLMs) with classical control algorithms to address the challenges of task coordination and scheduling in complex, dynamic environments. The framework is designed with a multi-tier architecture, utilizing a cloud-based LLM as the central planner alongside device-specific LLMs and Generative Agents to improve task execution efficiency and accuracy. Specifically targeting dynamic scenarios, the system enhances resource utilization and stabilizes task execution through refined task scheduling and real-time feedback mechanisms. In experiments conducted across logistics, inspection, and search \& rescue scenarios, AutoHMA-LLM demonstrated a 5.7\% improvement in task completion accuracy, a 46\% reduction in communication steps, and a 31\% decrease in token usage and API calls compared to baseline methods. These results highlight our framework's scalability and efficiency, offering substantial support for effective multi-agent collaboration in complex, resource-constrained environments.},
  keywords = {cloud computing,Collaboration,communication coordination,Dynamic scheduling,dynamic task allocation,Generative AI,heterogeneous multi-agent system (HMAS),Heuristic algorithms,Job shop scheduling,large language model (LLM),Multi-agent systems,Planning,Real-time systems,Resource management,Robots,Scalability},
  annotation = {21 citations (Semantic Scholar/DOI) [2026-01-30]},
  file = {G:\My Drive\03 Resources\Literature\2025 - AutoHMA-LLM Efficient Task Coordination and Execution in Heterogeneous Multi-Agent Systems Using Hybrid Large Language Models - Yang et al..pdf}
}

@inproceedings{2025_AutoMistyMultiAgent_wang,
  title = {{{AutoMisty}}: {{A Multi-Agent LLM Framework}} for {{Automated Code Generation}} in the {{Misty Social Robot}}},
  shorttitle = {{{AutoMisty}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Wang, Xiao and Dong, Lu and Rangasrinivasan, Sahana and Nwogu, Ifeoma and Setlur, Srirangaraj and Govindaraju, Venugopal},
  year = 2025,
  month = oct,
  pages = {9194--9201},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11247695},
  urldate = {2026-02-16},
  abstract = {The social robot's open API allows users to customize open-domain interactions. However, it remains inaccessible to those without programming experience. We introduce AutoMisty, the first LLM-powered multi-agent framework that converts natural-language commands into executable Misty robot code by decomposing high-level instructions, generating sub-task code, and integrating everything into a deployable program. Each agent employs a two-layer optimization mechanism: first, a self-reflective loop that instantly validates and automatically executes the generated code, regenerating whenever errors emerge; second, human review for refinement and final approval, ensuring alignment with user preferences and preventing error propagation. To evaluate AutoMisty's effectiveness, we designed a benchmark task set spanning four levels of complexity and conducted experiments in a real Misty robot environment. Extensive evaluations demonstrate that AutoMisty not only consistently generates high-quality code but also enables precise code control, significantly outperforming direct reasoning with ChatGPT-4o and ChatGPT-o1. All code, optimized APIs, and experimental videos will be publicly released through the webpage: AutoMisty.},
  keywords = {Codes,Complexity theory,Controllability,Intelligent robots,Optimization,Programming,Reviews,Robustness,Social robots,Videos},
  annotation = {5 citations (Semantic Scholar/DOI) [2026-02-18]\\
5 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - AutoMisty A Multi-Agent LLM Framework for Automated Code Generation in the Misty Social Robot - Wang et al..pdf}
}

@inproceedings{2025_CLGACollaborative_yu,
  title = {{{CLGA}}: {{A Collaborative LLM Framework}} for {{Dynamic Goal Assignment}} in {{Multi-Robot Systems}}},
  shorttitle = {{{CLGA}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Yu, Xin and Li, Haoyuan and Wang, Yandong and Li, Simin and Shi, Rongye and Ai, Gangzheng and Pu, Zhiqiang and Wu, Wenjun},
  year = 2025,
  month = oct,
  pages = {2788--2794},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11246692},
  urldate = {2026-02-16},
  abstract = {Goal assignment is a critical challenge in multi-robot systems. The emergence of large language models (LLMs) has enabled the use of natural language commands for tackling goal assignment problems. However, applying LLMs directly to these tasks presents two limitations: 1) limited accuracy and 2) excessive decision delays due to their autoregressive nature, hindering adaptability to unexpected changes. To address these issues, inspired by dual-process theory, we propose a framework called Collaborative LLMs for dynamic Goal Assignment (CLGA). Specifically, we leverage LLMs for pre-planning tasks and invoke an external solver to generate an initial goal assignment solution, ensuring solution accuracy. During execution, small-scale models enable real-time adjustments to respond to dynamic environmental changes. This approach integrates the strengths of slow, precise pre-planning and fast, adaptive online adjustments, allowing agents to efficiently handle real-world challenges. Additionally, we introduce a benchmark dataset for NLP-based goal assignment to advance research in this domain. Simulation and real-world experiments demonstrate that CLGA significantly enhances task execution efficiency and flexibility in multi-robot systems. The prompt, experimental videos, and datasets associated with this work are available at https://sites.google.com/view/project-clga/.},
  keywords = {Accuracy,Benchmark testing,Collaboration,Intelligent robots,Large language models,Multi-robot systems,Natural languages,Planning,Real-time systems,Videos},
  file = {G:\My Drive\03 Resources\Literature\2025 - CLGA A Collaborative LLM Framework for Dynamic Goal Assignment in Multi-Robot Systems - Yu et al..pdf}
}

@inproceedings{2025_CodeasSymbolicPlannerFoundation_chen,
  title = {Code-as-{{Symbolic-Planner}}: {{Foundation Model-Based Robot Planning}} via {{Symbolic Code Generation}}},
  shorttitle = {Code-as-{{Symbolic-Planner}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Chen, Yongchao and Hao, Yilun and Zhang, Yang and Fan, Chuchu},
  year = 2025,
  month = oct,
  pages = {19248--19254},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11247174},
  urldate = {2026-02-16},
  abstract = {Recent works have shown great potential of Large Language Models (LLMs) in robot task and motion planning (TAMP). Current LLM approaches generate text- or code-based reasoning chains with sub-goals and action plans. However, they do not fully leverage LLMs' symbolic computing and code generation capabilities. Many robot TAMP tasks involve complex optimization under multiple constraints, where pure textual reasoning is insufficient. While augmenting LLMs with predefined solvers and planners improves performance, it lacks generalization across tasks. Given LLMs' growing coding proficiency, we enhance their TAMP capabilities by steering them to generate code as symbolic planners for optimization and constraint verification. Unlike prior work that uses code to interface with robot action modules or pre-designed planners, we steer LLMs to generate code as solvers, planners, and checkers for TAMP tasks requiring symbolic computing, while still leveraging textual reasoning to incorporate common sense. With a multi-round guidance and answer evolution framework, the proposed Code-as-Symbolic-Planner improves success rates by average 24.1\% over best baseline methods across seven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner shows strong effectiveness and generalizability across discrete and continuous environments, 2D/3D simulations and real-world settings, as well as single- and multi-robot tasks with diverse requirements. See our project website\dag{} for prompts, videos, and code.},
  keywords = {Codes,Cognition,Encoding,Optimization,Planning,Robot sensing systems,Robots,Translation,Usability,Videos},
  annotation = {6 citations (Semantic Scholar/DOI) [2026-02-18]\\
6 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Code-as-Symbolic-Planner Foundation Model-Based Robot Planning via Symbolic Code Generation - Chen et al..pdf}
}

@inproceedings{2025_COHERENTCollaboration_liu,
  title = {{{COHERENT}}: {{Collaboration}} of {{Heterogeneous Multi-Robot System}} with {{Large Language Models}}},
  shorttitle = {{{COHERENT}}},
  booktitle = {2025 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Liu, Kehui and Tang, Zixin and Wang, Dong and Wang, Zhigang and Li, Xuelong and Zhao, Bin},
  year = 2025,
  month = may,
  pages = {10208--10214},
  doi = {10.1109/ICRA55743.2025.11127808},
  urldate = {2026-01-26},
  abstract = {Leveraging the powerful reasoning capabilities of large language models (LLMs), recent LLM-based robot task planning methods yield promising results. However, they mainly focus on single or multiple homogeneous robots on simple tasks. Practically, complex long-horizon tasks always require collaboration among multiple heterogeneous robots especially with more complex action spaces, which makes these tasks more challenging. To this end, we propose COHERENT, a novel LLM-based task planning framework for collaboration of heterogeneous multi-robot systems including quadrotors, robotic dogs, and robotic arms. Specifically, a Proposal-Execution-Feedback-Adjustment (PEFA) mechanism is designed to decompose and assign actions for individual robots, where a centralized task assigner makes a task planning proposal to decompose the complex task into subtasks, and then assigns subtasks to robot executors. Each robot executor selects a feasible action to implement the assigned subtask and reports self-reflection feedback to the task assigner for plan adjustment. The PEFA loops until the task is completed. Moreover, we create a challenging heterogeneous multi-robot task planning benchmark encompassing 100 complex long-horizon tasks. The experimental results show that our work surpasses the previous methods by a large margin in terms of success rate and execution efficiency. The experimental videos, code, and benchmark are released at https://github.com/MrKeee/COHERENT.},
  keywords = {Benchmark testing,Cognition,Collaboration,Large language models,Multi-robot systems,Planning,Quadrotors,Resource management,Robots,Videos},
  annotation = {32 citations (Semantic Scholar/DOI) [2026-02-18]\\
32 citations (Semantic Scholar/DOI) [2026-02-16]\\
7 citations (Crossref/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - COHERENT Collaboration of Heterogeneous Multi-Robot System with Large Language Models - Liu et al..pdf}
}

@misc{2025_CompositionalCoordination_huang,
  title = {Compositional {{Coordination}} for {{Multi-Robot Teams}} with {{Large Language Models}}},
  author = {Huang, Zhehui and Shi, Guangyao and Wu, Yuwei and Kumar, Vijay and Sukhatme, Gaurav S.},
  year = 2025,
  month = oct,
  number = {arXiv:2507.16068},
  eprint = {2507.16068},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.16068},
  urldate = {2026-01-30},
  abstract = {Multi-robot coordination has traditionally relied on a mission-specific and expert-driven pipeline, where natural language mission descriptions are manually translated by domain experts into mathematical formulation, algorithm design, and executable code. This conventional process is labor-intensive, inaccessible to non-experts, and inflexible to changes in mission requirements. Here, we propose LAN2CB (Language to Collective Behavior), a novel framework that leverages large language models (LLMs) to streamline and generalize the multi-robot coordination pipeline. LAN2CB transforms natural language (NL) mission descriptions into executable Python code for multi-robot systems through two core modules: (1) Mission Analysis, which parses mission descriptions into behavior trees, and (2) Code Generation, which leverages the behavior tree and a structured knowledge base to generate robot control code. We further introduce a dataset of natural language mission descriptions to support development and benchmarking. Experiments in both simulation and real-world environments demonstrate that LAN2CB enables robust and flexible multi-robot coordination from natural language, significantly reducing manual engineering effort and supporting broad generalization across diverse mission types. Website: https://sites.google.com/view/lan-cb},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Robotics},
  annotation = {0 citations (Semantic Scholar/arXiv) [2026-01-30]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2025 - Compositional Coordination for Multi-Robot Teams with Large Language Models - Huang et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\BP79IJGH\\2507.html}
}

@misc{2025_CoordFieldCoordination_zhang,
  title = {{{CoordField}}: {{Coordination Field}} for {{Agentic UAV Task Allocation In Low-altitude Urban Scenarios}}},
  shorttitle = {{{CoordField}}},
  author = {Zhang, Tengchao and Tian, Yonglin and Lin, Fei and Huang, Jun and S{\"u}li, Patrik P. and Ni, Qinghua and Qin, Rui and Wang, Xiao and Wang, Fei-Yue},
  year = 2025,
  month = jul,
  number = {arXiv:2505.00091},
  eprint = {2505.00091},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.00091},
  urldate = {2026-01-30},
  abstract = {With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV) swarms to perform complex tasks in urban environments, system design now faces major challenges, including efficient semantic understanding, flexible task planning, and the ability to dynamically adjust coordination strategies in response to evolving environmental conditions and continuously changing task requirements. To address the limitations of existing methods, this paper proposes CoordField, a coordination field agent system for coordinating heterogeneous drone swarms in complex urban scenarios. In this system, large language models (LLMs) is responsible for interpreting high-level human instructions and converting them into executable commands for the UAV swarms, such as patrol and target tracking. Subsequently, a Coordination field mechanism is proposed to guide UAV motion and task selection, enabling decentralized and adaptive allocation of emergent tasks. A total of 50 rounds of comparative testing were conducted across different models in a 2D simulation space to evaluate their performance. Experimental results demonstrate that the proposed system achieves superior performance in terms of task coverage, response time, and adaptability to dynamic changes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  annotation = {2 citations (Semantic Scholar/DOI) [2026-01-30]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2025 - CoordField Coordination Field for Agentic UAV Task Allocation In Low-altitude Urban Scenarios - Zhang et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\QR66TYTW\\2505.html}
}

@misc{2025_DARTLLMDependencyAware_wang,
  title = {{{DART-LLM}}: {{Dependency-Aware Multi-Robot Task Decomposition}} and {{Execution}} Using {{Large Language Models}}},
  shorttitle = {{{DART-LLM}}},
  author = {Wang, Yongdong and Xiao, Runze and Kasahara, Jun Younes Louhi and Yajima, Ryosuke and Nagatani, Keiji and Yamashita, Atsushi and Asama, Hajime},
  year = 2025,
  month = mar,
  number = {arXiv:2411.09022},
  eprint = {2411.09022},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.09022},
  urldate = {2026-01-30},
  abstract = {Large Language Models (LLMs) have demonstrated promising reasoning capabilities in robotics; however, their application in multi-robot systems remains limited, particularly in handling task dependencies. This paper introduces DART-LLM, a novel framework that employs Directed Acyclic Graphs (DAGs) to model task dependencies, enabling the decomposition of natural language instructions into well-coordinated subtasks for multi-robot execution. DART-LLM comprises four key components: a Question-Answering (QA) LLM module for dependency-aware task decomposition, a Breakdown Function module for robot assignment, an Actuation module for execution, and a Vision-Language Model (VLM)-based object detector for environmental perception, achieving end-to-end task execution. Experimental results across three task complexity levels demonstrate that DART-LLM achieves state-of-the-art performance, significantly outperforming the baseline across all evaluation metrics. Among the tested models, DeepSeek-r1-671B achieves the highest success rate, whereas Llama-3.1-8B exhibits superior response time reliability. Ablation studies further confirm that explicit dependency modeling notably enhances the performance of smaller models, facilitating efficient deployment on resource-constrained platforms. Please refer to the project website https://wyd0817.github.io/project-dart-llm/ for videos and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {G:\My Drive\03 Resources\Literature\2025 - DART-LLM Dependency-Aware Multi-Robot Task Decomposition and Execution using Large Language Models - Wang et al..pdf}
}

@misc{2025_DELIVERSystem_srivastava,
  title = {{{DELIVER}}: {{A System}} for {{LLM-Guided Coordinated Multi-Robot Pickup}} and {{Delivery}} Using {{Voronoi-Based Relay Planning}}},
  shorttitle = {{{DELIVER}}},
  author = {Srivastava, Alkesh K. and Levin, Jared Michael and Derrico, Alexander and Dames, Philip},
  year = 2025,
  month = aug,
  number = {arXiv:2508.19114},
  eprint = {2508.19114},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.19114},
  urldate = {2026-01-30},
  abstract = {We present DELIVER (Directed Execution of Language-instructed Item Via Engineered Relay), a fully integrated framework for cooperative multi-robot pickup and delivery driven by natural language commands. DELIVER unifies natural language understanding, spatial decomposition, relay planning, and motion execution to enable scalable, collision-free coordination in real-world settings. Given a spoken or written instruction, a lightweight instance of LLaMA3 interprets the command to extract pickup and delivery locations. The environment is partitioned using a Voronoi tessellation to define robot-specific operating regions. Robots then compute optimal relay points along shared boundaries and coordinate handoffs. A finite-state machine governs each robot's behavior, enabling robust execution. We implement DELIVER on the MultiTRAIL simulation platform and validate it in both ROS2-based Gazebo simulations and real-world hardware using TurtleBot3 robots. Empirical results show that DELIVER maintains consistent mission cost across varying team sizes while reducing per-agent workload by up to 55\% compared to a single-agent system. Moreover, the number of active relay agents remains low even as team size increases, demonstrating the system's scalability and efficient agent utilization. These findings underscore DELIVER's modular and extensible architecture for language-guided multi-robot coordination, advancing the frontiers of cyber-physical system integration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Multiagent Systems,Computer Science - Robotics},
  annotation = {1 citations (Semantic Scholar/DOI) [2026-01-30]},
  file = {C\:\\Users\\asmal\\Zotero\\storage\\YWKG3G65\\2025 - DELIVER A System for LLM-Guided Coordinated Multi-Robot Pickup and Delivery using Voronoi-Based Relay Planning - Srivastava et al..pdf;G\:\\My Drive\\03 Resources\\Literature\\2025 - DELIVER A System for LLM-Guided Coordinated Multi-Robot Pickup and Delivery using Voronoi-Based Relay Planning - Srivastava et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\B8YABE5U\\2508.html}
}

@inproceedings{2025_DEXTERLLMDynamic_zhu,
  title = {{{DEXTER-LLM}}: {{Dynamic}} and {{Explainable Coordination}} of {{Multi-Robot Systems}} in {{Unknown Environments}} via {{Large Language Models}}},
  shorttitle = {{{DEXTER-LLM}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Zhu, Yuxiao and Chen, Junfeng and Zhang, Xintong and Guo, Meng and Li, Zhongkui},
  year = 2025,
  month = oct,
  pages = {10182--10189},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11247301},
  urldate = {2026-02-16},
  abstract = {Online coordination of multi-robot systems in open and unknown environments faces significant challenges, particularly when semantic features detected during operation dynamically trigger new tasks. Recent large language model (LLMs)-based approaches for scene reasoning and planning primarily focus on one-shot, end-to-end solutions in known environments, lacking both dynamic adaptation capabilities for online operation and explainability in the processes of planning. To address these issues, a novel framework (DEXTER-LLM) for dynamic task planning in unknown environments, integrates four modules: (i) a mission comprehension module that resolves partial ordering of tasks specified by natural languages or linear temporal logic formulas (LTL); (ii) an online subtask generator based on LLMs that improves the accuracy and explainability of task decomposition via multi-stage reasoning; (iii) an optimal subtask assigner and scheduler that allocates subtasks to robots via search-based optimization; and (iv) a dynamic adaptation and human-in-the-loop verification module that implements multi-rate, event-based updates for both subtasks and their assignments, to cope with new features and tasks detected online. The framework effectively combines LLMs' open-world reasoning capabilities with the optimality of model-based assignment methods, simultaneously addressing the critical issue of online adaptability and explainability. Experimental evaluations demonstrate exceptional performances, with 100\% success rates across all scenarios, 160 tasks and 480 subtasks completed on average (3 times the baselines), 62\% less queries to LLMs during adaptation, and superior plan quality (2 times higher) for compound tasks. Project page at https://tcxm.github.io/DEXTER-LLM/.},
  keywords = {Cognition,Dynamic scheduling,Feature extraction,Large language models,Multi-robot systems,Natural languages,Optimization,Planning,Robot kinematics,Semantics},
  annotation = {2 citations (Semantic Scholar/DOI) [2026-02-18]\\
2 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - DEXTER-LLM Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models - Zhu et al. 1.pdf}
}

@article{2025_DigitalTwins_li,
  title = {Digital Twins to Embodied Artificial Intelligence: Review and Perspective},
  shorttitle = {Digital Twins to Embodied Artificial Intelligence},
  author = {Li, Junfei and Yang, Simon X.},
  year = 2025,
  month = mar,
  journal = {Intelligence \& Robotics},
  volume = {5},
  number = {1},
  pages = {202--227},
  publisher = {OAE Publishing Inc.},
  issn = {ISSN 2770-3541 (Online)},
  doi = {10.20517/ir.2025.11},
  urldate = {2026-01-25},
  abstract = {Embodied artificial intelligence (AI) is reshaping the landscape of intelligent robotic systems, particularly by providing many realistic solutions to execute actions in complex and dynamic environments. However, Embodied AI requires a huge data generation for training and evaluation to ensure safe interaction with physical environments. Therefore, it is necessary to build a cost-effective simulated environment that can provide enough data for training and optimization from the physical characteristics, object properties, and interactions. Digital twins (DTs) are vital issues in Industry 5.0, which enable real-time monitoring, simulation, and optimization of physical processes by mirroring the state and action of their real-world counterparts. This review explores how integrating DTs with Embodied AI can bridge the sim-to-real gap by transforming virtual environments into dynamic and data-rich platforms. The integration of DTs offers real-time monitoring and virtual simulations, enabling Embodied AI agents to train and adapt in virtual environments before deployment in real-world scenarios. In this review, the main challenges and the novel perspective of the future development of integrating DTs and Embodied AI are discussed. To the best of our knowledge, this is the first work to comprehensively review the synergies between DTs and Embodied AI.},
  langid = {english},
  file = {G:\My Drive\03 Resources\Literature\2025 - Digital twins to embodied artificial intelligence review and perspective - Li and Yang.pdf}
}

@misc{2025_EmbodiedAgentic_salimpour,
  title = {Towards {{Embodied Agentic AI}}: {{Review}} and {{Classification}} of {{LLM-}} and {{VLM-Driven Robot Autonomy}} and {{Interaction}}},
  shorttitle = {Towards {{Embodied Agentic AI}}},
  author = {Salimpour, Sahar and Fu, Lei and Rachwa{\l}, Kajetan and Bertrand, Pascal and O'Sullivan, Kevin and Jakob, Robert and Keramat, Farhad and Militano, Leonardo and Toffetti, Giovanni and Edelman, Harry and Queralta, Jorge Pe{\~n}a},
  year = 2025,
  month = nov,
  number = {arXiv:2508.05294},
  eprint = {2508.05294},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.05294},
  urldate = {2025-12-07},
  abstract = {Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (LBMs) are increasing the dexterity and capabilities of robotic systems. This survey paper reviews works that advance agentic applications and architectures, including initial efforts with GPT-style interfaces and more complex systems where AI agents function as coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,survey},
  annotation = {4 citations (Semantic Scholar/arXiv) [2025-12-16]\\
4 citations (Semantic Scholar/DOI) [2025-12-07]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2025 - Towards Embodied Agentic AI Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction - Salimpour et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\VCR2IP7D\\2508.html}
}

@inproceedings{2025_EmbodiedAgentScalable_wan,
  title = {{{EmbodiedAgent}}: {{A Scalable Hierarchical Approach}} to {{Overcome Practical Challenge}} in {{Multi-Robot Control}}},
  shorttitle = {{{EmbodiedAgent}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Wan, Hanwen and Chen, Yifei and Deng, Yixuan and Wei, Zeyu and Li, Dongrui and Lin, Zexin and Wu, Donghao and Cheng, Jiu and Ji, Xiaoqiang},
  year = 2025,
  month = oct,
  pages = {12140--12146},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11247740},
  urldate = {2026-02-16},
  abstract = {This paper introduces EmbodiedAgent, a hierarchical framework for heterogeneous multi-robot control. EmbodiedAgent addresses critical limitations of hallucination in impractical tasks. Our approach integrates a next-action prediction paradigm with a structured memory system to decompose tasks into executable robot skills while dynamically validating actions against environmental constraints. We present Mul-tiPlan+, a dataset of more than 18,000 annotated planning instances spanning 100 scenarios, including a subset of impractical cases to mitigate hallucination. To evaluate performance, we propose the Robot Planning Assessment Schema (RPAS), combining automated metrics with LLM-aided expert grading. Experiments demonstrate EmbodiedAgent's superiority over state-of-the-art models, achieving 71.85\% RPAS score. Real-world validation in an office service task highlights its ability to coordinate heterogeneous robots for long-horizon objectives.},
  keywords = {Benchmark testing,Intelligent robots,Measurement,Multi-robot systems,Planning,Robot kinematics,Robustness},
  annotation = {1 citations (Semantic Scholar/DOI) [2026-02-18]\\
1 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - EmbodiedAgent A Scalable Hierarchical Approach to Overcome Practical Challenge in Multi-Robot Control - Wan et al..pdf}
}

@misc{2025_EmbodiedAI_lisondra,
  title = {Embodied {{AI}} with {{Foundation Models}} for {{Mobile Service Robots}}: {{A Systematic Review}}},
  shorttitle = {Embodied {{AI}} with {{Foundation Models}} for {{Mobile Service Robots}}},
  author = {Lisondra, Matthew and Benhabib, Beno and Nejat, Goldie},
  year = 2025,
  month = may,
  number = {arXiv:2505.20503},
  eprint = {2505.20503},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.20503},
  urldate = {2026-01-26},
  abstract = {Rapid advancements in foundation models, including Large Language Models, Vision-Language Models, Multimodal Large Language Models, and Vision-Language-Action Models have opened new avenues for embodied AI in mobile service robotics. By combining foundation models with the principles of embodied AI, where intelligent systems perceive, reason, and act through physical interactions, robots can improve understanding, adapt to, and execute complex tasks in dynamic real-world environments. However, embodied AI in mobile service robots continues to face key challenges, including multimodal sensor fusion, real-time decision-making under uncertainty, task generalization, and effective human-robot interactions (HRI). In this paper, we present the first systematic review of the integration of foundation models in mobile service robotics, identifying key open challenges in embodied AI and examining how foundation models can address them. Namely, we explore the role of such models in enabling real-time sensor fusion, language-conditioned control, and adaptive task execution. Furthermore, we discuss real-world applications in the domestic assistance, healthcare, and service automation sectors, demonstrating the transformative impact of foundation models on service robotics. We also include potential future research directions, emphasizing the need for predictive scaling laws, autonomous long-term adaptation, and cross-embodiment generalization to enable scalable, efficient, and robust deployment of foundation models in human-centric robotic systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,survey},
  annotation = {6 citations (Semantic Scholar/arXiv) [2026-01-26]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2025 - Embodied AI with Foundation Models for Mobile Service Robots A Systematic Review - Lisondra et al. 1.pdf;C\:\\Users\\asmal\\Zotero\\storage\\F758FAYA\\2505.html}
}

@inproceedings{2025_EnablingNovel_royce,
  title = {Enabling {{Novel Mission Operations}} and {{Interactions}} with {{ROSA}}: {{The Robot Operating System Agent}}},
  shorttitle = {Enabling {{Novel Mission Operations}} and {{Interactions}} with {{ROSA}}},
  booktitle = {2025 {{IEEE Aerospace Conference}}},
  author = {Royce, Rob and Kaufmann, Marcel and Becktor, Jonathan and Moon, Sangwoo and Carpenter, Kalind and Pak, Kai and Towler, Amanda and Thakker, Rohan and Khattak, Shehryar},
  year = 2025,
  month = mar,
  pages = {1--16},
  issn = {2996-2358},
  doi = {10.1109/AERO63441.2025.11068426},
  urldate = {2026-01-26},
  abstract = {The advancement of robotic systems has revolutionized numerous industries, yet their operation often demands specialized technical knowledge, limiting accessibility for non-expert users. This paper introduces ROSA (Robot Operating System Agent), an AI-powered agent that bridges the gap between the Robot Operating System (ROS) and natural language interfaces. By leveraging state-of-the-art language models and integrating open-source frameworks, ROSA enables operators to interact with robots using natural language, translating commands into actions and interfacing with ROS through well-defined tools. ROSA's design is modular and extensible, offering seamless integration with both ROS1 and ROS2, along with safety mechanisms like parameter validation and constraint enforcement to ensure secure, reliable operations. While ROSA is originally designed for ROS, it can be extended to work with other robotics middle-wares to maximize compatibility across missions. ROSA enhances human-robot interaction by democratizing access to complex robotic systems, empowering users of all expertise levels with multi-modal capabilities such as speech integration and visual perception. Ethical considerations are thoroughly addressed, guided by foundational principles like Asimov's Three Laws of Robotics, ensuring that AI integration promotes safety, transparency, privacy, and accountability. By making robotic technology more user-friendly and accessible, ROSA not only improves operational efficiency but also sets a new standard for responsible AI use in robotics and potentially future mission operations. This paper introduces ROSA's architecture and showcases initial mock-up operations in JPL's Mars Yard, a laboratory, and a simulation using three different robots. The core ROSA library is available on GitHub 11https://github.com/nasa-jpl/rosa.},
  keywords = {Artificial intelligence,Ethics,Human-robot interaction,Operating systems,Privacy,Robots,Safety,Standards,Translation,Visual perception},
  annotation = {13 citations (Semantic Scholar/DOI) [2026-01-26]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Enabling Novel Mission Operations and Interactions with ROSA The Robot Operating System Agent - Royce et al..pdf}
}

@inproceedings{2025_FCRFFlexible_song,
  title = {{{FCRF}}: {{Flexible Constructivism Reflection}} for {{Long-Horizon Robotic Task Planning}} with {{Large Language Models}}},
  shorttitle = {{{FCRF}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Song, Yufan and Zhang, Jiatao and Gu, Zeng and Liang, Qingmiao and Hu, Tuocheng and Song, Wei and Zhu, Shiqiang},
  year = 2025,
  month = oct,
  pages = {5462--5469},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11246083},
  urldate = {2026-02-16},
  abstract = {Autonomous error correction is critical for domestic robots to achieve reliable execution of complex long-horizon tasks. Prior work has explored self-reflection in Large Language Models (LLMs) for task planning error correction; however, existing methods are constrained by inflexible self-reflection mechanisms that limit their effectiveness. Motivated by these limitations and inspired by human cognitive adaptation, we propose the Flexible Constructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture that enables LLMs to perform flexible self-reflection based on task difficulty, while constructively integrating historical valuable experience with failure lessons. We evaluated FCRF on diverse domestic tasks through simulation in AlfWorld and physical deployment in the real-world environment. Experimental results demonstrate that FCRF significantly improves overall performance and self-reflection flexibility in complex long-horizon robotic tasks. Website at https://mongoosesyf.github.io/FCRF.github.io/},
  keywords = {Error correction,Knowledge engineering,Large language models,Memory management,Organizations,Planning,Reflection,Reliability,Search problems,Trajectory},
  annotation = {0 citations (Semantic Scholar/DOI) [2026-02-18]\\
0 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - FCRF Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models - Song et al..pdf}
}

@misc{2025_GeminiRobotics_team,
  title = {Gemini {{Robotics}}: {{Bringing AI}} into the {{Physical World}}},
  shorttitle = {Gemini {{Robotics}}},
  author = {Team, Gemini Robotics and Abeyruwan, Saminda and Ainslie, Joshua and Alayrac, Jean-Baptiste and Arenas, Montserrat Gonzalez and Armstrong, Travis and Balakrishna, Ashwin and Baruch, Robert and Bauza, Maria and Blokzijl, Michiel and Bohez, Steven and Bousmalis, Konstantinos and Brohan, Anthony and Buschmann, Thomas and Byravan, Arunkumar and Cabi, Serkan and Caluwaerts, Ken and Casarini, Federico and Chang, Oscar and Chen, Jose Enrique and Chen, Xi and Chiang, Hao-Tien Lewis and Choromanski, Krzysztof and D'Ambrosio, David and Dasari, Sudeep and Davchev, Todor and Devin, Coline and Palo, Norman Di and Ding, Tianli and Dostmohamed, Adil and Driess, Danny and Du, Yilun and Dwibedi, Debidatta and Elabd, Michael and Fantacci, Claudio and Fong, Cody and Frey, Erik and Fu, Chuyuan and Giustina, Marissa and Gopalakrishnan, Keerthana and Graesser, Laura and Hasenclever, Leonard and Heess, Nicolas and Hernaez, Brandon and Herzog, Alexander and Hofer, R. Alex and Humplik, Jan and Iscen, Atil and Jacob, Mithun George and Jain, Deepali and Julian, Ryan and Kalashnikov, Dmitry and Karagozler, M. Emre and Karp, Stefani and Kew, Chase and Kirkland, Jerad and Kirmani, Sean and Kuang, Yuheng and Lampe, Thomas and Laurens, Antoine and Leal, Isabel and Lee, Alex X. and Lee, Tsang-Wei Edward and Liang, Jacky and Lin, Yixin and Maddineni, Sharath and Majumdar, Anirudha and Michaely, Assaf Hurwitz and Moreno, Robert and Neunert, Michael and Nori, Francesco and Parada, Carolina and Parisotto, Emilio and Pastor, Peter and Pooley, Acorn and Rao, Kanishka and Reymann, Krista and Sadigh, Dorsa and Saliceti, Stefano and Sanketi, Pannag and Sermanet, Pierre and Shah, Dhruv and Sharma, Mohit and Shea, Kathryn and Shu, Charles and Sindhwani, Vikas and Singh, Sumeet and Soricut, Radu and Springenberg, Jost Tobias and Sterneck, Rachel and Surdulescu, Razvan and Tan, Jie and Tompson, Jonathan and Vanhoucke, Vincent and Varley, Jake and Vesom, Grace and Vezzani, Giulia and Vinyals, Oriol and Wahid, Ayzaan and Welker, Stefan and Wohlhart, Paul and Xia, Fei and Xiao, Ted and Xie, Annie and Xie, Jinyu and Xu, Peng and Xu, Sichun and Xu, Ying and Xu, Zhuo and Yang, Yuxiang and Yao, Rui and Yaroshenko, Sergey and Yu, Wenhao and Yuan, Wentao and Zhang, Jingwei and Zhang, Tingnan and Zhou, Allan and Zhou, Yuxiang},
  year = 2025,
  month = mar,
  number = {arXiv:2503.20020},
  eprint = {2503.20020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.20020},
  urldate = {2026-01-30},
  abstract = {Recent advancements in large multimodal models have led to the emergence of remarkable generalist capabilities in digital domains, yet their translation to physical agents such as robots remains a significant challenge. This report introduces a new family of AI models purposefully designed for robotics and built upon the foundation of Gemini 2.0. We present Gemini Robotics, an advanced Vision-Language-Action (VLA) generalist model capable of directly controlling robots. Gemini Robotics executes smooth and reactive movements to tackle a wide range of complex manipulation tasks while also being robust to variations in object types and positions, handling unseen environments as well as following diverse, open vocabulary instructions. We show that with additional fine-tuning, Gemini Robotics can be specialized to new capabilities including solving long-horizon, highly dexterous tasks, learning new short-horizon tasks from as few as 100 demonstrations and adapting to completely novel robot embodiments. This is made possible because Gemini Robotics builds on top of the Gemini Robotics-ER model, the second model we introduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends Gemini's multimodal reasoning capabilities into the physical world, with enhanced spatial and temporal understanding. This enables capabilities relevant to robotics including object detection, pointing, trajectory and grasp prediction, as well as multi-view correspondence and 3D bounding box predictions. We show how this novel combination can support a variety of robotics applications. We also discuss and address important safety considerations related to this new class of robotics foundation models. The Gemini Robotics family marks a substantial step towards developing general-purpose robots that realizes AI's potential in the physical world.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  annotation = {0 citations (Semantic Scholar/arXiv) [2026-01-30]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2025 - Gemini Robotics Bringing AI into the Physical World - Team et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\JUZMI7UU\\2503.html}
}

@inproceedings{2025_GeneralizedMission_gupta,
  title = {Generalized {{Mission Planning}} for {{Heterogeneous Multi-Robot Teams}} via {{LLM-Constructed Hierarchical Trees}}},
  booktitle = {2025 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Gupta, Piyush and Isele, David and Sachdeva, Enna and Huang, Pin-Hao and Dariush, Behzad and Lee, Kwonjoon and Bae, Sangjae},
  year = 2025,
  month = may,
  pages = {10187--10193},
  doi = {10.1109/ICRA55743.2025.11128711},
  urldate = {2026-01-30},
  abstract = {We present a novel mission-planning strategy for heterogeneous multi-robot teams, taking into account the specific constraints and capabilities of each robot. Our approach employs hierarchical trees to systematically break down complex missions into manageable sub-tasks. We develop specialized APIs and tools, which are utilized by Large Language Models (LLMs) to efficiently construct these hierarchical trees. Once the hierarchical tree is generated, it is further decomposed to create optimized schedules for each robot, ensuring adherence to their individual constraints and capabilities. We demonstrate the effectiveness of our framework through detailed examples covering a wide range of missions, showcasing its flexibility and scalability.},
  keywords = {Heuristic algorithms,Large language models,Multi-robot systems,Robots,Scalability,Schedules},
  annotation = {10 citations (Semantic Scholar/DOI) [2026-02-18]\\
3 citations (Crossref/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Generalized Mission Planning for Heterogeneous Multi-Robot Teams via LLM-Constructed Hierarchical Trees - Gupta et al..pdf}
}

@inproceedings{2025_GMATPLLMGeneral_deng,
  title = {{{GMATP-LLM}}: {{A General Multi-Agent Task Dynamic Planning Method}} Using {{Large Language Models}}},
  shorttitle = {{{GMATP-LLM}}},
  booktitle = {2025 44th {{Chinese Control Conference}} ({{CCC}})},
  author = {Deng, Xiangkun and Tao, Gang and Wen, Chenxu and Zhang, Xi and Ju, Zhiyang and Gong, Jianwei},
  year = 2025,
  month = jul,
  pages = {5792--5798},
  issn = {1934-1768},
  doi = {10.23919/CCC64809.2025.11179615},
  urldate = {2026-01-30},
  abstract = {In this work, we introduce a generalized task planning innovation framework named GMATP-LLM, which is designed for multi-agent systems. This method utilizes Chain-of-Thought (CoT) prompting to enable Large Language Models (LLMs) to perform task decomposition and assignment processes, transforming high-level task instructions into a set of sub-tasks. Based on the assignment strategy, it generates a PDDL goal plan, which is solved by an intelligent planner to generate a sequence of actions. The framework introduces a multi-agent three-dimensional Spatio-Temporal Motion Corridor (STMC) to constrain and optimize the parallel motion of the agents, improving system efficiency. This method combines the reasoning capabilities of LLMs and the fast solving advantage of the intelligent PDDL planners. It has been verified through simulation and real-world experiments across various task categories, achieving favorable results in multi-agent task planning.},
  keywords = {Cognition,Dynamics,large language models,Large language models,Multi-agent systems,multi-agent task planning,PDDL planning,Planning,Technological innovation},
  annotation = {0 citations (Semantic Scholar/DOI) [2026-01-30]},
  file = {G:\My Drive\03 Resources\Literature\2025 - GMATP-LLM A General Multi-Agent Task Dynamic Planning Method using Large Language Models - Deng et al..pdf}
}

@misc{2025_HierarchicalLLMs_wu,
  title = {Hierarchical {{LLMs In-the-Loop Optimization}} for {{Real-Time Multi-Robot Target Tracking}} under {{Unknown Hazards}}},
  author = {Wu, Yuwei and Tao, Yuezhan and Li, Peihan and Shi, Guangyao and Sukhatme, Gaurav S. and Kumar, Vijay and Zhou, Lifeng},
  year = 2025,
  month = nov,
  number = {arXiv:2409.12274},
  eprint = {2409.12274},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.12274},
  urldate = {2026-01-26},
  abstract = {Real-time multi-robot coordination in hazardous and adversarial environments requires fast, reliable adaptation to dynamic threats. While Large Language Models (LLMs) offer strong high-level reasoning capabilities, the lack of safety guarantees limits their direct use in critical decision-making. In this paper, we propose a hierarchical optimization framework that integrates LLMs into the decision loop for multi-robot target tracking in dynamic and hazardous environments. Rather than generating control actions directly, LLMs are used to generate task configuration and adjust parameters in a bi-level task allocation and planning problem. We formulate multi-robot coordination for tracking tasks as a bi-level optimization problem, with LLMs to reason about potential hazards in the environment and the status of the robot team and modify both the inner and outer levels of the optimization. This hierarchical approach enables real-time adjustments to the robots' behavior. Additionally, a human supervisor can offer broad guidance and assessments to address unexpected dangers, model mismatches, and performance issues arising from local minima. We validate our proposed framework in both simulation and real-world experiments with comprehensive evaluations, demonstrating its effectiveness and showcasing its capability for safe LLM integration for multi-robot systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2025 - Hierarchical LLMs In-the-Loop Optimization for Real-Time Multi-Robot Target Tracking under Unknown Hazards - Wu et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\HNRDSEYA\\2409.html}
}

@misc{2025_HMCFHumanintheloop_li,
  title = {{{HMCF}}: {{A Human-in-the-loop Multi-Robot Collaboration Framework Based}} on {{Large Language Models}}},
  shorttitle = {{{HMCF}}},
  author = {Li, Zhaoxing and Wu, Wenbo and Wang, Yue and Xu, Yanran and Hunt, William and Stein, Sebastian},
  year = 2025,
  month = may,
  number = {arXiv:2505.00820},
  eprint = {2505.00820},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.00820},
  urldate = {2026-01-30},
  abstract = {Rapid advancements in artificial intelligence (AI) have enabled robots to performcomplex tasks autonomously with increasing precision. However, multi-robot systems (MRSs) face challenges in generalization, heterogeneity, and safety, especially when scaling to large-scale deployments like disaster response. Traditional approaches often lack generalization, requiring extensive engineering for new tasks and scenarios, and struggle with managing diverse robots. To overcome these limitations, we propose a Human-in-the-loop Multi-Robot Collaboration Framework (HMCF) powered by large language models (LLMs). LLMs enhance adaptability by reasoning over diverse tasks and robot capabilities, while human oversight ensures safety and reliability, intervening only when necessary. Our framework seamlessly integrates human oversight, LLM agents, and heterogeneous robots to optimize task allocation and execution. Each robot is equipped with an LLM agent capable of understanding its capabilities, converting tasks into executable instructions, and reducing hallucinations through task verification and human supervision. Simulation results show that our framework outperforms state-of-the-art task planning methods, achieving higher task success rates with an improvement of 4.76\%. Real-world tests demonstrate its robust zero-shot generalization feature and ability to handle diverse tasks and environments with minimal human intervention.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  annotation = {5 citations (Semantic Scholar/DOI) [2026-01-30]},
  file = {G:\My Drive\03 Resources\Literature\2025 - HMCF A Human-in-the-loop Multi-Robot Collaboration Framework Based on Large Language Models - Li et al..pdf}
}

@inproceedings{2025_ICCOLearning_yano,
  title = {{{ICCO}}: {{Learning}} an {{Instruction-conditioned Coordinator}} for {{Language-guided Task-aligned Multi-robot Control}}},
  shorttitle = {{{ICCO}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Yano, Yoshiki and Shibata, Kazuki and Kokshoorn, Maarten and Matsubara, Takamitsu},
  year = 2025,
  month = oct,
  pages = {18700--18707},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11247312},
  urldate = {2026-02-16},
  abstract = {Recent advances in Large Language Models (LLMs) have permitted the development of language-guided multi-robot systems, which allow robots to execute tasks based on natural language instructions. However, achieving effective coordination in distributed multi-agent environments remains challenging due to (1) misalignment between instructions and task requirements and (2) inconsistency in robot behaviors when they independently interpret ambiguous instructions. To address these challenges, we propose Instruction-Conditioned Coordinator (ICCO), a Multi-Agent Reinforcement Learning (MARL) framework designed to enhance coordination in language-guided multi-robot systems. ICCO consists of a Coordinator agent and multiple Local Agents, where the Coordinator generates Task-Aligned and Consistent Instructions (TACI) by integrating language instructions with environmental states, ensuring task alignment and behavioral consistency. The Coordinator and Local Agents are jointly trained to optimize a reward function that balances task efficiency and instruction following. A Consistency Enhancement Term is added to the learning objective to maximize mutual information between instructions and robot behaviors, further improving coordination. Simulation and real-world experiments validate the effectiveness of ICCO in achieving language-guided task-aligned multi-robot control. The demonstration can be found at https://yanoyoshiki.github.io/ICCO/.},
  keywords = {Intelligent robots,Large language models,Multi-robot systems,Mutual information,Natural languages,Prompt engineering,Reinforcement learning,Robot kinematics},
  annotation = {1 citations (Semantic Scholar/DOI) [2026-02-18]\\
1 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - ICCO Learning an Instruction-conditioned Coordinator for Language-guided Task-aligned Multi-robot Control - Yano et al..pdf}
}

@article{2025_IntegratingLarge_liu,
  title = {Integrating {{Large Language Models}} into {{Robotic Autonomy}}: {{A Review}} of {{Motion}}, {{Voice}}, and {{Training Pipelines}}},
  shorttitle = {Integrating {{Large Language Models}} into {{Robotic Autonomy}}},
  author = {Liu, Yutong and Sun, Qingquan and Kapadia, Dhruvi Rajeshkumar},
  year = 2025,
  month = jul,
  journal = {AI},
  volume = {6},
  number = {7},
  pages = {158},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2673-2688},
  doi = {10.3390/ai6070158},
  urldate = {2026-01-26},
  abstract = {This survey provides a comprehensive review of the integration of large language models (LLMs) into autonomous robotic systems, organized around four key pillars: locomotion, navigation, manipulation, and voice-based interaction. We examine how LLMs enhance robotic autonomy by translating high-level natural language commands into low-level control signals, supporting semantic planning and enabling adaptive execution. Systems like SayTap improve gait stability through LLM-generated contact patterns, while TrustNavGPT achieves a 5.7\% word error rate (WER) under noisy voice-guided conditions by modeling user uncertainty. Frameworks such as MapGPT, LLM-Planner, and 3D-LOTUS++ integrate multi-modal data---including vision, speech, and proprioception---for robust planning and real-time recovery. We also highlight the use of physics-informed neural networks (PINNs) to model object deformation and support precision in contact-rich manipulation tasks. To bridge the gap between simulation and real-world deployment, we synthesize best practices from benchmark datasets (e.g., RH20T, Open X-Embodiment) and training pipelines designed for one-shot imitation learning and cross-embodiment generalization. Additionally, we analyze deployment trade-offs across cloud, edge, and hybrid architectures, emphasizing latency, scalability, and privacy. The survey concludes with a multi-dimensional taxonomy and cross-domain synthesis, offering design insights and future directions for building intelligent, human-aligned robotic systems powered by LLMs.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {autonomous navigation,cloud-edge hybrid architecture,human-robot interaction (HRI),large language models (LLMs),multi-modal datasets,physics-informed neural networks (PINNs),reinforcement learning,robot manipulation,semantic reasoning,simulation-to-real transfer,survey,Task and Motion Planning (TAMP),voice-based interaction},
  file = {G:\My Drive\03 Resources\Literature\2025 - Integrating Large Language Models into Robotic Autonomy A Review of Motion, Voice, and Training Pipelines - Liu et al. 1.pdf}
}

@article{2025_IntegratingLarge_xue,
  title = {Integrating Large Language Models for Intuitive Robot Navigation},
  author = {Xue, Ziheng and Elksnis, Arturs and Wang, Ning},
  year = 2025,
  month = sep,
  journal = {Frontiers in Robotics and AI},
  volume = {12},
  publisher = {Frontiers},
  issn = {2296-9144},
  doi = {10.3389/frobt.2025.1627937},
  urldate = {2026-01-26},
  abstract = {Home assistance robots face challenges in natural language interaction, object detection, and navigation, mainly when operating in resource-constrained home environments, which limits their practical deployment. In this study, we propose an AI agent framework based on Large Language Models (LLMs), which includes EnvNet, RoutePlanner, and AIBrain, to explore solutions for these issues. Utilizing quantized LLMs allows the system to operate on resource-limited devices while maintaining robust interaction capabilities. Our proposed method shows promising results in improving natural language understanding and navigation accuracy in home environments, also providing a valuable exploration for deploying home assistance robots.},
  langid = {english},
  keywords = {AI agent,home assistance robots,large language models,LoRA fine-tuning,quantization},
  file = {G:\My Drive\03 Resources\Literature\2025 - Integrating large language models for intuitive robot navigation - Xue et al..pdf}
}

@misc{2025_IntegratingQuantized_gonzalez-santamarta,
  title = {Integrating {{Quantized LLMs}} into {{Robotics Systems}} as {{Edge AI}} to {{Leverage}} Their {{Natural Language Processing Capabilities}}},
  author = {{Gonz{\'a}lez-Santamarta}, Miguel {\'A} and {Rodr{\'i}guez-Lera}, Francisco J. and {Sobr{\'i}n-Hidalgo}, David and {Guerrero-Higueras}, {\'A}ngel Manuel and {Matell{\'A}n-Olivera}, Vicente},
  year = 2025,
  month = jun,
  number = {arXiv:2506.09581},
  eprint = {2506.09581},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.09581},
  urldate = {2026-01-26},
  abstract = {Large Language Models (LLMs) have experienced great advancements in the last year resulting in an increase of these models in several fields to face natural language tasks. The integration of these models in robotics can also help to improve several aspects such as human-robot interaction, navigation, planning and decision-making. Therefore, this paper introduces llama\textbackslash\_ros, a tool designed to integrate quantized Large Language Models (LLMs) into robotic systems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine, llama\textbackslash\_ros enables the efficient execution of quantized LLMs as edge artificial intelligence (AI) in robotics systems with resource-constrained environments, addressing the challenges of computational efficiency and memory limitations. By deploying quantized LLMs, llama\textbackslash\_ros empowers robots to leverage the natural language understanding and generation for enhanced decision-making and interaction which can be paired with prompt engineering, knowledge graphs, ontologies or other tools to improve the capabilities of autonomous robots. Additionally, this paper provides insights into some use cases of using llama\textbackslash\_ros for planning and explainability in robotics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  annotation = {1 citations (Semantic Scholar/DOI) [2026-01-26]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2025 - Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities - González-Santamarta et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\SDMUR86C\\2506.html}
}

@inproceedings{2025_LaMMAPGeneralizable_zhang,
  title = {{{LaMMA-P}}: {{Generalizable Multi-Agent Long-Horizon Task Allocation}} and {{Planning}} with {{LM-Driven PDDL Planner}}},
  shorttitle = {{{LaMMA-P}}},
  booktitle = {2025 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Zhang, Xiaopan and Qin, Hao and Wang, Fuquan and Dong, Yue and Li, Jiachen},
  year = 2025,
  month = may,
  pages = {10221--10221},
  doi = {10.1109/ICRA55743.2025.11127951},
  urldate = {2026-01-30},
  abstract = {Language models (LMs) possess a strong capability to comprehend natural language, making them effective in translating human instructions into detailed plans for simple robot tasks. Nevertheless, it remains a significant challenge to handle long-horizon tasks, especially in subtask identification and allocation for cooperative heterogeneous robot teams. To address this issue, we propose a Language Model-Driven MultiAgent PDDL Planner (LaMMA-P), a novel multi-agent task planning framework that achieves state-of-the-art performance on long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoning capability and the traditional heuristic search planner to achieve a high success rate and efficiency while demonstrating strong generalization across tasks. Additionally, we create MAT-THOR, a comprehensive benchmark that features household tasks with two different levels of complexity based on the AI2-THOR environment. The experimental results demonstrate that LaMMA-P achieves a 105\% higher success rate and 36 \% higher efficiency than existing LM-based multiagent planners. The experimental videos, code, datasets, and detailed prompts used in each module can be found on the project website: https://lamma-p.github.io.},
  keywords = {Benchmark testing,Codes,Cognition,Complexity theory,Multi-robot systems,Planning,Resource management,Robots,Translation,Videos},
  annotation = {28 citations (Semantic Scholar/DOI) [2026-02-18]\\
28 citations (Semantic Scholar/DOI) [2026-02-16]\\
6 citations (Crossref/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - LaMMA-P Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner - Zhang et al..pdf}
}

@inproceedings{2025_LargeLanguage_kwon,
  title = {Large {{Language Model Based Autonomous Task Planning}} for {{Abstract Commands}}},
  booktitle = {2025 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Kwon, Seokjoon and Park, Jae-Hyeon and Jang, Hee-Deok and Roh, CheolLae and Chang, Dong Eui},
  year = 2025,
  month = may,
  pages = {11010--11016},
  doi = {10.1109/ICRA55743.2025.11128343},
  urldate = {2026-02-16},
  abstract = {Recent advances in large language models (LLMs) have demonstrated exceptional reasoning capabilities in natural language processing, sparking interest in applying LLMs to task planning problems in robotics. Most studies focused on task planning for clear natural language commands that specify target objects and their locations. However, for more user-friendly task execution, it is crucial for robots to autonomously plan and carry out tasks based on abstract natural language commands that may not explicitly mention target objects or locations, such as `Put the food ingredients in the same place.' In this study, we propose an LLM-based autonomous task planning framework that generates task plans for abstract natural language commands. This framework consists of two phases: an environment recognition phase and a task planning phase. In the environment recognition phase, a large vision-language model generates a hierarchical scene graph that captures the relationships between objects and spaces in the environment surrounding a robot agent. During the task planning phase, an LLM uses the scene graph and the abstract user command to formulate a plan for the given task. We validate the effectiveness of the proposed framework in the AI2THOR simulation environment, demonstrating its superior performance in task execution when handling abstract commands.},
  keywords = {Cognition,Large language models,Natural language processing,Planning,Real-time systems,Robots,Semantics},
  annotation = {1 citations (Semantic Scholar/DOI) [2026-02-18]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Large Language Model Based Autonomous Task Planning for Abstract Commands - Kwon et al..pdf}
}

@misc{2025_LargeLanguage_li,
  title = {Large {{Language Models}} for {{Multi-Robot Systems}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}} for {{Multi-Robot Systems}}},
  author = {Li, Peihan and An, Zijian and Abrar, Shams and Zhou, Lifeng},
  year = 2025,
  month = sep,
  number = {arXiv:2502.03814},
  eprint = {2502.03814},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.03814},
  urldate = {2026-01-26},
  abstract = {The rapid advancement of Large Language Models (LLMs) has opened new possibilities in Multi-Robot Systems (MRS), enabling enhanced communication, task planning, and human-robot interaction. Unlike traditional single-robot and multi-agent systems, MRS poses unique challenges, including coordination, scalability, and real-world adaptability. This survey provides the first comprehensive exploration of LLM integration into MRS. It systematically categorizes their applications across high-level task allocation, mid-level motion planning, low-level action generation, and human intervention. We highlight key applications in diverse domains, such as household robotics, construction, formation control, target tracking, and robot games, showcasing the versatility and transformative potential of LLMs in MRS. Furthermore, we examine the challenges that limit adapting LLMs in MRS, including mathematical reasoning limitations, hallucination, latency issues, and the need for robust benchmarking systems. Finally, we outline opportunities for future research, emphasizing advancements in fine-tuning, reasoning techniques, and task-specific models. This survey aims to guide researchers in the intelligence and real-world deployment of MRS powered by LLMs. Based on the fast-evolving nature of research in the field, we keep updating the papers in the open-source GitHub repository.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2025 - Large Language Models for Multi-Robot Systems A Survey - Li et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\5S988EWC\\2502.html}
}

@inproceedings{2025_LeveragingLLMs_zuzuarregui,
  title = {Leveraging {{LLMs}} for {{Mission Planning}} in {{Precision Agriculture}}},
  booktitle = {2025 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Zuzu{\'a}rregui, Marcos Abel and Carpin, Stefano},
  year = 2025,
  month = may,
  pages = {7146--7152},
  doi = {10.1109/ICRA55743.2025.11128633},
  urldate = {2026-02-16},
  abstract = {Robotics and artificial intelligence hold significant potential for advancing precision agriculture. While robotic systems have been successfully deployed for various tasks, adapting them to perform diverse missions remains challenging, particularly because end users often lack technical expertise. In this paper, we present an end-to-end system that leverages large language models (LLMs), specifically ChatGPT, to enable users to assign complex data collection tasks to autonomous robots using natural language instructions. To enhance reusability, mission plans are encoded using an existing IEEE task specification standard, and are executed on robots via ROS2 nodes that bridge high-level mission descriptions with existing ROS libraries. Through extensive experiments, we highlight the strengths and limitations of LLMs in this context, particularly regarding spatial reasoning and solving complex routing challenges, and show how our proposed implementation-overcomes them.},
  keywords = {Chatbots,Cognition,Data collection,Large language models,Libraries,Natural languages,Planning,Precision agriculture,Routing,Standards},
  annotation = {1 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Leveraging LLMs for Mission Planning in Precision Agriculture - Zuzuárregui and Carpin.pdf}
}

@article{2025_LiPLLMIntegrating_obataa,
  title = {{{LiP-LLM}}: {{Integrating Linear Programming}} and {{Dependency Graph With Large Language Models}} for {{Multi-Robot Task Planning}}},
  shorttitle = {{{LiP-LLM}}},
  author = {Obata, Kazuma and Aoki, Tatsuya and Horii, Takato and Taniguchi, Tadahiro and Nagai, Takayuki},
  year = 2025,
  month = feb,
  journal = {IEEE Robotics and Automation Letters},
  volume = {10},
  number = {2},
  pages = {1122--1129},
  issn = {2377-3766},
  doi = {10.1109/LRA.2024.3518105},
  urldate = {2026-02-16},
  abstract = {This study proposes LiP-LLM: integrating linear programming and dependency graph with large language models (LLMs) for multi-robot task planning. For multi-robots to efficiently perform tasks, it is necessary to manage the precedence dependencies between tasks. Although multi-robot decentralized and centralized task planners using LLMs have been proposed, none of these studies focus on precedence dependencies from the perspective of task efficiency or leverage traditional optimization methods. It addresses key challenges in managing dependencies between skills and optimizing task allocation. LiP-LLM consists of three steps: skill list generation and dependency graph generation by LLMs, as well as task allocation using linear programming. The LLMs are utilized to generate a comprehensive list of skills and to construct a dependency graph that maps the relationships and sequential constraints among these skills. To ensure the feasibility and efficiency of skill execution, the skill list is generated by calculated likelihood, and linear programming is used to optimally allocate tasks to each robot. Experimental evaluations in simulated environments demonstrate that this method outperforms existing task planners, achieving higher success rates and efficiency in executing complex, multi-robot tasks. The results indicate the potential of combining LLMs with optimization techniques to enhance the capabilities of multi-robot systems in executing coordinated tasks accurately and efficiently. In an environment with two robots, a maximum success rate difference of 0.82 is observed in the language instruction group with a change in the object name.},
  keywords = {Linear programming,Multi-robot systems,Natural languages,Navigation,Optimization,Planning,Programming,Resource management,Robot kinematics,Robots,task planning},
  annotation = {24 citations (Semantic Scholar/DOI) [2026-02-18]\\
23 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - LiP-LLM Integrating Linear Programming and Dependency Graph With Large Language Models for Multi-Robot Task Planning - Obata et al. 1.pdf}
}

@article{2025_LLMBasedMultiAgent_sun,
  title = {{{LLM-Based Multi-Agent Decision-Making}}: {{Challenges}} and {{Future Directions}}},
  shorttitle = {{{LLM-Based Multi-Agent Decision-Making}}},
  author = {Sun, Chuanneng and Huang, Songjun and Pompili, Dario},
  year = 2025,
  month = jun,
  journal = {IEEE Robotics and Automation Letters},
  volume = {10},
  number = {6},
  pages = {5681--5688},
  issn = {2377-3766},
  doi = {10.1109/LRA.2025.3562371},
  urldate = {2026-02-16},
  abstract = {In recent years, Large Language Models (LLMs) have shown great abilities in various tasks, including question answering, arithmetic problem solving, and poetry writing, among others. Although research on LLM-as-an-agent has shown that LLM can be applied to Decision-Making (DM) and achieve decent results, the extension of LLM-based agents to Multi-Agent DM (MADM) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in the DM frameworks of a single agent. To inspire more research on LLM-based MADM, in this letter, we survey the existing LLM-based single-agent and multi-agent decision-making frameworks and provide potential research directions for future research. In particular, we focus on the cooperative tasks of multiple agents with a common goal and communication among them. We also consider human-in/on-the-loop scenarios enabled by the language component in the framework.},
  keywords = {Adaptation models,Decision making,Multi-agent system,Multi-agent systems,natural language models,Natural language processing,Observability,Recurrent neural networks,Reinforcement learning,robotics,Surveys,System performance,Training},
  annotation = {26 citations (Semantic Scholar/DOI) [2026-02-18]},
  file = {G:\My Drive\03 Resources\Literature\2025 - LLM-Based Multi-Agent Decision-Making Challenges and Future Directions - Sun et al..pdf}
}

@inproceedings{2025_LLMCBTLLMDriven_tian,
  title = {{{LLM-CBT}}: {{LLM-Driven Closed-Loop Behavior Tree Planning}} for {{Heterogeneous UAV-UGV Swarm Collaboration}}},
  shorttitle = {{{LLM-CBT}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Tian, Yuanyuan and Song, Weilong and Fu, Jinna and Li, Zhenhui and Fang, Chenyu and Wang, Linbo and Hu, Wanyang and Liu, Yabo},
  year = 2025,
  month = oct,
  pages = {3581--3586},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11246793},
  urldate = {2026-02-16},
  abstract = {The heterogeneous cluster system holds significant application potential in scenarios such as collaborative logistics, disaster response operations, and precision agriculture, but achieving effective task planning for its subsystems remains a challenging issue due to specialized robotic hardware and distinct action spaces. To this end, an innovative framework called LLM-driven Closed-Loop Behavior Tree (LLM-CBT) is proposed. LLMs and behavior trees (BTs) are integrated for task planning in heterogeneous unmanned clusters, including Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs). Particularly, a novel mechanism, Generation-Refinement-Execution-Feedback (GREF), is introduced, in which an initial behavior tree is generated by LLM and iteratively refined. The refined behavior tree is then executed, and adjustments are made based on the execution results, forming a closed-loop process that ultimately achieves the task objectives. In this way, the executability of BTs is improved, and the robustness of task execution in dynamic environments is enhanced. Experiments were conducted across three scenarios with varying task complexity. The results show that the GREF closed-loop mechanism is essential for the effective operation of heterogeneous unmanned clusters.},
  keywords = {Collaboration,Complexity theory,Logistics,Planning,Precision agriculture,Real-time systems,Robot kinematics,Robustness,Trees (botanical),Vehicle dynamics},
  file = {G:\My Drive\03 Resources\Literature\2025 - LLM-CBT LLM-Driven Closed-Loop Behavior Tree Planning for Heterogeneous UAV-UGV Swarm Collaboration - Tian et al..pdf}
}

@inproceedings{2025_LLMDrivenHierarchical_wang,
  title = {{{LLM-Driven Hierarchical Planning}}: {{Long-horizon Task Allocation}} for {{Multi-Robot Systems}} in {{Cross-Regional Environments}}},
  shorttitle = {{{LLM-Driven Hierarchical Planning}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Wang, Yachao and Dong, Yangshuo and Yang, Yunting and Zhang, Xiang and Wang, Yinchuan and Wang, Yuhan and Wang, Chaoqun and Meng, Max Q.-H.},
  year = 2025,
  month = oct,
  pages = {14140--14147},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11247013},
  urldate = {2026-02-16},
  abstract = {Long-horizon composite task planning for multi-robot systems in cross-regional complex scenarios faces dual challenges: spatial-semantic comprehension of natural language described tasks and collaborative optimization of subtask al-location. To address these challenges, this paper proposes a progressive three-stage task planning framework. First, an augmented scene graph is constructed to enable large language models (LLMs) to comprehend environmental structures, thereby generating simplified Linear Temporal Logic (LTL) task sequences. Subsequently, a novel heuristic function is employed to select optimal task allocation plans. Finally, LLMs are used to generate low-level executable robot instructions based on robotic system instruction templates. We establish a long-horizon composite task dataset for experimental validation on real-world quadrupedal multi-robot systems. Experimental results demonstrate the effectiveness of our approach in resolving cross-regional composite tasks.},
  keywords = {Intelligent robots,Large language models,Logic,Multi-robot systems,Natural languages,Optimization,Planning,Quadrupedal robots,Resource management,Spatial resolution},
  annotation = {0 citations (Semantic Scholar/DOI) [2026-02-18]},
  file = {G:\My Drive\03 Resources\Literature\2025 - LLM-Driven Hierarchical Planning Long-horizon Task Allocation for Multi-Robot Systems in Cross-Regional Environments - Wang et al..pdf}
}

@misc{2025_LLMFlockDecentralized_li,
  title = {{{LLM-Flock}}: {{Decentralized Multi-Robot Flocking}} via {{Large Language Models}} and {{Influence-Based Consensus}}},
  shorttitle = {{{LLM-Flock}}},
  author = {Li, Peihan and Zhou, Lifeng},
  year = 2025,
  month = may,
  number = {arXiv:2505.06513},
  eprint = {2505.06513},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.06513},
  urldate = {2026-01-30},
  abstract = {Large Language Models (LLMs) have advanced rapidly in recent years, demonstrating strong capabilities in problem comprehension and reasoning. Inspired by these developments, researchers have begun exploring the use of LLMs as decentralized decision-makers for multi-robot formation control. However, prior studies reveal that directly applying LLMs to such tasks often leads to unstable and inconsistent behaviors, where robots may collapse to the centroid of their positions or diverge entirely due to hallucinated reasoning, logical inconsistencies, and limited coordination awareness. To overcome these limitations, we propose a novel framework that integrates LLMs with an influence-based plan consensus protocol. In this framework, each robot independently generates a local plan toward the desired formation using its own LLM. The robots then iteratively refine their plans through a decentralized consensus protocol that accounts for their influence on neighboring robots. This process drives the system toward a coherent and stable flocking formation in a fully decentralized manner. We evaluate our approach through comprehensive simulations involving both state-of-the-art closed-source LLMs (e.g., o3-mini, Claude 3.5) and open-source models (e.g., Llama3.1-405b, Qwen-Max, DeepSeek-R1). The results show notable improvements in stability, convergence, and adaptability over previous LLM-based methods. We further validate our framework on a physical team of Crazyflie drones, demonstrating its practical viability and effectiveness in real-world multi-robot systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  annotation = {3 citations (Semantic Scholar/DOI) [2026-01-30]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2025 - LLM-Flock Decentralized Multi-Robot Flocking via Large Language Models and Influence-Based Consensus - Li and Zhou.pdf;C\:\\Users\\asmal\\Zotero\\storage\\JKP4DT4L\\2505.html}
}

@inproceedings{2025_LookYou_mu,
  title = {Look {{Before You Leap}}: {{Using Serialized State Machine}} for {{Language Conditioned Robotic Manipulation}}},
  shorttitle = {Look {{Before You Leap}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Mu, Tong and Liu, Yihao and Armand, Mehran},
  year = 2025,
  month = oct,
  pages = {8096--8102},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11245926},
  urldate = {2026-02-16},
  abstract = {Imitation learning frameworks for robotic manipulation have drawn attention in the recent development of language model grounded robotics. However, the success of the frameworks largely depends on the coverage of the demonstration cases: When the demonstration set does not include examples of how to act in all possible situations, the action may fail and can result in cascading errors. To solve this problem, we propose a framework that uses serialized Finite State Machine (FSM) to generate demonstrations and improve the success rate in manipulation tasks requiring a long sequence of precise interactions. To validate its effectiveness, we use environmentally evolving and long-horizon puzzles that require long sequential actions. Experimental results show that our approach achieves a success rate of up to 98\% in these tasks, compared to the controlled condition using existing approaches, which only had a success rate of up to 60\%, and, in some tasks, almost failed completely. The source code for this project can be accessed at https://imitate.finite-state.com/.},
  keywords = {Automata,Codes,Hardware,Imitation learning,Intelligent robots,Power system faults,Power system protection,Sequential analysis,Solids,Source coding},
  annotation = {2 citations (Semantic Scholar/DOI) [2026-02-18]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Look Before You Leap Using Serialized State Machine for Language Conditioned Robotic Manipulation - Mu et al..pdf}
}

@inproceedings{2025_LTLCodeGenCode_rabiei,
  title = {{{LTLCodeGen}}: {{Code Generation}} of {{Syntactically Correct Temporal Logic}} for {{Robot Task Planning}}},
  shorttitle = {{{LTLCodeGen}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Rabiei, Behrad and A R, Mahesh Kumar and Dai, Zhirui and Pilla, Surya L.S.R. and Dong, Qiyue and Atanasov, Nikolay},
  year = 2025,
  month = oct,
  pages = {19240--19247},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11246351},
  urldate = {2026-02-16},
  abstract = {This paper focuses on planning robot navigation tasks from natural language specifications. We develop a modular approach, where a large language model (LLM) translates the natural language instructions into a linear temporal logic (LTL) formula with propositions defined by object classes in a semantic occupancy map. The LTL formula and the semantic occupancy map are provided to a motion planning algorithm to generate a collision-free robot path that satisfies the natural language instructions. Our main contribution is LTLCodeGen, a method to translate natural language to syntactically correct LTL using code generation. We demonstrate the complete task planning method in real-world experiments involving human speech to provide navigation instructions to a mobile robot. We also thoroughly evaluate our approach in simulated and real-world experiments in comparison to end-to-end LLM task planning and state-of-the-art LLM-to-LTL translation methods.},
  keywords = {Codes,Collision avoidance,Logic,Mobile robots,Navigation,Planning,Semantics,Training,Training data,Translation},
  annotation = {3 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - LTLCodeGen Code Generation of Syntactically Correct Temporal Logic for Robot Task Planning - Rabiei et al..pdf}
}

@inproceedings{2025_MultiAgentSystems_chen,
  title = {Multi-{{Agent Systems}} for {{Robotic Autonomy}} with {{LLMs}}},
  booktitle = {2025 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Chen, Junhong and Yang, Ziqi and Xu, Haoyuan G and Zhang, Dandan and Mylonas, George},
  year = 2025,
  month = jun,
  pages = {4194--4204},
  issn = {2160-7516},
  doi = {10.1109/CVPRW67362.2025.00403},
  urldate = {2026-02-16},
  abstract = {Since the advent of Large Language Models (LLMs), various research based on such models have maintained significant academic attention and impact, especially in AI and robotics. In this paper, we propose a multi-agent framework with LLMs to construct an integrated system for robotic task analysis, mechanical design, and path generation. The framework includes three core agents: Task Analyst, Robot Designer, and Reinforcement Learning Designer. Outputs are formatted as multimodal results, such as code files or technical reports, for stronger understandability and usability. To evaluate generalizability comparatively, we conducted experiments with models from both GPT and DeepSeek. Results demonstrate that the proposed system can design feasible robots with control strategies when appropriate task inputs are provided, exhibiting substantial potential for enhancing the efficiency and accessibility of robotic system development in research and industrial applications.},
  keywords = {Human-machine systems,Large language model,Large language models,Multi agent systems,Multi-agent systems,Natural languages,Path Generation,Reinforcement learning,Robots,Service robots,Task analysis,Usability,Writing},
  annotation = {7 citations (Semantic Scholar/DOI) [2026-02-18]\\
7 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Multi-Agent Systems for Robotic Autonomy with LLMs - Chen et al..pdf}
}

@article{2025_NextgenerationHumanrobot_koubaa,
  title = {Next-Generation Human-Robot Interaction with {{ChatGPT}} and Robot Operating System},
  author = {Koubaa, Anis and Ammar, Adel and Boulila, Wadii},
  year = 2025,
  journal = {Software: Practice and Experience},
  volume = {55},
  number = {2},
  pages = {355--382},
  issn = {1097-024X},
  doi = {10.1002/spe.3377},
  urldate = {2026-01-26},
  abstract = {This article presents an innovative concept that harnesses the capabilities of large language models (LLMs) to revolutionize human-robot interaction. This work aims to connect large language models with the Robot Operating System (ROS), the primary development framework for robotics applications. We develop a package for ROS that seamlessly integrates ChatGPT with ROS2-based robotic systems. The core idea is to leverage prompt engineering with LLMs, utilizing unique properties such as ability eliciting, chain-of-thought, and instruction tuning. The concept employs ontology development to convert unstructured natural language commands into structured robotic instructions specific to the application context through prompt engineering. We capitalize on LLMs' zero-shots and few-shots learning capabilities by eliciting structured robotic commands from unstructured human language inputs. To demonstrate the feasibility of this concept, we implemented a proof-of-concept that integrates ChatGPT with ROS2, showcasing the transformation of human language instructions into spatial navigation commands for a ROS2-enabled robot. Besides, we quantitatively evaluated this transformation over three use cases (ground robot, unmanned aerial vehicle, and Robotic arm) and five LLMs (LLaMA-7b, LLaMA2-7b, LLaMA2-70b, GPT-3.5, and GPT-4) on a set of 3000 natural language commands. Our system serves as a new stride towards Artificial General Intelligence (AGI) and paves the way for the robotics and natural language processing communities to collaborate in creating novel, intuitive human-robot interactions. The open-source implementation of our system on ROS 2 is available on GitHub.},
  copyright = {\copyright{} 2024 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {ChatGPT,human-robot interaction,large language models,mobile robots,ontology,robot operating system},
  file = {C:\Users\asmal\Zotero\storage\L7L5K8QL\spe.html}
}

@article{2025_Nl2Hltl2PlanScaling_xu,
  title = {{{Nl2Hltl2Plan}}: {{Scaling Up Natural Language Understanding}} for {{Multi-Robots Through Hierarchical Temporal Logic Task Specifications}}},
  shorttitle = {{{Nl2Hltl2Plan}}},
  author = {Xu, Shaojun and Luo, Xusheng and Huang, Yutong and Leng, Letian and Liu, Ruixuan and Liu, Changliu},
  year = 2025,
  month = oct,
  journal = {IEEE Robotics and Automation Letters},
  volume = {10},
  number = {10},
  pages = {10482--10489},
  issn = {2377-3766},
  doi = {10.1109/LRA.2025.3598648},
  urldate = {2026-01-30},
  abstract = {To enable non-experts to specify long-horizon, multi-robot collaborative tasks, language models are increasingly used to translate natural language commands into formal specifications. However, because translation can occur in multiple ways, such translations may lack accuracy or lead to inefficient multi-robot planning. Our key insight is that concise hierarchical specifications can simplify planning while remaining straightforward to derive from human instructions. We propose Nl2Hltl2Plan, a framework that translates natural language commands into hierarchical Linear Temporal Logic (LTL) and solves the corresponding planning problem. The translation involves two steps leveraging Large Language Models (LLMs). First, an LLM transforms instructions into a Hierarchical Task Tree, capturing logical and temporal relations. Next, a fine-tuned LLM converts sub-tasks into standard LTL formulas, which are aggregated into hierarchical specifications, with the lowest level corresponding to ordered robot actions. These specifications are then used with off-the-shelf planners. Our Nl2Hltl2Plan demonstrates the potential of LLMs in hierarchical reasoning for multi-robot task planning. Evaluations in simulation and real-world experiments with human participants show that Nl2Hltl2Plan outperforms existing methods, handling more complex instructions while achieving higher success rates and lower costs in task allocation and planning.},
  keywords = {Costs,Data mining,Formal methods in robotics and automation,human-robot interaction,Logic,multi-robot systems,Natural languages,Planning,Resource management,Robots,Standards,Training,Translation},
  annotation = {5 citations (Semantic Scholar/DOI) [2026-02-18]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Nl2Hltl2Plan Scaling Up Natural Language Understanding for Multi-Robots Through Hierarchical Temporal Logic Task Specifications - Xu et al..pdf}
}

@inproceedings{2025_OpenWorldTask_tang,
  title = {Open-{{World Task Planning}} for {{Humanoid Bimanual Dexterous Manipulation}} via {{Vision-Language Models}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Tang, Zixin and Li, Zhihao and Liu, Junjia and Li, Zhuo and Chen, Fei},
  year = 2025,
  month = oct,
  pages = {21551--21558},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11247026},
  urldate = {2026-02-16},
  abstract = {Open-world task planning, characterized by handling unstructured and dynamic environments, has been increasingly explored to integrate with long-horizon robotic manipulation tasks. However, existing evaluations of the capabilities of these planners primarily focus on single-arm systems in structured scenarios with limited skill primitives, which is insufficient for numerous bimanual dexterous manipulation scenarios prevalent in the real world. To this end, we introduce OBiMan-Bench, a large-scale benchmark designed to rigorously evaluate open-world planning capabilities in bimanual dexterous manipulation, including task-scenario grounding, workspace constraint handling, and long-horizon cooperative reasoning. In addition, we propose OBiMan-Planner, a vision-language model-based zero-shot planning framework tailored for bimanual dexterous manipulation. OBiMan-Planner comprises two key components, the scenario grounding module for grounding open-world task instructions with specific scenarios and the task planning module for generating sequential stages. Extensive experiments on OBiMan-Bench demonstrate the effectiveness of our method in addressing complex bimanual dexterous manipulation tasks in open-world scenarios. The code, benchmark, and supplementary material are released at https://github.com/Zixin-Tang/OBiMan.},
  keywords = {Benchmark testing,Cognition,Constraint handling,Grounding,Humanoid robots,Intelligent robots,Manipulator dynamics,Periodic structures,Planning,Robot kinematics},
  annotation = {0 citations (Semantic Scholar/DOI) [2026-02-18]\\
0 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Open-World Task Planning for Humanoid Bimanual Dexterous Manipulation via Vision-Language Models - Tang et al..pdf}
}

@inproceedings{2025_OrchestratingHumanAI_masters,
  title = {Orchestrating {{Human-AI Teams}}: {{The Manager Agent}} as {{aUnifying Research Challenge}}},
  shorttitle = {Orchestrating {{Human-AI Teams}}},
  booktitle = {Proceedings of the 2025 7th {{International Conference}} on {{Distributed Artificial Intelligence}}},
  author = {Masters, Charlie and Vellanki, Advaith and Shangguan, Jiangbo and Kultys, Bart and Gilmore, Jonathan and Moore, Alastair and Albrecht, Stefano},
  year = 2025,
  month = dec,
  series = {{{DAI}} '25},
  pages = {91--107},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3772429.3772439},
  urldate = {2026-01-30},
  abstract = {While agentic AI has advanced in automating individual tasks, managing complex multi-agent workflows remains a challenging problem. This paper presents a research vision for autonomous agentic systems that orchestrate collaboration within dynamic human-AI teams. We propose the Autonomous Manager Agent as a core challenge: an agent that decomposes complex goals into task graphs, allocates tasks to human and AI workers, monitors progress, adapts to changing conditions, and maintains transparent stakeholder communication. We formalize workflow management as a Partially Observable Stochastic Game and identify four foundational challenges: (1) compositional reasoning for hierarchical decomposition, (2) multi-objective optimization under shifting preferences, (3) coordination and planning in ad hoc teams, and (4) governance and compliance by design. To advance this agenda, we release MA-Gym, an open-source simulation and evaluation framework for multi-agent workflow orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we find they struggle to jointly optimize for goal completion, constraint adherence, and workflow runtime--underscoring workflow management as a difficult open problem. We conclude with organizational and ethical implications of autonomous management systems.},
  isbn = {979-8-4007-2275-2},
  annotation = {4 citations (Semantic Scholar/DOI) [2026-01-30]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Orchestrating Human-AI Teams The Manager Agent as aUnifying Research Challenge - Masters et al..pdf}
}

@article{2025_ProbabilisticallyCorrect_wanga,
  title = {Probabilistically {{Correct Language-Based Multi-Robot Planning Using Conformal Prediction}}},
  author = {Wang, Jun and He, Guocheng and Kantaros, Yiannis},
  year = 2025,
  month = jan,
  journal = {IEEE Robotics and Automation Letters},
  volume = {10},
  number = {1},
  pages = {160--167},
  issn = {2377-3766},
  doi = {10.1109/LRA.2024.3504233},
  urldate = {2026-02-16},
  abstract = {This paper addresses task planning problems for language-instructed robot teams. Tasks are expressed in natural language (NL), requiring the robots to apply their skills at various locations and semantic objects. Several recent works have addressed similar planning problems by leveraging pre-trained Large Language Models (LLMs) to design effective multi-robot plans. However, these approaches lack performance guarantees. To address this challenge, we introduce a new distributed LLM-based planner, called S-ATLAS for Safe plAnning for Teams of Language-instructed AgentS, that can achieve user-defined mission success rates. This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool. CP allows the proposed multi-robot planner to reason about its inherent uncertainty, due to imperfections of LLMs, in a distributed fashion, enabling robots to make local decisions when they are sufficiently confident and seek help otherwise. We show, both theoretically and empirically, that the proposed planner can achieve user-specified task success rates, assuming successful plan execution, while minimizing the average number of help requests. We provide comparative experiments against related works showing that our method is significantly more computational efficient and achieves lower help rates.},
  keywords = {AI-enabled robotics,Computational efficiency,History,Multi-robot systems,Planning,planning under uncertainty,Probabilistic logic,Reliability,Robot kinematics,Semantics,Surveys,task and motion planning,Uncertainty,Upper bound},
  annotation = {21 citations (Semantic Scholar/DOI) [2026-02-18]\\
21 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Probabilistically Correct Language-Based Multi-Robot Planning Using Conformal Prediction - Wang et al. 1.pdf}
}

@article{2025_PromptingRobotic_benjdira,
  title = {Prompting {{Robotic Modalities}} ({{PRM}}): {{A}} Structured Architecture for Centralizing Language Models in Complex Systems},
  shorttitle = {Prompting {{Robotic Modalities}} ({{PRM}})},
  author = {Benjdira, Bilel and Koubaa, Anis and Ali, Anas M.},
  year = 2025,
  month = may,
  journal = {Future Generation Computer Systems},
  volume = {166},
  pages = {107723},
  issn = {0167-739X},
  doi = {10.1016/j.future.2025.107723},
  urldate = {2026-01-26},
  abstract = {Despite significant advancements in robotics and AI, existing systems often struggle to integrate diverse modalities (e.g., image, sound, actuator data) into a unified framework, resulting in fragmented architectures that limit adaptability, scalability, and explainability. To address these gaps, this paper introduces Prompting Robotic Modalities (PRM), a novel architecture that centralizes language models for controlling and managing complex systems through natural language. In PRM, each system modality (e.g., image, sound, actuator) is handled independently by a Modality Language Model (MLM), while a central Task Modality, powered by a Large Language Model (LLM), orchestrates complex tasks using information from the MLMs. Each MLM is trained on datasets that pair modality-specific data with rich textual descriptions, enabling intuitive, language-based interaction. We validate PRM with two main contributions: (1) ROSGPT\_Vision, a new open-source ROS 2 package (available at https://github.com/bilel-bj/ROSGPT\_Vision) for visual modality tasks, achieving up to 66\% classification accuracy in driver-focus monitoring---surpassing other tested models in its category; and (2) CarMate, a driver-distraction detection application that significantly reduces development time and cost by allowing rapid adaptation to new monitoring tasks via simple prompt adjustments. In addition, we develop a Navigation Language Model (NLM) that converts free-form human language orders into detailed ROS commands, underscoring PRM's modality-agnostic adaptability. Experimental results demonstrate that PRM simplifies system development, outperforms baseline vision-language approaches in specialized tasks (e.g., driver monitoring), reduces complexity through prompt engineering rather than extensive coding, and enhances explainability via natural-language-based diagnostics. Hence, PRM lays a promising foundation for next-generation complex and robotic systems by integrating advanced language model capabilities at their core, making them more adaptable to new environments, cost-effective, and user-friendly.},
  keywords = {Expert systems architectures,Languages models in robotics,Large language models,LLM prompt,LLMs,Prompting robotic modalities,Robotic operating system,Robotic prompt engineering,Robotics,ROS,ROS2,Vision language models,Visual prompt,VLMs},
  file = {C:\Users\asmal\Zotero\storage\J9RJLW9K\S0167739X25000184.html}
}

@inproceedings{2025_RAIFlexible_rachwal,
  title = {{{RAI}}: {{Flexible Agent Framework}} for~{{Embodied AI}}},
  shorttitle = {{{RAI}}},
  booktitle = {Highlights in {{Practical Applications}} of {{Agents}}, {{Multi-Agent Systems}} and {{Computational Social Science}}. {{The PAAMS Collection}}},
  author = {Rachwa{\l}, Kajetan and Majek, Maciej and Boczek, Bart{\l}omiej and D{\k a}browski, Kacper and Liberadzki, Pawe{\l} and D{\k a}browski, Adam and Ganzha, Maria},
  editor = {Nongaillard, Antoine and Caron, Anne-Cecile and {Gonz{\'a}lez-Briones}, Alfonso and Fern{\'a}ndez, Alberto and Dur{\~a}es, Dalila and Sharaf, Nada},
  year = 2025,
  pages = {195--206},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-032-05925-3_16},
  abstract = {With an increase in the capabilities of generative language models, a growing interest in embodied AI has followed. This contribution introduces RAI -- a framework for creating embodied Multi Agent Systems for robotics. The proposed framework implements tools for Agents' integration with robotic stacks, Large Language Models, and simulations. It provides out-of-the-box integration with state-of-the-art systems like ROS~2. It also comes with dedicated mechanisms for the embodiment of Agents. These mechanisms have been tested on a physical robot, Husarion ROSBot XL, which was coupled with its digital twin, for rapid prototyping. Furthermore, these mechanisms have been deployed in two simulations: (1)~robot arm manipulator and (2)~tractor controller. All of these deployments have been evaluated in terms of their control capabilities, effectiveness of embodiment, and perception ability. The proposed framework has been used successfully to build systems with multiple agents. It has demonstrated effectiveness in all the aforementioned tasks. It also enabled identifying and addressing the shortcomings of the generative models used for embodied AI.},
  isbn = {978-3-032-05925-3},
  langid = {english},
  keywords = {Digital Twin,Embodied AI,Generative AI,Multi-Agent Systems,Robotics,Simulations}
}

@article{2025_RALLYRoleAdaptive_wang,
  title = {{{RALLY}}: {{Role-Adaptive LLM-Driven Yoked Navigation}} for {{Agentic UAV Swarms}}},
  shorttitle = {{{RALLY}}},
  author = {Wang, Ziyao and Li, Rongpeng and Li, Sizhao and Xiang, Yuming and Wang, Haiping and Zhao, Zhifeng and Zhang, Honggang},
  year = 2025,
  journal = {IEEE Open Journal of Vehicular Technology},
  volume = {6},
  pages = {2693--2708},
  issn = {2644-1330},
  doi = {10.1109/OJVT.2025.3610852},
  urldate = {2026-01-30},
  abstract = {Intelligent control of Uncrewed Aerial Vehicles (UAVs) swarms has emerged as a critical research focus, and it typically requires the swarm to navigate effectively while avoiding obstacles and achieving continuous coverage over multiple mission targets. Although traditional Multi-Agent Reinforcement Learning (MARL) approaches offer dynamic adaptability, they are hindered by the semantic gap in black-boxed communication and the rigidity of homogeneous role structures, resulting in poor generalization and limited task scalability. Recent advances in Large Language Model (LLM)-based control frameworks demonstrate strong semantic reasoning capabilities by leveraging extensive prior knowledge. However, due to the lack of online learning and over-reliance on static priors, these works often struggle with effective exploration, leading to reduced individual potential and overall system performance. To address these limitations, we propose a Role-Adaptive LLM-Driven Yoked navigation algorithm RALLY. Specifically, we first develop an LLM-driven semantic decision framework that uses structured natural language for efficient semantic communication and collaborative reasoning. Afterward, we introduce a dynamic role-heterogeneity mechanism for adaptive role switching and personalized decision-making. Furthermore, we propose a Role-value Mixing Network (RMIX)-based assignment strategy that integrates LLM offline priors with MARL online policies to enable offline training of role selection strategies. Experiments in the Multi-Agent Particle Environment (MPE) and a Software-In-The-Loop (SITL) platform demonstrate that RALLY outperforms conventional approaches in terms of task coverage, convergence speed, and generalization, highlighting its strong potential for collaborative navigation in agentic multi-UAV systems.},
  keywords = {agentic AI,Artificial intelligence,Autonomous aerial vehicles,Cognition,Collaboration,Decision making,Heuristic algorithms,Intelligent transportation systems,Intelligent UAV swarm control,large language model,multi-agent reinforcement learning,Multi-agent systems,Navigation,role-heterogeneous network,Semantics,Training,Vehicle dynamics},
  file = {G:\My Drive\03 Resources\Literature\2025 - RALLY Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms - Wang et al..pdf}
}

@misc{2025_ReasonRFTReinforcement_tan,
  title = {Reason-{{RFT}}: {{Reinforcement Fine-Tuning}} for {{Visual Reasoning}} of {{Vision Language Models}}},
  shorttitle = {Reason-{{RFT}}},
  author = {Tan, Huajie and Ji, Yuheng and Hao, Xiaoshuai and Chen, Xiansheng and Wang, Pengwei and Wang, Zhongyuan and Zhang, Shanghang},
  year = 2025,
  month = mar,
  urldate = {2026-01-25},
  abstract = {Visual reasoning abilities play a crucial role in understanding complex multimodal data, advancing both domain-specific applications and artificial general intelligence (AGI). Existing methods enhance Vision-Language Models (VLMs) through Chain-of-Thought (CoT) supervised fine-tuning using meticulously annotated data. However, this approach may lead to overfitting and cognitive rigidity, limiting the model's generalization ability under domain shifts and reducing real-world applicability. To overcome these limitations, we propose Reason-RFT, a two-stage reinforcement fine-tuning framework for visual reasoning. First, Supervised Fine-Tuning (SFT) with curated CoT data activates the reasoning potential of VLMs. This is followed by reinforcement learning based on Group Relative Policy Optimization (GRPO), which generates multiple reasoning-response pairs to enhance adaptability to domain shifts. To evaluate Reason-RFT, we reconstructed a comprehensive dataset covering visual counting, structural perception, and spatial transformation, serving as a benchmark for systematic assessment across three key dimensions. Experimental results highlight three advantages: (1) performance enhancement, with Reason-RFT achieving state-of-the-art results and outperforming both open-source and proprietary models; (2) generalization superiority, maintaining robust performance under domain shifts across various tasks; and (3) data efficiency, excelling in few-shot learning scenarios and surpassing full-dataset SFT baselines. Reason-RFT introduces a novel training paradigm for visual reasoning and marks a significant step forward in multimodal research. Project website: https://tanhuajie.github.io/ReasonRFT},
  langid = {english},
  annotation = {10 citations (Semantic Scholar/arXiv) [2026-01-25]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Reason-RFT Reinforcement Fine-Tuning for Visual Reasoning of Vision Language Models - Tan et al..pdf}
}

@article{2025_ReviewEmbodied_zhang,
  title = {A Review of Embodied Intelligence Systems: A Three-Layer Framework Integrating Multimodal Perception, World Modeling, and Structured Strategies},
  shorttitle = {A Review of Embodied Intelligence Systems},
  author = {Zhang, Yunwei and Tian, Jing and Xiong, Qiaochu},
  year = 2025,
  month = nov,
  journal = {Frontiers in Robotics and AI},
  volume = {12},
  publisher = {Frontiers},
  issn = {2296-9144},
  doi = {10.3389/frobt.2025.1668910},
  urldate = {2026-01-25},
  abstract = {Embodied intelligent systems build upon the foundations of behavioral robotics and classical cognitive architectures. They integrate multimodal perception, world modeling, and adaptive control to support closed-loop interaction in dynamic and uncertain environments. Recent breakthroughs in Multimodal Large Models (MLMs) and World Models (WMs) are profoundly transforming this field, providing the tools to achieve its long-envisioned capabilities of semantic understanding and robust generalization. Targeting the central challenge of how modern MLMs and WMs jointly advance embodied intelligence, this review provides a comprehensive overview across key dimensions, including multimodal perception, cross-modal alignment, adaptive decision-making, and Sim-to-Real transfer. Furthermore, we systematize these components into a three-stage theoretical framework termed ``Dynamic Perception--Task Adaptation (DP-TA)''. This framework integrates multimodal perception modeling, causally driven world state prediction, and semantically guided strategy optimization, establishing a comprehensive ``perception--modeling--decision'' loop. To support this, we introduce a ``Feature-Conditioned Modal Alignment (F-CMA)'' mechanism to enhance cross-modal fusion under task constraints.},
  langid = {english},
  keywords = {cross-modal learning,embodied AI,multimodal learning,reinforcement learning,sim-to-real transfer,world models},
  file = {G:\My Drive\03 Resources\Literature\2025 - A review of embodied intelligence systems a three-layer framework integrating multimodal perception, world modeling, and structured strategies - Zhang et al..pdf}
}

@misc{2025_RoboBrain20_team,
  title = {{{RoboBrain}} 2.0 {{Technical Report}}},
  author = {Team, BAAI RoboBrain and Cao, Mingyu and Tan, Huajie and Ji, Yuheng and Chen, Xiansheng and Lin, Minglan and Li, Zhiyu and Cao, Zhou and Wang, Pengwei and Zhou, Enshen and Han, Yi and Tang, Yingbo and Xu, Xiangqi and Guo, Wei and Lyu, Yaoxu and Xu, Yijie and Shi, Jiayu and Du, Mengfei and Chi, Cheng and Zhao, Mengdi and Hao, Xiaoshuai and Zhao, Junkai and Zhang, Xiaojie and Rong, Shanyu and Lyu, Huaihai and Cai, Zhengliang and Fu, Yankai and Chen, Ning and Zhang, Bolun and Zhang, Lingfeng and Zhang, Shuyi and Liu, Dong and Feng, Xi and Wang, Songjing and Liu, Xiaodan and Jiao, Yance and Lyu, Mengsi and Chen, Zhuo and He, Chenrui and Ao, Yulong and Sun, Xue and He, Zheqi and Zheng, Jingshu and Yang, Xi and Shi, Donghai and Xie, Kunchang and Zhang, Bochao and Nie, Shaokai and Men, Chunlei and Lin, Yonghua and Wang, Zhongyuan and Huang, Tiejun and Zhang, Shanghang},
  year = 2025,
  month = jul,
  urldate = {2026-01-25},
  abstract = {We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, designed to unify perception, reasoning, and planning for complex embodied tasks in physical environments. It comes in two variants: a lightweight 7B model and a full-scale 32B model, featuring a heterogeneous architecture with a vision encoder and a language model. Despite its compact size, RoboBrain 2.0 achieves strong performance across a wide spectrum of embodied reasoning tasks. On both spatial and temporal benchmarks, the 32B variant achieves leading results, surpassing prior open-source and proprietary models. In particular, it supports key real-world embodied AI capabilities, including spatial understanding (e.g., affordance prediction, spatial referring, trajectory forecasting) and temporal decision-making (e.g., closed-loop interaction, multi-agent long-horizon planning, and scene graph updating). This report details the model architecture, data construction, multi-stage training strategies, infrastructure and practical applications. We hope RoboBrain 2.0 advances embodied AI research and serves as a practical step toward building generalist embodied agents. The code, checkpoint and benchmark are available at https://superrobobrain.github.io.},
  langid = {english},
  annotation = {47 citations (Semantic Scholar/arXiv) [2026-01-25]},
  file = {G:\My Drive\03 Resources\Literature\2025 - RoboBrain 2.0 Technical Report - Team et al..pdf}
}

@inproceedings{2025_RoboBrainUnified_ji,
  title = {{{RoboBrain}}: {{A Unified Brain Model}} for {{Robotic Manipulation}} from {{Abstract}} to {{Concrete}}},
  shorttitle = {{{RoboBrain}}},
  booktitle = {2025 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ji, Yuheng and Tan, Huajie and Shi, Jiayu and Hao, Xiaoshuai and Zhang, Yuan and Zhang, Hengyuan and Wang, Pengwei and Zhao, Mengdi and Mu, Yao and An, Pengju and Xue, Xinda and Su, Qinghang and Lyu, Huaihai and Zheng, Xiaolong and Liu, Jiaming and Wang, Zhongyuan and Zhang, Shanghang},
  year = 2025,
  month = jun,
  pages = {1724--1734},
  issn = {2575-7075},
  doi = {10.1109/CVPR52734.2025.00168},
  urldate = {2026-01-25},
  abstract = {Recent advancements in Multimodal Large Language Models (MLLMs) have shown remarkable capabilities across various multimodal contexts. However, their application in robotic scenarios, particularly for long-horizon manipulation tasks, reveals significant limitations. These limitations arise from the current MLLMs lacking three essential robotic brain capabilities: Planning Capability, which involves decomposing complex manipulation instructions into manageable sub-tasks; Affordance Perception, the ability to recognize and interpret the affordances of interactive objects; and Trajectory Prediction, the foresight to anticipate the complete manipulation trajectory necessary for successful execution. To enhance the robotic brain's core capabilities from abstract to concrete, we introduce ShareRobot, a high-quality heterogeneous dataset that labels multi-dimensional information such as task planning, object affordance, and end-effector trajectory. ShareRobot's diversity and accuracy have been meticulously refined by three human annotators. Building on this dataset, we developed RoboBrain, an MLLM-based model that combines robotic and general multi-modal data, utilizes a multi-stage training strategy, and incorporates long videos and high-resolution images to improve its robotic manipulation capabilities. Extensive experiments demonstrate that RoboBrain achieves state-of-the-art performance across various robotic tasks, highlighting its potential to advance robotic brain capabilities. Project website: RoboBrain.},
  keywords = {Affordances,Brain modeling,Data models,End effectors,Pattern recognition,Planning,Robots,Training,Trajectory,Videos},
  annotation = {95 citations (Semantic Scholar/DOI) [2026-02-18]\\
94 citations (Semantic Scholar/DOI) [2026-02-16]\\
8 citations (Crossref/DOI) [2026-02-16]\\
89 citations (Semantic Scholar/DOI) [2026-01-25]},
  file = {G:\My Drive\03 Resources\Literature\2025 - RoboBrain A Unified Brain Model for Robotic Manipulation from Abstract to Concrete - Ji et al..pdf}
}

@misc{2025_RoboDopamineGeneral_tan,
  title = {Robo-{{Dopamine}}: {{General Process Reward Modeling}} for {{High-Precision Robotic Manipulation}}},
  shorttitle = {Robo-{{Dopamine}}},
  author = {Tan, Huajie and Chen, Sixiang and Xu, Yijie and Wang, Zixiao and Ji, Yuheng and Chi, Cheng and Lyu, Yaoxu and Zhao, Zhongxia and Chen, Xiansheng and Co, Peterson and Xie, Shaoxuan and Yao, Guocai and Wang, Pengwei and Wang, Zhongyuan and Zhang, Shanghang},
  year = 2025,
  month = dec,
  urldate = {2026-01-25},
  abstract = {The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing a semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, a novel reward modeling method for learning a general-purpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on a vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, a robust policy learning framework that employs a theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to a new task in a one-shot manner from a single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95\% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: https://robo-dopamine.github.io},
  langid = {english},
  annotation = {4 citations (Semantic Scholar/arXiv) [2026-01-25]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Robo-Dopamine General Process Reward Modeling for High-Precision Robotic Manipulation - Tan et al..pdf}
}

@misc{2025_RoboOSHierarchical_tan,
  title = {{{RoboOS}}: {{A Hierarchical Embodied Framework}} for {{Cross-Embodiment}} and {{Multi-Agent Collaboration}}},
  shorttitle = {{{RoboOS}}},
  author = {Tan, Huajie and Hao, Xiaoshuai and Chi, Cheng and Lin, Minglan and Lyu, Yaoxu and Cao, Mingyu and Liang, Dong and Chen, Zhuo and Lyu, Mengsi and Peng, Cheng and He, Chenrui and Ao, Yulong and Lin, Yonghua and Wang, Pengwei and Wang, Zhongyuan and Zhang, Shanghang},
  year = 2025,
  month = may,
  urldate = {2026-01-25},
  abstract = {The dawn of embodied intelligence has ushered in an unprecedented imperative for resilient, cognition-enabled multi-agent collaboration across next-generation ecosystems, revolutionizing paradigms in autonomous manufacturing, adaptive service robotics, and cyber-physical production architectures. However, current robotic systems face significant limitations, such as limited cross-embodiment adaptability, inefficient task scheduling, and insufficient dynamic error correction. While End-to-end VLA models demonstrate inadequate long-horizon planning and task generalization, hierarchical VLA models suffer from a lack of cross-embodiment and multi-agent coordination capabilities. To address these challenges, we introduce RoboOS, the first open-source embodied system built on a Brain-Cerebellum hierarchical architecture, enabling a paradigm shift from single-agent to multi-agent intelligence. Specifically, RoboOS consists of three key components: (1) Embodied Brain Model (RoboBrain), a MLLM designed for global perception and high-level decision-making; (2) Cerebellum Skill Library, a modular, plug-and-play toolkit that facilitates seamless execution of multiple skills; and (3) Real-Time Shared Memory, a spatiotemporal synchronization mechanism for coordinating multi-agent states. By integrating hierarchical information flow, RoboOS bridges Embodied Brain and Cerebellum Skill Library, facilitating robust planning, scheduling, and error correction for long-horizon tasks, while ensuring efficient multi-agent collaboration through Real-Time Shared Memory. Furthermore, we enhance edge-cloud communication and cloud-based distributed inference to facilitate high-frequency interactions and enable scalable deployment. Extensive real-world experiments across various scenarios, demonstrate RoboOS's versatility in supporting heterogeneous embodiments. Project website: https://github.com/FlagOpen/RoboOS},
  langid = {english},
  file = {G:\My Drive\03 Resources\Literature\2025 - RoboOS A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration - Tan et al. 1.pdf}
}

@misc{2025_RoboOSNeXTUnified_tan,
  title = {{{RoboOS-NeXT}}: {{A Unified Memory-based Framework}} for {{Lifelong}}, {{Scalable}}, and {{Robust Multi-Robot Collaboration}}},
  shorttitle = {{{RoboOS-NeXT}}},
  author = {Tan, Huajie and Chi, Cheng and Chen, Xiansheng and Ji, Yuheng and Zhao, Zhongxia and Hao, Xiaoshuai and Lyu, Yaoxu and Cao, Mingyu and Zhao, Junkai and Lyu, Huaihai and Zhou, Enshen and Chen, Ning and Fu, Yankai and Peng, Cheng and Guo, Wei and Liang, Dong and Chen, Zhuo and Lyu, Mengsi and He, Chenrui and Ao, Yulong and Lin, Yonghua and Wang, Pengwei and Wang, Zhongyuan and Zhang, Shanghang},
  year = 2025,
  month = oct,
  urldate = {2026-01-25},
  abstract = {The proliferation of collaborative robots across diverse tasks and embodiments presents a central challenge: achieving lifelong adaptability, scalable coordination, and robust scheduling in multi-agent systems. Existing approaches, from vision-language-action (VLA) models to hierarchical frameworks, fall short due to their reliance on limited or dividual-agent memory. This fundamentally constrains their ability to learn over long horizons, scale to heterogeneous teams, or recover from failures, highlighting the need for a unified memory representation. To address these limitations, we introduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable, and robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel Spatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene geometry, temporal event history, and embodiment profiles into a shared representation. This memory-centric design is integrated into a brain-cerebellum framework, where a high-level brain model performs global planning by retrieving and updating STEM, while low-level controllers execute actions locally. This closed loop between cognition, memory, and execution enables dynamic task allocation, fault-tolerant collaboration, and consistent state synchronization. We conduct extensive experiments spanning complex coordination tasks in restaurants, supermarkets, and households. Our results demonstrate that RoboOS-NeXT achieves superior performance across heterogeneous embodiments, validating its effectiveness in enabling lifelong, scalable, and robust multi-robot collaboration. Project website: https://flagopen.github.io/RoboOS/},
  langid = {english},
  annotation = {5 citations (Semantic Scholar/arXiv) [2026-01-25]},
  file = {G:\My Drive\03 Resources\Literature\2025 - RoboOS-NeXT A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration - Tan et al..pdf}
}

@misc{2025_RoboReferSpatial_zhou,
  title = {{{RoboRefer}}: {{Towards Spatial Referring}} with {{Reasoning}} in {{Vision-Language Models}} for {{Robotics}}},
  shorttitle = {{{RoboRefer}}},
  author = {Zhou, Enshen and An, Jingkun and Chi, Cheng and Han, Yi and Rong, Shanyu and Zhang, Chi and Wang, Pengwei and Wang, Zhongyuan and Huang, Tiejun and Sheng, Lu and Zhang, Shanghang},
  year = 2025,
  month = jun,
  urldate = {2026-01-25},
  abstract = {Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the instruction-indicated locations for interaction. To this end, we propose RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding by integrating a disentangled but dedicated depth encoder via supervised fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process reward functions tailored for spatial referring tasks. To support SFT and RFT training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x prior), covering 31 spatial relations (vs. 15 prior) and supporting complex reasoning processes (up to 5 steps). In addition, we introduce RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial referring with multi-step reasoning. Experiments show that SFT-trained RoboRefer achieves state-of-the-art spatial understanding, with an average success rate of 89.6\%. RFT-trained RoboRefer further outperforms all other baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4\% in average accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (e,g., UR5, G1 humanoid) in cluttered real-world scenes.},
  langid = {english},
  file = {G:\My Drive\03 Resources\Literature\2025 - RoboRefer Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics - Zhou et al..pdf}
}

@article{2025_RobotLearning_xiao,
  title = {Robot Learning in the Era of Foundation Models: A Survey},
  shorttitle = {Robot Learning in the Era of Foundation Models},
  author = {Xiao, Xuan and Liu, Jiahang and Wang, Zhipeng and Zhou, Yanmin and Qi, Yong and Jiang, Shuo and He, Bin and Cheng, Qian},
  year = 2025,
  month = jul,
  journal = {Neurocomputing},
  volume = {638},
  pages = {129963},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2025.129963},
  urldate = {2025-12-10},
  abstract = {The proliferation of large language models (LLMs) has fueled a shift in robot learning from automation towards general embodied artificial intelligence (AI). Adopting foundation models together with traditional learning methods for robot learning has increasingly gained interest in the research community and shown potential for real-life application. However, there is little literature that comprehensively reviews the relatively new technologies combined with robotics. The purpose of this review is to systematically assess the state-of-the-art foundation models in robot learning and to identify future potential areas. Specifically, we first summarized the technical evolution of robot learning and identified the necessary preliminary preparations for foundation models, including the simulators, datasets, and foundation model framework. In addition, we focused on the following four mainstream areas of robot learning, including manipulation, navigation, task planning, and reasoning, and demonstrated how the foundation model can be adopted in the above scenarios. Furthermore, critical issues that are neglected in the current literature, including robot hardware and software decoupling, dynamic data, generalization performance in the presence of humans, etc., were discussed. This review highlights the state-of-the-art progress of foundation models in robot learning. Future research should focus on multimodal interaction, especially dynamics data, robotics-specific foundation models, AI alignment, etc.},
  keywords = {Embodied Artificial Intelligence,Foundation Models,Robot Learning,survey},
  annotation = {12 citations (Semantic Scholar/DOI) [2025-12-16]\\
10 citations (Crossref/DOI) [2025-12-16]\\
11 citations (Semantic Scholar/DOI) [2025-12-10]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2025 - Robot learning in the era of foundation models a survey - Xiao et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\WKF68ZUV\\S0925231225006356.html}
}

@misc{2025_RoboTracerMastering_zhou,
  title = {{{RoboTracer}}: {{Mastering Spatial Trace}} with {{Reasoning}} in {{Vision-Language Models}} for {{Robotics}}},
  shorttitle = {{{RoboTracer}}},
  author = {Zhou, Enshen and Chi, Cheng and Li, Yibo and An, Jingkun and Zhang, Jiayuan and Rong, Shanyu and Han, Yi and Ji, Yuheng and Liu, Mengzhen and Wang, Pengwei and Wang, Zhongyuan and Sheng, Lu and Zhang, Shanghang},
  year = 2025,
  month = dec,
  urldate = {2026-01-25},
  abstract = {Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1\%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36\% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes. See the project page at https://zhoues.github.io/RoboTracer.},
  langid = {english},
  annotation = {4 citations (Semantic Scholar/arXiv) [2026-01-25]},
  file = {G:\My Drive\03 Resources\Literature\2025 - RoboTracer Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics - Zhou et al..pdf}
}

@misc{2025_ROS2_aloqaily,
  title = {{{ROS}} 2 in a {{Nutshell}}: {{A Survey}}},
  shorttitle = {{{ROS}} 2 in a {{Nutshell}}},
  author = {Aloqaily, Hamad and Koubaa, Anis and Abdelkader, Mohamed},
  year = 2025,
  month = may,
  number = {2024101204},
  eprint = {2024101204},
  publisher = {Preprints},
  doi = {10.20944/preprints202410.1204.v3},
  urldate = {2026-01-25},
  abstract = {This study presents a comprehensive review that addresses the critical transition from ROS 1 to ROS 2, spotlighting the significant enhancements and the pressing need for a detailed exploration of ROS 2 within the robotics community. Despite the extensive deployment and adaptations of ROS in varied robotics applications, literature lacks a cohesive synthesis that delineates the advancements, limitations, and broader impacts of ROS 2 compared to its predecessor, ROS 1. Our contribution bridges this gap by assembling the largest database of ROS-related research, encompassing 7,371 articles, with a focused analysis in this survey on 435 ROS2-specific publications. We categorize these into i.) articles that discuss and analyze core ROS 2 concepts, ii.) articles that propose frameworks or tools for ROS 2, and iii.) articles utilizing ROS 2. Furthermore, we summarize literature findings of ROS 2 challenges, advancements, and future direction in the fields of a.) security, b.) real-time, c.) middleware, d.) embedded and distributed systems, e.) communication reliability and QoS, and f.) multi-robot systems. The methodology involved meticulous data collection and categorization from multiple databases, facilitating an in-depth online accessible resource. Results underscore ROS2's enhancements in modularity, real-time capabilities, and security, extending its applicability across various robotic platforms and industries. However, challenges in scalability and reliability persist, signaling avenues for future enhancements. This review not only deepens the understanding of ROS2's contributions but also charts a path for ongoing improvements in robotic systems design. The original data presented in the study are openly available in https://www.ros.riotu-lab.org/.},
  archiveprefix = {Preprints},
  langid = {english},
  keywords = {autonomous systems,DDS,distributed robotics,embeddedsystems,modularity,multi-robot systems,open-source robotics,real-time,robot software ecosystems,robotics middleware,ROS,ROS 2,ROS frameworks,ROS Packages,ROS QoS,ROS simulators,ROSdatabase,security,simulation,UAV},
  file = {G:\My Drive\03 Resources\Literature\2025 - ROS 2 in a Nutshell A Survey - Aloqaily et al..pdf}
}

@misc{2025_SafePlanLeveraging_obi,
  title = {{{SafePlan}}: {{Leveraging Formal Logic}} and {{Chain-of-Thought Reasoning}} for {{Enhanced Safety}} in {{LLM-based Robotic Task Planning}}},
  shorttitle = {{{SafePlan}}},
  author = {Obi, Ike and Venkatesh, Vishnunandan L. N. and Wang, Weizheng and Wang, Ruiqi and Suh, Dayoon and Amosa, Temitope I. and Jo, Wonse and Min, Byung-Cheol},
  year = 2025,
  month = mar,
  number = {arXiv:2503.06892},
  eprint = {2503.06892},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.06892},
  urldate = {2026-01-30},
  abstract = {Robotics researchers increasingly leverage large language models (LLM) in robotics systems, using them as interfaces to receive task commands, generate task plans, form team coalitions, and allocate tasks among multi-robot and human agents. However, despite their benefits, the growing adoption of LLM in robotics has raised several safety concerns, particularly regarding executing malicious or unsafe natural language prompts. In addition, ensuring that task plans, team formation, and task allocation outputs from LLMs are adequately examined, refined, or rejected is crucial for maintaining system integrity. In this paper, we introduce SafePlan, a multi-component framework that combines formal logic and chain-of-thought reasoners for enhancing the safety of LLM-based robotics systems. Using the components of SafePlan, including Prompt Sanity COT Reasoner and Invariant, Precondition, and Postcondition COT reasoners, we examined the safety of natural language task prompts, task plans, and task allocation outputs generated by LLM-based robotic systems as means of investigating and enhancing system safety profile. Our results show that SafePlan outperforms baseline models by leading to 90.5\% reduction in harmful task prompt acceptance while still maintaining reasonable acceptance of safe tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  annotation = {7 citations (Semantic Scholar/DOI) [2026-01-30]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2025 - SafePlan Leveraging Formal Logic and Chain-of-Thought Reasoning for Enhanced Safety in LLM-based Robotic Task Planning - Obi et al. 1.pdf;C\:\\Users\\asmal\\Zotero\\storage\\4NZNXTFM\\2503.html}
}

@inproceedings{2025_SafetyAware_khan,
  title = {Safety {{Aware Task Planning}} via {{Large Language Models}} in {{Robotics}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Khan, Azal Ahmad and Andrev, Michael and Murtaza, Muhammad Ali and Aguilera, Sergio and Zhang, Rui and Ding, Jie and Hutchinson, Seth and Anwar, Ali},
  year = 2025,
  month = oct,
  pages = {21024--21031},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11246041},
  urldate = {2026-02-16},
  abstract = {The integration of large language models (LLMs) into robotic task planning has unlocked better reasoning capabilities for complex, long-horizon workflows. However, ensuring safety in LLM-driven plans remains a critical challenge, as these models often prioritize task completion over risk mitigation. This paper introduces SAFER (Safety-Aware Framework for Execution in Robotics), a multi-LLM framework designed to embed safety awareness into robotic task planning. SAFER employs a Safety Agent that operates alongside the primary task planner, providing safety feedback. Additionally, we introduce LLM-as-a-Judge, a novel metric leveraging LLMs as evaluators to quantify safety violations within generated task plans. Our framework integrates safety feedback at multiple stages of execution, enabling real-time risk assessment, proactive error correction, and transparent safety evaluation. We also integrate a control framework using Control Barrier Functions (CBFs) to ensure safety guarantees within SAFER's task planning. We evaluated SAFER against state-of-the-art LLM planners on complex long-horizon tasks involving heterogeneous robotic agents, demonstrating its effectiveness in reducing safety violations while maintaining task efficiency. We also verify the task planner and safety planner through actual hardware experiments involving multiple robots and a human.},
  keywords = {Large language models,Measurement,Pipelines,Planning,Real-time systems,Refining,Reliability,Risk mitigation,Robots,Safety},
  annotation = {17 citations (Semantic Scholar/DOI) [2026-02-18]\\
17 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Safety Aware Task Planning via Large Language Models in Robotics - Khan et al..pdf}
}

@misc{2025_TemporalPlanning_saccon,
  title = {A {{Temporal Planning Framework}} for {{Multi-Agent Systems}} via {{LLM-Aided Knowledge Base Management}}},
  author = {Saccon, Enrico and Tikna, Ahmet and Martini, Davide De and Lamon, Edoardo and Palopoli, Luigi and Roveri, Marco},
  year = 2025,
  month = feb,
  number = {arXiv:2502.19135},
  eprint = {2502.19135},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.19135},
  urldate = {2026-01-30},
  abstract = {This paper presents a novel framework, called PLANTOR (PLanning with Natural language for Task-Oriented Robots), that integrates Large Language Models (LLMs) with Prolog-based knowledge management and planning for multi-robot tasks. The system employs a two-phase generation of a robot-oriented knowledge base, ensuring reusability and compositional reasoning, as well as a three-step planning procedure that handles temporal dependencies, resource constraints, and parallel task execution via mixed-integer linear programming. The final plan is converted into a Behaviour Tree for direct use in ROS2. We tested the framework in multi-robot assembly tasks within a block world and an arch-building scenario. Results demonstrate that LLMs can produce accurate knowledge bases with modest human feedback, while Prolog guarantees formal correctness and explainability. This approach underscores the potential of LLM integration for advanced robotics tasks requiring flexible, scalable, and human-understandable planning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Robotics},
  annotation = {1 citations (Semantic Scholar/DOI) [2026-01-30]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2025 - A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided Knowledge Base Management - Saccon et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\TJAJB54T\\2502.html}
}

@inproceedings{2025_TrustworthyRobot_yuan,
  title = {Trustworthy {{Robot Behavior Tree Generation Based}} on {{Multi-Source Heterogeneous Knowledge Graph}}},
  booktitle = {2025 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Yuan, Jianchao and Yang, Shuo and Zhang, Qi and Li, Ge and Tang, Jianping},
  year = 2025,
  month = may,
  pages = {10751--10757},
  doi = {10.1109/ICRA55743.2025.11127406},
  urldate = {2026-02-16},
  abstract = {In robotics, the design of robot behavior trees generally requires roboticists to comprehensively and customizable consider all the relevant factors including the robot hardware capabilities, task descriptions, etc, posing great challenges for design quality and efficiency. The mainstream practice of BT design paradigm has been utilizing the BT component framework to develop task-specific BT structures manually. In contrast, the latest advances in Generative Pretrained Transformers (GPTs) have also opened up the possibility of BT design automation. However, these approaches generally show low efficiency or are less trustworthy for complex robot task goals due to time-consuming manual design and unreliable GPT reasoning. To solve the above limitations, this paper proposes a novel knowledge-driven approach that develops a specialized knowledge graph from multi-sourced and heterogeneous highquality robot knowledge to reason out a trustworthy robot plan for achieving complex task goals. Then we present the plan transformation and BT merging algorithms to automatically generate the plan-level BT structure. The comparative experiment results have shown that our approach can generate highquality and trustworthy BT structure regarding the task plan accuracy and consistency, as well as the BT generation time, compared with the manual design and GPT-based approaches.},
  keywords = {Accuracy,Cognition,Hardware,Knowledge graphs,Manuals,Merging,Real-time systems,Robots,Soft sensors,Transformers},
  annotation = {0 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Trustworthy Robot Behavior Tree Generation Based on Multi-Source Heterogeneous Knowledge Graph - Yuan et al..pdf}
}

@misc{2025_VerifyLLMLLMBased_grigorev,
  title = {{{VerifyLLM}}: {{LLM-Based Pre-Execution Task Plan Verification}} for {{Robots}}},
  shorttitle = {{{VerifyLLM}}},
  author = {Grigorev, Danil S. and Kovalev, Alexey K. and Panov, Aleksandr I.},
  year = 2025,
  month = jul,
  number = {arXiv:2507.05118},
  eprint = {2507.05118},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.05118},
  urldate = {2026-01-26},
  abstract = {In the field of robotics, researchers face a critical challenge in ensuring reliable and efficient task planning. Verifying high-level task plans before execution significantly reduces errors and enhance the overall performance of these systems. In this paper, we propose an architecture for automatically verifying high-level task plans before their execution in simulator or real-world environments. Leveraging Large Language Models (LLMs), our approach consists of two key steps: first, the conversion of natural language instructions into Linear Temporal Logic (LTL), followed by a comprehensive analysis of action sequences. The module uses the reasoning capabilities of the LLM to evaluate logical coherence and identify potential gaps in the plan. Rigorous testing on datasets of varying complexity demonstrates the broad applicability of the module to household tasks. We contribute to improving the reliability and efficiency of task planning and addresses the critical need for robust pre-execution verification in autonomous systems. The code is available at https://verifyllm.github.io.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  annotation = {6 citations (Semantic Scholar/arXiv) [2026-01-26]},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2025 - VerifyLLM LLM-Based Pre-Execution Task Plan Verification for Robots - Grigorev et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\BD7WXGYG\\2507.html}
}

@article{2025_VisionLanguageActionModels_kawaharazuka,
  title = {Vision-{{Language-Action Models}} for {{Robotics}}: {{A Review Towards Real-World Applications}}},
  shorttitle = {Vision-{{Language-Action Models}} for {{Robotics}}},
  author = {Kawaharazuka, Kento and Oh, Jihoon and Yamada, Jun and Posner, Ingmar and Zhu, Yuke},
  year = 2025,
  journal = {IEEE Access},
  volume = {13},
  pages = {162467--162504},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2025.3609980},
  urldate = {2025-12-07},
  abstract = {Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable real-world deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: https://vla-survey.github.io},
  keywords = {Computational modeling,Computer architecture,Data models,foundation models,imitation learning,Robot kinematics,robot learning,Robot sensing systems,robotics,Robots,survey,Surveys,Training,Transformers,Vision-language-action models,Visualization},
  annotation = {18 citations (Semantic Scholar/DOI) [2025-12-16]\\
3 citations (Crossref/DOI) [2025-12-16]\\
14 citations (Semantic Scholar/DOI) [2025-12-07]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Vision-Language-Action Models for Robotics A Review Towards Real-World Applications - Kawaharazuka et al..pdf}
}

@inproceedings{2025_ZeroKnowledgeTask_hoffmeister,
  title = {Towards {{Zero-Knowledge Task Planning}} via a {{Language-based Approach}}},
  booktitle = {2025 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Hoffmeister, Liam Merz and Scassellati, Brian and Rakita, Daniel},
  year = 2025,
  month = oct,
  pages = {19233--19239},
  issn = {2153-0866},
  doi = {10.1109/IROS60139.2025.11247082},
  urldate = {2026-02-16},
  abstract = {In this work, we introduce and formalize the Zero-Knowledge Task Planning (ZKTP) problem, i.e., formulating a sequence of actions to achieve some goal without task-specific knowledge. Additionally, we present a first investigation and approach for ZKTP that leverages a large language model (LLM) to decompose natural language instructions into subtasks and generate behavior trees (BTs) for execution. If errors arise during task execution, the approach also uses an LLM to adjust the BTs on-the-fly in a refinement loop. Experimental validation in the AI2-THOR simulator demonstrate our approach's effectiveness in improving overall task performance compared to alternative approaches that leverage task-specific knowledge. Our work demonstrates the potential of LLMs to effectively address several aspects of the ZKTP problem, providing a robust framework for automated behavior generation with no task-specific setup.},
  keywords = {Intelligent robots,Large language models,Natural languages,Planning,Trees (botanical)},
  annotation = {0 citations (Semantic Scholar/DOI) [2026-02-18]\\
5 citations (Semantic Scholar/DOI) [2026-02-16]},
  file = {G:\My Drive\03 Resources\Literature\2025 - Towards Zero-Knowledge Task Planning via a Language-based Approach - Hoffmeister et al..pdf}
}

@article{2026_AutomatedGeneration_saccon,
  title = {Automated {{Generation}} of {{MDPs Using Logic Programming}} and {{LLMs}} for {{Robotic Applications}}},
  author = {Saccon, Enrico and De Martini, Davide and Saveriano, Matteo and Lamon, Edoardo and Palopoli, Luigi and Roveri, Marco},
  year = 2026,
  month = feb,
  journal = {IEEE Robotics and Automation Letters},
  volume = {11},
  number = {2},
  pages = {1770--1777},
  issn = {2377-3766},
  doi = {10.1109/LRA.2025.3643276},
  urldate = {2026-02-17},
  abstract = {We present a novel framework that integrates Large Language Models (LLMs) with automated planning and formal verification to streamline the creation and use of Markov Decision Processes (MDP). Our system leverages LLMs to extract structured knowledge in the form of a Prolog knowledge base from natural language (NL) descriptions. It then automatically constructs an MDP through reachability analysis, and synthesises optimal policies using the Storm model checker. The resulting policy is exported as a state-action table for execution. We validate the framework in three human-robot interaction scenarios, demonstrating its ability to produce executable policies with minimal manual effort. This work highlights the potential of combining language models with formal methods to enable more accessible and scalable probabilistic planning in robotics.},
  keywords = {AI-Based methods,Cognition,human-robot collaboration,Human-robot interaction,Knowledge based systems,Logic programming,Planning,Planning under uncertainty,Probabilistic logic,Robot kinematics,Robots,Storms,Uncertainty},
  file = {G:\My Drive\03 Resources\Literature\2026 - Automated Generation of MDPs Using Logic Programming and LLMs for Robotic Applications - Saccon et al..pdf}
}

@misc{2026_AutomatikaroboticsEmbodiedagents_,
  title = {Automatika-Robotics/Embodied-Agents},
  year = 2026,
  month = jan,
  urldate = {2026-01-30},
  abstract = {EmbodiedAgents is a fully-loaded ROS2 based framework for creating interactive physical agents that can understand, remember, and act upon contextual information from their environment.},
  copyright = {MIT},
  howpublished = {Automatika Robotics},
  keywords = {deeplearning,embodied-agent,embodied-ai,generative-ai,godel-machine,llm,machine-learning,multimodal,ollama,physical-ai,roboml,robotics,ros2,vllm}
}

@misc{2026_RoboBrain25_tan,
  title = {{{RoboBrain}} 2.5: {{Depth}} in {{Sight}}, {{Time}} in {{Mind}}},
  shorttitle = {{{RoboBrain}} 2.5},
  author = {Tan, Huajie and Zhou, Enshen and Li, Zhiyu and Xu, Yijie and Ji, Yuheng and Chen, Xiansheng and Chi, Cheng and Wang, Pengwei and Jia, Huizhu and Ao, Yulong and Cao, Mingyu and Chen, Sixiang and Li, Zhe and Liu, Mengzhen and Wang, Zixiao and Rong, Shanyu and Lyu, Yaoxu and Zhao, Zhongxia and Co, Peterson and Li, Yibo and Han, Yi and Xie, Shaoxuan and Yao, Guocai and Wang, Songjing and Zhang, Leiduo and Yang, Xi and Jiao, Yance and Shi, Donghai and Xie, Kunchang and Nie, Shaokai and Men, Chunlei and Lin, Yonghua and Wang, Zhongyuan and Huang, Tiejun and Zhang, Shanghang},
  year = 2026,
  month = jan,
  number = {arXiv:2601.14352},
  eprint = {2601.14352},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2601.14352},
  urldate = {2026-01-25},
  abstract = {We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {G\:\\My Drive\\03 Resources\\Literature\\2026 - RoboBrain 2.5 Depth in Sight, Time in Mind - Tan et al..pdf;C\:\\Users\\asmal\\Zotero\\storage\\PMVESIWQ\\2601.html}
}
